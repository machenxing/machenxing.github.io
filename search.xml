<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[如何在Spring-Boot中做参数校验？]]></title>
    <url>%2F2019%2F12%2F07%2F%E5%A6%82%E4%BD%95%E5%9C%A8Spring-Boot%E4%B8%AD%E5%81%9A%E5%8F%82%E6%95%B0%E6%A0%A1%E9%AA%8C%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[数据的校验的重要性就不用说了，即使在前端对数据进行校验的情况下，我们还是要对传入后端的数据再进行一遍校验，避免用户绕过浏览器直接通过一些 HTTP 工具直接向后端请求一些违法数据。 我个人觉得这个和统一异常处理一样是后端很容易做好的一件事情，同时也是很有必要的事情。如果对后端如何统一异常处理不太清楚的朋友，也可以留言一下，我后面会分享自己在项目中学到的统一异常处理的方法。 本文结合自己在项目中的实际使用经验，可以说文章介绍的内容很实用，不了解的朋友可以学习一下，后面可以立马实践到项目上去。 下面我会通过实例程序演示如何在 Java 程序中尤其是 Spring 程序中优雅地的进行参数验证。 基础设施搭建相关依赖如果开发普通 Java 程序的的话，你需要可能需要像下面这样依赖： 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.hibernate.validator&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt; &lt;version&gt;6.0.9.Final&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;javax.el&lt;/groupId&gt; &lt;artifactId&gt;javax.el-api&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.glassfish.web&lt;/groupId&gt; &lt;artifactId&gt;javax.el&lt;/artifactId&gt; &lt;version&gt;2.2.6&lt;/version&gt; &lt;/dependency&gt; 使用 Spring Boot 程序的话只需要spring-boot-starter-web 就够了，它的子依赖包含了我们所需要的东西。除了这个依赖，下面的演示还用到了 lombok ，所以不要忘记添加上相关依赖。 12345678910111213141516&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 实体类下面这个是示例用到的实体类。 123456789101112131415161718192021@Data@AllArgsConstructor@NoArgsConstructorpublic class Person &#123; @NotNull(message = "classId 不能为空") private String classId; @Size(max = 33) @NotNull(message = "name 不能为空") private String name; @Pattern(regexp = "((^Man$|^Woman$|^UGM$))", message = "sex 值不在可选范围") @NotNull(message = "sex 不能为空") private String sex; @Email(message = "email 格式不正确") @NotNull(message = "email 不能为空") private String email;&#125; 正则表达式说明： 12345&gt; - ^string : 匹配以 string 开头的字符串&gt; - string$ ：匹配以 string 结尾的字符串&gt; - ^string$ ：精确匹配 string 字符串&gt; - ((^Man$|^Woman$|^UGM$)) : 值只能在 Man,Woman,UGM 这三个值中选择&gt; 下面这部分校验注解说明内容参考自：https://www.cnkirito.moe/spring-validation/ ，感谢@徐靖峰。 JSR提供的校验注解: @Null 被注释的元素必须为 null @NotNull 被注释的元素必须不为 null @AssertTrue 被注释的元素必须为 true @AssertFalse 被注释的元素必须为 false @Min(value) 被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @Max(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @DecimalMin(value) 被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @DecimalMax(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @Size(max=, min=) 被注释的元素的大小必须在指定的范围内 @Digits (integer, fraction) 被注释的元素必须是一个数字，其值必须在可接受的范围内 @Past 被注释的元素必须是一个过去的日期 @Future 被注释的元素必须是一个将来的日期 @Pattern(regex=,flag=) 被注释的元素必须符合指定的正则表达式 Hibernate Validator提供的校验注解： @NotBlank(message =) 验证字符串非null，且长度必须大于0 @Email 被注释的元素必须是电子邮箱地址 @Length(min=,max=) 被注释的字符串的大小必须在指定的范围内 @NotEmpty 被注释的字符串的必须非空 @Range(min=,max=,message=) 被注释的元素必须在合适的范围内 验证Controller的输入验证请求体(RequestBody)Controller： 我们在需要验证的参数上加上了@Valid注解，如果验证失败，它将抛出MethodArgumentNotValidException。默认情况下，Spring会将此异常转换为HTTP Status 400（错误请求）。 123456789@RestController@RequestMapping("/api")public class PersonController &#123; @PostMapping("/person") public ResponseEntity&lt;Person&gt; getPerson(@RequestBody @Valid Person person) &#123; return ResponseEntity.ok().body(person); &#125;&#125; ExceptionHandler： 自定义异常处理器可以帮助我们捕获异常，并进行一些简单的处理。如果对于下面的处理异常的代码不太理解的话，可以查看这篇文章 《SpringBoot 处理异常的几种常见姿势》。 1234567891011121314@ControllerAdvice(assignableTypes = &#123;PersonController.class&#125;)public class GlobalExceptionHandler &#123; @ExceptionHandler(MethodArgumentNotValidException.class) public ResponseEntity&lt;Map&lt;String, String&gt;&gt; handleValidationExceptions( MethodArgumentNotValidException ex) &#123; Map&lt;String, String&gt; errors = new HashMap&lt;&gt;(); ex.getBindingResult().getAllErrors().forEach((error) -&gt; &#123; String fieldName = ((FieldError) error).getField(); String errorMessage = error.getDefaultMessage(); errors.put(fieldName, errorMessage); &#125;); return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(errors); &#125;&#125; 通过测试验证： 下面我通过 MockMvc 模拟请求 Controller 的方式来验证是否生效，当然你也可以通过 Postman 这种工具来验证。 我们试一下所有参数输入正确的情况。 123456789101112131415161718192021222324252627@RunWith(SpringRunner.class)@SpringBootTest@AutoConfigureMockMvcpublic class PersonControllerTest &#123; @Autowired private MockMvc mockMvc; @Autowired private ObjectMapper objectMapper; @Test public void should_get_person_correctly() throws Exception &#123; Person person = new Person(); person.setName("SnailClimb"); person.setSex("Man"); person.setClassId("82938390"); person.setEmail("Snailclimb@qq.com"); mockMvc.perform(post("/api/person") .contentType(MediaType.APPLICATION_JSON_UTF8) .content(objectMapper.writeValueAsString(person))) .andExpect(MockMvcResultMatchers.jsonPath("name").value("SnailClimb")) .andExpect(MockMvcResultMatchers.jsonPath("classId").value("82938390")) .andExpect(MockMvcResultMatchers.jsonPath("sex").value("Man")) .andExpect(MockMvcResultMatchers.jsonPath("email").value("Snailclimb@qq.com")); &#125;&#125; 验证出现参数不合法的情况抛出异常并且可以正确被捕获。 1234567891011121314@Test public void should_check_person_value() throws Exception &#123; Person person = new Person(); person.setSex("Man22"); person.setClassId("82938390"); person.setEmail("SnailClimb"); mockMvc.perform(post("/api/person") .contentType(MediaType.APPLICATION_JSON_UTF8) .content(objectMapper.writeValueAsString(person))) .andExpect(MockMvcResultMatchers.jsonPath("sex").value("sex 值不在可选范围")) .andExpect(MockMvcResultMatchers.jsonPath("name").value("name 不能为空")) .andExpect(MockMvcResultMatchers.jsonPath("email").value("email 格式不正确")); &#125; 使用 Postman 验证结果如下： 验证请求参数(Path Variables 和 Request Parameters)Controller： 一定一定不要忘记在类上加上 Validated 注解了，这个参数可以告诉 Spring 去校验方法参数。 123456789101112131415@RestController@RequestMapping("/api")@Validatedpublic class PersonController &#123; @GetMapping("/person/&#123;id&#125;") public ResponseEntity&lt;Integer&gt; getPersonByID(@Valid @PathVariable("id") @Max(value = 5,message = "超过 id 的范围了") Integer id) &#123; return ResponseEntity.ok().body(id); &#125; @PutMapping("/person") public ResponseEntity&lt;String&gt; getPersonByName(@Valid @RequestParam("name") @Size(max = 6,message = "超过 name 的范围了") String name) &#123; return ResponseEntity.ok().body(name); &#125;&#125; ExceptionHandler： 1234@ExceptionHandler(ConstraintViolationException.class)ResponseEntity&lt;String&gt; handleConstraintViolationException(ConstraintViolationException e) &#123; return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(e.getMessage());&#125; 通过测试验证： 123456789101112131415161718@Test public void should_check_param_value() throws Exception &#123; mockMvc.perform(get("/api/person/6") .contentType(MediaType.APPLICATION_JSON_UTF8)) .andExpect(status().isBadRequest()) .andExpect(content().string("getPersonByID.id: 超过 id 的范围了")); &#125; @Test public void should_check_param_value2() throws Exception &#123; mockMvc.perform(put("/api/person") .param("name","snailclimbsnailclimb") .contentType(MediaType.APPLICATION_JSON_UTF8)) .andExpect(status().isBadRequest()) .andExpect(content().string("getPersonByName.name: 超过 name 的范围了")); &#125; 验证 Service 中的方法我们还可以验证任何Spring组件的输入，而不是验证控制器级别的输入，我们可以使用@Validated和@Valid注释的组合来实现这一需求。 一定一定不要忘记在类上加上 Validated 注解了，这个参数可以告诉 Spring 去校验方法参数。 12345678@Service@Validatedpublic class PersonService &#123; public void validatePerson(@Valid Person person)&#123; // do something &#125;&#125; 通过测试验证： 1234567891011121314151617@RunWith(SpringRunner.class)@SpringBootTest@AutoConfigureMockMvcpublic class PersonServiceTest &#123; @Autowired private PersonService service; @Test(expected = ConstraintViolationException.class) public void should_throw_exception_when_person_is_not_valid() &#123; Person person = new Person(); person.setSex("Man22"); person.setClassId("82938390"); person.setEmail("SnailClimb"); service.validatePerson(person); &#125;&#125; Validator 编程方式手动进行参数验证某些场景下可能会需要我们手动校验并获得校验结果。 123456789101112131415161718@Testpublic void check_person_manually() &#123; ValidatorFactory factory = Validation.buildDefaultValidatorFactory(); Validator validator = factory.getValidator(); Person person = new Person(); person.setSex("Man22"); person.setClassId("82938390"); person.setEmail("SnailClimb"); Set&lt;ConstraintViolation&lt;Person&gt;&gt; violations = validator.validate(person); //output: //email 格式不正确 //name 不能为空 //sex 值不在可选范围 for (ConstraintViolation&lt;Person&gt; constraintViolation : violations) &#123; System.out.println(constraintViolation.getMessage()); &#125;&#125; 上面我们是通过 Validator 工厂类获得的 Validator 示例，当然你也可以通过 @Autowired 直接注入的方式。但是在非 Spring Component 类中使用这种方式的话，只能通过工厂类来获得 Validator。 12@AutowiredValidator validate 自定以 Validator(实用)如果自带的校验注解无法满足你的需求的话，你还可以自定义实现注解。 案例一:校验特定字段的值是否在可选范围比如我们现在多了这样一个需求：Person类多了一个 region 字段，region 字段只能是China、China-Taiwan、China-HongKong这三个中的一个。 第一步你需要创建一个注解： 123456789101112@Target(&#123;FIELD&#125;)@Retention(RUNTIME)@Constraint(validatedBy = RegionValidator.class)@Documentedpublic @interface Region &#123; String message() default "Region 值不在可选范围内"; Class&lt;?&gt;[] groups() default &#123;&#125;; Class&lt;? extends Payload&gt;[] payload() default &#123;&#125;;&#125; 第二步你需要实现 ConstraintValidator接口，并重写isValid 方法： 123456789101112131415import javax.validation.ConstraintValidator;import javax.validation.ConstraintValidatorContext;import java.util.HashSet;public class RegionValidator implements ConstraintValidator&lt;Region, String&gt; &#123; @Override public boolean isValid(String value, ConstraintValidatorContext context) &#123; HashSet&lt;Object&gt; regions = new HashSet&lt;&gt;(); regions.add("China"); regions.add("China-Taiwan"); regions.add("China-HongKong"); return regions.contains(value); &#125;&#125; 现在你就可以使用这个注解： 12@Regionprivate String region; 案例二:校验电话号码校验我们的电话号码是否合法，这个可以通过正则表达式来做，相关的正则表达式都可以在网上搜到，你甚至可以搜索到针对特定运营商电话号码段的正则表达式。 PhoneNumber.java 123456789101112131415161718import javax.validation.Constraint;import java.lang.annotation.Documented;import java.lang.annotation.Retention;import java.lang.annotation.Target;import static java.lang.annotation.ElementType.FIELD;import static java.lang.annotation.ElementType.PARAMETER;import static java.lang.annotation.RetentionPolicy.RUNTIME;@Documented@Constraint(validatedBy = PhoneNumberValidator.class)@Target(&#123;FIELD, PARAMETER&#125;)@Retention(RUNTIME)public @interface PhoneNumber &#123; String message() default "Invalid phone number"; Class[] groups() default &#123;&#125;; Class[] payload() default &#123;&#125;;&#125; PhoneNumberValidator.java 1234567891011121314import javax.validation.ConstraintValidator;import javax.validation.ConstraintValidatorContext;public class PhoneNumberValidator implements ConstraintValidator&lt;PhoneNumber,String&gt; &#123; @Override public boolean isValid(String phoneField, ConstraintValidatorContext context) &#123; if (phoneField == null) &#123; // can be null return true; &#125; return phoneField.matches("^1(3[0-9]|4[57]|5[0-35-9]|8[0-9]|70)\\d&#123;8&#125;$") &amp;&amp; phoneField.length() &gt; 8 &amp;&amp; phoneField.length() &lt; 14; &#125;&#125; 搞定，我们现在就可以使用这个注解了。 123@PhoneNumber(message = "phoneNumber 格式不正确")@NotNull(message = "phoneNumber 不能为空")private String phoneNumber; 使用验证组某些场景下我们需要使用到验证组，这样说可能不太清楚，说简单点就是对对象操作的不同方法有不同的验证规则，示例如下（这个就我目前经历的项目来说使用的比较少，因为本身这个在代码层面理解起来是比较麻烦的，然后写起来也比较麻烦）。 先创建两个接口： 1234public interface AddPersonGroup &#123;&#125;public interface DeletePersonGroup &#123;&#125; 我们可以这样去使用验证组 123@NotNull(groups = DeletePersonGroup.class)@Null(groups = AddPersonGroup.class)private String group; 12345678910111213141516171819@Service@Validatedpublic class PersonService &#123; public void validatePerson(@Valid Person person) &#123; // do something &#125; @Validated(AddPersonGroup.class) public void validatePersonGroupForAdd(@Valid Person person) &#123; // do something &#125; @Validated(DeletePersonGroup.class) public void validatePersonGroupForDelete(@Valid Person person) &#123; // do something &#125;&#125; 通过测试验证： 123456789101112131415161718@Test(expected = ConstraintViolationException.class) public void should_check_person_with_groups() &#123; Person person = new Person(); person.setSex("Man22"); person.setClassId("82938390"); person.setEmail("SnailClimb"); person.setGroup("group1"); service.validatePersonGroupForAdd(person); &#125; @Test(expected = ConstraintViolationException.class) public void should_check_person_with_groups2() &#123; Person person = new Person(); person.setSex("Man22"); person.setClassId("82938390"); person.setEmail("SnailClimb"); service.validatePersonGroupForDelete(person); &#125; 使用验证组这种方式的时候一定要小心，这是一种反模式，还会造成代码逻辑性变差。 代码地址：https://github.com/Snailclimb/springboot-guide/tree/master/source-code/advanced/bean-validation-demo @NotNull vs @Column(nullable = false)(重要)在使用 JPA 操作数据的时候会经常碰到 @Column(nullable = false) 这种类型的约束，那么它和 @NotNull 有何区别呢？搞清楚这个还是很重要的！ @NotNull是 JSR 303 Bean验证批注,它与数据库约束本身无关。 @Column(nullable = false) : 是JPA声明列为非空的方法。 总结来说就是即前者用于验证，而后者则用于指示数据库创建表的时候对表的约束。 参考文档原文出处：https://mp.weixin.qq.com/s/hGBTtXlncXwY5TU5cG8IMA https://reflectoring.io/bean-validation-with-spring-boot/ https://www.cnkirito.moe/spring-validation/]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在浏览器输入 URL 回车之后发生了什么？]]></title>
    <url>%2F2019%2F10%2F01%2F%E5%9C%A8%E6%B5%8F%E8%A7%88%E5%99%A8%E8%BE%93%E5%85%A5-URL-%E5%9B%9E%E8%BD%A6%E4%B9%8B%E5%90%8E%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[前言这个问题已经是老生常谈了，更是经常被作为面试的压轴题出现，网上也有很多文章，但最近闲的无聊，然后就自己做了一篇笔记，感觉比之前理解更透彻了。 这篇笔记是我这两天看了数十篇文章总结出来的，所以相对全面一点，但由于我是做前端的，所以会比较重点分析浏览器渲染页面那一部分，至于其他部分我会罗列出关键词，感兴趣的可以自行查阅. 注意：本文的步骤是建立在，请求的是一个简单的 HTTP 请求，没有 HTTPS、HTTP2、最简单的 DNS、没有代理、并且服务器没有任何问题的基础上，尽管这是不切实际的。 大致流程 URL 解析 DNS 查询 TCP 连接 处理请求 接受响应 渲染页面 URL 解析地址解析： 首先判断你输入的是一个合法的 URL 还是一个待搜索的关键词，并且根据你输入的内容进行自动完成、字符编码等操作。 HSTS 由于安全隐患，会使用 HSTS 强制客户端使用 HTTPS 访问页面。详见：你所不知道的 HSTS[1]。 其他操作 浏览器还会进行一些额外的操作，比如安全检查、访问限制（之前国产浏览器限制 996.icu）。 检查缓存 DNS 查询基本步骤 1. 浏览器缓存 浏览器会先检查是否在缓存中，没有则调用系统库函数进行查询。 2. 操作系统缓存 操作系统也有自己的 DNS缓存，但在这之前，会向检查域名是否存在本地的 Hosts 文件里，没有则向 DNS 服务器发送查询请求。 3. 路由器缓存 路由器也有自己的缓存。 4. ISP DNS 缓存 ISP DNS 就是在客户端电脑上设置的首选 DNS 服务器，它们在大多数情况下都会有缓存。 根域名服务器查询 在前面所有步骤没有缓存的情况下，本地 DNS 服务器会将请求转发到互联网上的根域，下面这个图很好的诠释了整个流程： 需要注意的点 递归方式：一路查下去中间不返回，得到最终结果才返回信息（浏览器到本地DNS服务器的过程） 迭代方式，就是本地DNS服务器到根域名服务器查询的方式。 什么是 DNS 劫持 前端 dns-prefetch 优化 TCP 连接TCP/IP 分为四层，在发送数据时，每层都要对数据进行封装： 应用层：发送 HTTP 请求在前面的步骤我们已经得到服务器的 IP 地址，浏览器会开始构造一个 HTTP 报文，其中包括： 请求报头（Request Header）：请求方法、目标地址、遵循的协议等等 请求主体（其他参数） 其中需要注意的点： 浏览器只能发送 GET、POST 方法，而打开网页使用的是 GET 方法 传输层：TCP 传输报文传输层会发起一条到达服务器的 TCP 连接，为了方便传输，会对数据进行分割（以报文段为单位），并标记编号，方便服务器接受时能够准确地还原报文信息。 在建立连接前，会先进行 TCP 三次握手。 关于 TCP/IP 三次握手，网上已经有很多段子和图片生动地描述了。 相关知识点： SYN 泛洪攻击 网络层：IP协议查询Mac地址将数据段打包，并加入源及目标的IP地址，并且负责寻找传输路线。 判断目标地址是否与当前地址处于同一网络中，是的话直接根据 Mac 地址发送，否则使用路由表查找下一跳地址，以及使用 ARP 协议查询它的 Mac 地址。 注意：在 OSI 参考模型中 ARP 协议位于链路层，但在 TCP/IP 中，它位于网络层。 链路层：以太网协议以太网协议 根据以太网协议将数据分为以“帧”为单位的数据包，每一帧分为两个部分： 标头：数据包的发送者、接受者、数据类型 数据：数据包具体内容 Mac 地址 以太网规定了连入网络的所有设备都必须具备“网卡”接口，数据包都是从一块网卡传递到另一块网卡，网卡的地址就是 Mac 地址。每一个 Mac 地址都是独一无二的，具备了一对一的能力。 广播 发送数据的方法很原始，直接把数据通过 ARP 协议，向本网络的所有机器发送，接收方根据标头信息与自身 Mac 地址比较，一致就接受，否则丢弃。 注意：接收方回应是单播。 相关知识点： ARP 攻击 服务器接受请求 接受过程就是把以上步骤逆转过来，参见上图。 服务器处理请求大致流程 HTTPD 最常见的 HTTPD 有 Linux 上常用的 Apache 和 Nginx，以及 Windows 上的 IIS。 它会监听得到的请求，然后开启一个子进程去处理这个请求。 处理请求 接受 TCP 报文后，会对连接进行处理，对HTTP协议进行解析（请求方法、域名、路径等），并且进行一些验证： 验证是否配置虚拟主机 验证虚拟主机是否接受此方法 验证该用户可以使用该方法（根据 IP 地址、身份信息等） 重定向 假如服务器配置了 HTTP 重定向，就会返回一个 301永久重定向响应，浏览器就会根据响应，重新发送 HTTP 请求（重新执行上面的过程）。 关于更多：详见这篇文章[2] URL 重写 然后会查看 URL 重写规则，如果请求的文件是真实存在的，比如图片、html、css、js文件等，则会直接把这个文件返回。 否则服务器会按照规则把请求重写到 一个 REST 风格的 URL 上。 然后根据动态语言的脚本，来决定调用什么类型的动态文件解释器来处理这个请求。 以 PHP 语言的 MVC 框架举例，它首先会初始化一些环境的参数，根据 URL 由上到下地去匹配路由，然后让路由所定义的方法去处理请求。 浏览器接受响应浏览器接收到来自服务器的响应资源后，会对资源进行分析。 首先查看 Response header，根据不同状态码做不同的事（比如上面提到的重定向）。 如果响应资源进行了压缩（比如 gzip），还需要进行解压。 然后，对响应资源做缓存。 接下来，根据响应资源里的 MIME[3] 类型去解析响应内容（比如 HTML、Image各有不同的解析方式）。 渲染页面浏览器内核 不同的浏览器内核，渲染过程也不完全相同，但大致流程都差不多。 基本流程 HTML 解析首先要知道浏览器解析是从上往下一行一行地解析的。 解析的过程可以分为四个步骤： ① 解码（encoding） 传输回来的其实都是一些二进制字节数据，浏览器需要根据文件指定编码（例如UTF-8）转换成字符串，也就是HTML 代码。 ② 预解析（pre-parsing） 预解析做的事情是提前加载资源，减少处理时间，它会识别一些会请求资源的属性，比如img标签的src属性，并将这个请求加到请求队列中。 ③ 符号化（Tokenization） 符号化是词法分析的过程，将输入解析成符号，HTML 符号包括，开始标签、结束标签、属性名和属性值。 它通过一个状态机去识别符号的状态，比如遇到&lt;，&gt;状态都会产生变化。 ④ 构建树（tree construction） 注意：符号化和构建树是并行操作的，也就是说只要解析到一个开始标签，就会创建一个 DOM 节点。 在上一步符号化中，解析器获得这些标记，然后以合适的方法创建DOM对象并把这些符号插入到DOM对象中。 1234567891011&lt;html&gt;&lt;head&gt; &lt;title&gt;Web page parsing&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div&gt; &lt;h1&gt;Web page parsing&lt;/h1&gt; &lt;p&gt;This is an example Web page.&lt;/p&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 浏览器容错机制 你从来没有在浏览器看过类似”语法无效”的错误，这是因为浏览器去纠正错误的语法，然后继续工作。 事件 当整个解析的过程完成以后，浏览器会通过DOMContentLoaded事件来通知DOM解析完成。 CSS 解析一旦浏览器下载了 CSS，CSS 解析器就会处理它遇到的任何 CSS，根据语法规范[4]解析出所有的 CSS 并进行标记化，然后我们得到一个规则表。 CSS 匹配规则 在匹配一个节点对应的 CSS 规则时，是按照从右到左的顺序的，例如：div p { font-size :14px }会先寻找所有的p标签然后判断它的父元素是否为div。 所以我们写 CSS 时，尽量用 id 和 class，千万不要过度层叠。 渲染树其实这就是一个 DOM 树和 CSS 规则树合并的过程。 注意：渲染树会忽略那些不需要渲染的节点，比如设置了display:none的节点。 计算 通过计算让任何尺寸值都减少到三个可能之一：auto、百分比、px，比如把rem转化为px。 级联 浏览器需要一种方法来确定哪些样式才真正需要应用到对应元素，所以它使用一个叫做specificity的公式，这个公式会通过： 标签名、class、id 是否内联样式 !important 然后得出一个权重值，取最高的那个。 渲染阻塞 当遇到一个script标签时，DOM 构建会被暂停，直至脚本完成执行，然后继续构建 DOM 树。 但如果 JS 依赖 CSS 样式，而它还没有被下载和构建时，浏览器就会延迟脚本执行，直至 CSS Rules 被构建。 所有我们知道： CSS 会阻塞 JS 执行 JS 会阻塞后面的 DOM 解析 为了避免这种情况，应该以下原则： CSS 资源排在 JavaScript 资源前面 JS 放在 HTML 最底部，也就是 &lt;/body&gt;前 另外，如果要改变阻塞模式，可以使用 defer 与 async，详见：这篇文章[5] 布局与绘制确定渲染树种所有节点的几何属性，比如：位置、大小等等，最后输入一个盒子模型，它能精准地捕获到每个元素在屏幕内的准确位置与大小。 然后遍历渲染树，调用渲染器的 paint() 方法在屏幕上显示其内容。 合并渲染层把以上绘制的所有图片合并，最终输出一张图片。 回流与重绘回流(reflow) 当浏览器发现某个部分发现变化影响了布局时，需要倒回去重新渲染，会从html标签开始递归往下，重新计算位置和大小。 reflow基本是无法避免的，因为当你滑动一下鼠标、resize 窗口，页面就会产生变化。 重绘(repaint) 改变了某个元素的背景色、文字颜色等等不会影响周围元素的位置变化时，就会发生重绘。 每次重绘后，浏览器还需要合并渲染层并输出到屏幕上。 回流的成本要比重绘高很多，所以我们应该尽量避免产生回流。 比如： display:none 会触发回流，而 visibility:hidden 只会触发重绘。 JavaScript 编译执行大致流程 可以分为三个阶段： 词法分析JS 脚本加载完毕后，会首先进入语法分析阶段，它首先会分析代码块的语法是否正确，不正确则抛出“语法错误”，停止执行。 几个步骤： 分词，例如将var a = 2，，分成var、a、=、2这样的词法单元。 解析，将词法单元转换成抽象语法树（AST）。 代码生成，将抽象语法树转换成机器指令。 预编译JS 有三种运行环境： 全局环境 函数环境 eval 每进入一个不同的运行环境都会创建一个对应的执行上下文，根据不同的上下文环境，形成一个函数调用栈，栈底永远是全局执行上下文，栈顶则永远是当前执行上下文。 创建执行上下文 创建执行上下文的过程中，主要做了以下三件事： 创建变量对象 参数、函数、变量 建立作用域链 确认当前执行环境是否能访问变量 确定 This 指向 执行JS 线程 虽然 JS 是单线程的，但实际上参与工作的线程一共有四个： 其中三个只是协助，只有 JS 引擎线程是真正执行的 JS 引擎线程：也叫 JS 内核，负责解析执行 JS 脚本程序的主线程，例如 V8 引擎 事件触发线程：属于浏览器内核线程，主要用于控制事件，例如鼠标、键盘等，当事件被触发时，就会把事件的处理函数推进事件队列，等待 JS 引擎线程执行 定时器触发线程：主要控制setInterval和setTimeout，用来计时，计时完毕后，则把定时器的处理函数推进事件队列中，等待 JS 引擎线程。 HTTP 异步请求线程：通过XMLHttpRequest连接后，通过浏览器新开的一个线程，监控readyState状态变更时，如果设置了该状态的回调函数，则将该状态的处理函数推进事件队列中，等待JS引擎线程执行。 注：浏览器对同一域名的并发连接数是有限的，通常为 6 个。 宏任务 分为： 同步任务：按照顺序执行，只有前一个任务完成后，才能执行后一个任务 异步任务：不直接执行，只有满足触发条件时，相关的线程将该异步任务推进任务队列中，等待JS引擎主线程上的任务执行完毕时才开始执行，例如异步Ajax、DOM事件，setTimeout等。 微任务 微任务是ES6和Node环境下的，主要 API 有：Promise，process.nextTick。 微任务的执行在宏任务的同步任务之后，在异步任务之前。 代码例子 123456789101112131415console.log('1'); // 宏任务 同步setTimeout(function() &#123; console.log('2'); // 宏任务 异步&#125;)new Promise(function(resolve) &#123; console.log('3'); // 宏任务 同步 resolve();&#125;).then(function() &#123; console.log('4') // 微任务&#125;)console.log('5') // 宏任务 同步 以上代码输出顺序为：1,3,5,4,2 参考文档原文出处： https://4ark.me/post/b6c7c0a2.html 123456789101112131415161718192021[1] 你所不知道的 HSTS: http://t.cn/AiR8pTqx[2] 详见这篇文章: http://t.cn/AiR8pnEC[3] MIME: http://t.cn/AiR8prtm[4] 语法规范: http://t.cn/AiR80GdO[5] 这篇文章: http://t.cn/AiR80c1k[6] what-happens-when-zh_CN: http://t.cn/AiR80xb5[7] Tags to DOM:http://t.cn/AiR80djX[8] 彻底理解浏览器的缓存机制: http://t.cn/AiR8Ovob[9] 浏览器的工作原理：新式网络浏览器幕后揭秘: http://t.cn/AiR8Oz06[10] 深入浅出浏览器渲染原理: http://t.cn/AiR8O4fO[11] js引擎的执行过程（一）:http://t.cn/AiR8Ot3s]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[new一个对象的过程中发生了什么？]]></title>
    <url>%2F2019%2F10%2F01%2Fnew%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E7%9A%84%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[java在new一个对象的时候，会先查看对象所属的类有没有被加载到内存，如果没有的话，就会先通过类的全限定名来加载。加载并初始化类完成后，再进行对象的创建工作。 我们先假设是第一次使用该类，这样的话new一个对象就可以分为两个过程：加载并初始化类和创建对象。 类加载过程（第一次使用该类）java是使用双亲委派模型来进行类的加载的，所以在描述类加载过程前，我们先看一下它的工作过程： 双亲委托模型的工作过程是：如果一个类加载器（ClassLoader）收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委托给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到顶层的启动类加载器中，只有当父类加载器反馈自己无法完成这个加载请求（它的搜索范围中没有找到所需要加载的类）时，子加载器才会尝试自己去加载。 使用双亲委托机制的好处是：能够有效确保一个类的全局唯一性，当程序中出现多个限定名相同的类时，类加载器在执行加载时，始终只会加载其中的某一个类。 加载由类加载器负责根据一个类的全限定名来读取此类的二进制字节流到JVM内部，并存储在运行时内存区的方法区，然后将其转换为一个与目标类型对应的java.lang.Class对象实例 验证格式验证：验证是否符合class文件规范 语义验证：检查一个被标记为final的类型是否包含子类；检查一个类中的final方法是否被子类进行重写；确保父类和子类之间没有不兼容的一些方法声明（比如方法签名相同，但方法的返回值不同） 操作验证：在操作数栈中的数据必须进行正确的操作，对常量池中的各种符号引用执行验证（通常在解析阶段执行，检查是否可以通过符号引用中描述的全限定名定位到指定类型上，以及类成员信息的访问修饰符是否允许访问等） 准备为类中的所有静态变量分配内存空间，并为其设置一个初始值（由于还没有产生对象，实例变量不在此操作范围内） 被final修饰的static变量（常量），会直接赋值； 解析将常量池中的符号引用转为直接引用（得到类或者字段、方法在内存中的指针或者偏移量，以便直接调用该方法），这个可以在初始化之后再执行。解析需要静态绑定的内容。 // 所有不会被重写的方法和域都会被静态绑定 以上2、3、4三个阶段又合称为链接阶段，链接阶段要做的是将加载到JVM中的二进制字节流的类数据信息合并到JVM的运行时状态中。 初始化（先父后子） 1 为静态变量赋值 2 执行static代码块 注意：static代码块只有jvm能够调用 如果是多线程需要同时初始化一个类，仅仅只能允许其中一个线程对其执行初始化操作，其余线程必须等待，只有在活动线程执行完对类的初始化操作之后，才会通知正在等待的其他线程。 因为子类存在对父类的依赖，所以类的加载顺序是先加载父类后加载子类，初始化也一样。不过，父类初始化时，子类静态变量的值也有有的，是默认值。 最终，方法区会存储当前类类信息，包括类的静态变量、类初始化代码（定义静态变量时的赋值语句 和 静态初始化代码块）、实例变量定义、实例初始化代码（定义实例变量时的赋值语句实例代码块和构造方法）和实例方法，还有父类的类信息引用。 创建对象在堆区分配对象需要的内存分配的内存包括本类和父类的所有实例变量，但不包括任何静态变量 对所有实例变量赋默认值将方法区内对实例变量的定义拷贝一份到堆区，然后赋默认值 执行实例初始化代码初始化顺序是先初始化父类再初始化子类，初始化时先执行实例代码块然后是构造方法 如果有类似于Child c = new Child()形式的c引用的话，在栈区定义Child类型引用变量c，然后将堆区对象的地址赋值给它 需要注意的是，每个子类对象持有父类对象的引用，可在内部通过super关键字来调用父类对象，但在外部不可访问 补充：通过实例引用调用实例方法的时候，先从方法区中对象的实际类型信息找，找不到的话再去父类类型信息中找。 如果继承的层次比较深，要调用的方法位于比较上层的父类，则调用的效率是比较低的，因为每次调用都要经过很多次查找。这时候大多系统会采用一种称为虚方法表的方法来优化调用的效率。 所谓虚方法表，就是在类加载的时候，为每个类创建一个表，这个表包括该类的对象所有动态绑定的方法及其地址，包括父类的方法，但一个方法只有一条记录，子类重写了父类方法后只会保留子类的。当通过对象动态绑定方法的时候，只需要查找这个表就可以了，而不需要挨个查找每个父类。 参考原文出处：https://www.cnblogs.com/JackPn/p/9386182.html]]></content>
      <categories>
        <category>Java虚拟机</category>
      </categories>
      <tags>
        <tag>对象</tag>
        <tag>类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8的Stream集合操作]]></title>
    <url>%2F2019%2F09%2F07%2FJava8%E7%9A%84Stream%E9%9B%86%E5%90%88%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[简介java8也出来好久了，接口默认方法，lambda表达式，函数式接口，Date API等特性还是有必要去了解一下。比如在项目中经常用到集合，遍历集合可以试下lambda表达式，经常还要对集合进行过滤和排序，Stream就派上用场了。用习惯了，不得不说真的很好用。 Stream作为java8的新特性，基于lambda表达式，是对集合对象功能的增强，它专注于对集合对象进行各种高效、便利的聚合操作或者大批量的数据操作，提高了编程效率和代码可读性。 Stream的原理：将要处理的元素看做一种流，流在管道中传输，并且可以在管道的节点上处理，包括过滤筛选、去重、排序、聚合等。元素流在管道中经过中间操作的处理，最后由最终操作得到前面处理的结果。 集合有两种方式生成流： stream() − 为集合创建串行流 parallelStream() - 为集合创建并行流 上图中是Stream类的类结构图，里面包含了大部分的中间和终止操作。 中间操作主要有以下方法（此类型方法返回的都是Stream）：map (mapToInt, flatMap 等)、 filter、 distinct、 sorted、 peek、 limit、 skip、 parallel、 sequential、 unordered 终止操作主要有以下方法：forEach、 forEachOrdered、 toArray、 reduce、 collect、 min、 max、 count、 anyMatch、 allMatch、 noneMatch、 findFirst、 findAny、 iterator 举例说明首先为了说明Stream对对象集合的操作，新建一个Student类（学生类）,覆写了equals()和hashCode()方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public class Student &#123; private Long id; private String name; private int age; private String address; public Student() &#123;&#125; public Student(Long id, String name, int age, String address) &#123; this.id = id; this.name = name; this.age = age; this.address = address; &#125; @Override public String toString() &#123; return "Student&#123;" + "id=" + id + ", name='" + name + '\'' + ", age=" + age + ", address='" + address + '\'' + '&#125;'; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Student student = (Student) o; return age == student.age &amp;&amp; Objects.equals(id, student.id) &amp;&amp; Objects.equals(name, student.name) &amp;&amp; Objects.equals(address, student.address); &#125; @Override public int hashCode() &#123; return Objects.hash(id, name, age, address); &#125; public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getAddress() &#123; return address; &#125; public void setAddress(String address) &#123; this.address = address; &#125; &#125; filter（筛选）1234567891011121314151617181920212223242526public static void main(String [] args) &#123; Student s1 = new Student(1L, "肖战", 15, "浙江"); Student s2 = new Student(2L, "王一博", 15, "湖北"); Student s3 = new Student(3L, "杨紫", 17, "北京"); Student s4 = new Student(4L, "李现", 17, "浙江"); List&lt;Student&gt; students = new ArrayList&lt;&gt;(); students.add(s1); students.add(s2); students.add(s3); students.add(s4); List&lt;Student&gt; streamStudents = testFilter(students); streamStudents.forEach(System.out::println); &#125; /** * 集合的筛选 * @param students * @return */ private static List&lt;Student&gt; testFilter(List&lt;Student&gt; students) &#123; //筛选年龄大于15岁的学生// return students.stream().filter(s -&gt; s.getAge()&gt;15).collect(Collectors.toList()); //筛选住在浙江省的学生 return students.stream().filter(s -&gt;"浙江".equals(s.getAddress())).collect(Collectors.toList()); &#125; 运行结果： 这里我们创建了四个学生，经过filter的筛选，筛选出地址是浙江的学生集合。 map(转换)12345678910111213141516171819202122232425public static void main(String [] args) &#123; Student s1 = new Student(1L, "肖战", 15, "浙江"); Student s2 = new Student(2L, "王一博", 15, "湖北"); Student s3 = new Student(3L, "杨紫", 17, "北京"); Student s4 = new Student(4L, "李现", 17, "浙江"); List&lt;Student&gt; students = new ArrayList&lt;&gt;(); students.add(s1); students.add(s2); students.add(s3); students.add(s4); testMap(students);&#125;/** * 集合转换 * @param students * @return */private static void testMap(List&lt;Student&gt; students) &#123; //在地址前面加上部分信息，只获取地址输出 List&lt;String&gt; addresses = students.stream().map(s -&gt;"住址:"+s.getAddress()).collect(Collectors.toList()); addresses.forEach(a -&gt;System.out.println(a));&#125; 运行结果 map就是将对应的元素按照给定的方法进行转换。 distinct(去重)12345678910111213public static void main(String [] args) &#123; testDistinct1();&#125;/** * 集合去重（基本类型） */private static void testDistinct1() &#123; //简单字符串的去重 List&lt;String&gt; list = Arrays.asList("111","222","333","111","222"); list.stream().distinct().forEach(System.out::println);&#125; 运行结果： 1234567891011121314151617181920212223public static void main(String [] args) &#123; testDistinct2(); &#125; /** * 集合去重（引用对象） */ private static void testDistinct2() &#123; //引用对象的去重，引用对象要实现hashCode和equal方法，否则去重无效 Student s1 = new Student(1L, "肖战", 15, "浙江"); Student s2 = new Student(2L, "王一博", 15, "湖北"); Student s3 = new Student(3L, "杨紫", 17, "北京"); Student s4 = new Student(4L, "李现", 17, "浙江"); Student s5 = new Student(1L, "肖战", 15, "浙江"); List&lt;Student&gt; students = new ArrayList&lt;&gt;(); students.add(s1); students.add(s2); students.add(s3); students.add(s4); students.add(s5); students.stream().distinct().forEach(System.out::println); &#125; 运行结果： 可以看出，两个重复的“肖战”同学进行了去重，这不仅因为使用了distinct()方法，而且因为Student对象重写了equals和hashCode()方法，否则去重是无效的。 sorted(排序)123456789101112public static void main(String [] args) &#123; testSort1();&#125;/** * 集合排序（默认排序） */private static void testSort1() &#123; List&lt;String&gt; list = Arrays.asList("333","222","111"); list.stream().sorted().forEach(System.out::println);&#125; 运行结果： 1234567891011121314151617181920212223public static void main(String [] args) &#123; testSort2();&#125;/** * 集合排序（指定排序规则） */private static void testSort2() &#123; Student s1 = new Student(1L, "肖战", 15, "浙江"); Student s2 = new Student(2L, "王一博", 15, "湖北"); Student s3 = new Student(3L, "杨紫", 17, "北京"); Student s4 = new Student(4L, "李现", 17, "浙江"); List&lt;Student&gt; students = new ArrayList&lt;&gt;(); students.add(s1); students.add(s2); students.add(s3); students.add(s4); students.stream() .sorted((stu1,stu2) -&gt;Long.compare(stu2.getId(), stu1.getId())) .sorted((stu1,stu2) -&gt; Integer.compare(stu2.getAge(),stu1.getAge())) .forEach(System.out::println);&#125; 运行结果： 上面指定排序规则，先按照学生的id进行降序排序，再按照年龄进行降序排序 limit（限制返回个数）123456789101112public static void main(String [] args) &#123; testLimit();&#125;/** * 集合limit，返回前几个元素 */private static void testLimit() &#123; List&lt;String&gt; list = Arrays.asList("333","222","111"); list.stream().limit(2).forEach(System.out::println);&#125; 运行结果： skip(删除元素)123456789101112public static void main(String [] args) &#123; testSkip();&#125;/** * 集合skip，删除前n个元素 */private static void testSkip() &#123; List&lt;String&gt; list = Arrays.asList("333","222","111"); list.stream().skip(2).forEach(System.out::println);&#125; 运行结果： reduce(聚合)1234567891011public static void main(String [] args) &#123; testReduce();&#125;/** * 集合reduce,将集合中每个元素聚合成一条数据 */private static void testReduce() &#123; List&lt;String&gt; list = Arrays.asList("欢","迎","你"); String appendStr = list.stream().reduce("北京",(a,b) -&gt; a+b); System.out.println(appendStr);&#125; 运行结果： min(求最小值)1234567891011121314151617181920public static void main(String [] args) &#123; testMin();&#125;/** * 求集合中元素的最小值 */private static void testMin() &#123; Student s1 = new Student(1L, "肖战", 14, "浙江"); Student s2 = new Student(2L, "王一博", 15, "湖北"); Student s3 = new Student(3L, "杨紫", 17, "北京"); Student s4 = new Student(4L, "李现", 17, "浙江"); List&lt;Student&gt; students = new ArrayList&lt;&gt;(); students.add(s1); students.add(s2); students.add(s3); students.add(s4); Student minS = students.stream().min((stu1,stu2) -&gt;Integer.compare(stu1.getAge(),stu2.getAge())).get(); System.out.println(minS.toString());&#125; 运行结果： 上面是求所有学生中年龄最小的一个，max同理，求最大值。 anyMatch/allMatch/noneMatch（匹配）123456789101112131415161718192021222324252627public static void main(String [] args) &#123; testMatch();&#125;private static void testMatch() &#123; Student s1 = new Student(1L, "肖战", 15, "浙江"); Student s2 = new Student(2L, "王一博", 15, "湖北"); Student s3 = new Student(3L, "杨紫", 17, "北京"); Student s4 = new Student(4L, "李现", 17, "浙江"); List&lt;Student&gt; students = new ArrayList&lt;&gt;(); students.add(s1); students.add(s2); students.add(s3); students.add(s4); Boolean anyMatch = students.stream().anyMatch(s -&gt;"湖北".equals(s.getAddress())); if (anyMatch) &#123; System.out.println("有湖北人"); &#125; Boolean allMatch = students.stream().allMatch(s -&gt; s.getAge()&gt;=15); if (allMatch) &#123; System.out.println("所有学生都满15周岁"); &#125; Boolean noneMatch = students.stream().noneMatch(s -&gt; "杨洋".equals(s.getName())); if (noneMatch) &#123; System.out.println("没有叫杨洋的同学"); &#125;&#125; 运行结果 anyMatch：Stream 中任意一个元素符合传入的 predicate，返回 true allMatch：Stream 中全部元素符合传入的 predicate，返回 true noneMatch：Stream 中没有一个元素符合传入的 predicate，返回 true 总结上面介绍了Stream常用的一些方法，虽然对集合的遍历和操作可以用以前常规的方式，但是当业务逻辑复杂的时候，你会发现代码量很多，可读性很差，明明一行代码解决的事情，你却写了好几行。试试lambda表达式，试试Stream，你会有不一样的体验。 参考原文出处：https://juejin.im/post/5d5e2616f265da03b638b28a]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息队列7大问]]></title>
    <url>%2F2019%2F09%2F07%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%977%E5%A4%A7%E9%97%AE%2F</url>
    <content type="text"><![CDATA[为什么要使用消息队列?分析:一个用消息队列的人，不知道为啥用，有点尴尬。没有复习这点，很容易被问蒙，然后就开始胡扯了。 回答:这个问题,咱只答三个最主要的应用场景(不可否认还有其他的，但是只答三个主要的),即以下六个字:解耦、异步、削峰 解耦传统模式: 传统模式的缺点： 系统间耦合性太强，如上图所示，系统A在代码中直接调用系统B和系统C的代码，如果将来D系统接入，系统A还需要修改代码，过于麻烦！ 中间件模式: 中间件模式的的优点： 将消息写入消息队列，需要消息的系统自己从消息队列中订阅，从而系统A不需要做任何修改。 异步传统模式: 传统模式的缺点： 一些非必要的业务逻辑以同步的方式运行，太耗费时间。 中间件模式: 中间件模式的的优点： 将消息写入消息队列，非必要的业务逻辑以异步的方式运行，加快响应速度 削峰传统模式 传统模式的缺点： 并发量大的时候，所有的请求直接怼到数据库，造成数据库连接异常 中间件模式: 中间件模式的的优点： 系统A慢慢的按照数据库能处理的并发量，从消息队列中慢慢拉取消息。在生产中，这个短暂的高峰期积压是允许的。 使用了消息队列会有什么缺点?分析:一个使用了MQ的项目，如果连这个问题都没有考虑过，就把MQ引进去了，那就给自己的项目带来了风险。 我们引入一个技术，要对这个技术的弊端有充分的认识，才能做好预防。要记住，不要给公司挖坑！ 回答:回答也很容易，从以下两个个角度来答 系统可用性降低: 你想啊，本来其他系统只要运行好好的，那你的系统就是正常的。 现在你非要加个消息队列进去，那消息队列挂了，你的系统不是呵呵了。因此，系统可用性降低 系统复杂性增加: 要多考虑很多方面的问题，比如一致性问题、如何保证消息不被重复消费，如何保证保证消息可靠传输。 因此，需要考虑的东西更多，系统复杂性增大。 但是，我们该用还是要用的。 消息队列如何选型?先说一下，博主只会ActiveMQ,RabbitMQ,RocketMQ,Kafka，对什么ZeroMQ等其他MQ没啥理解，因此只能基于这四种MQ给出回答。 分析:既然在项目中用了MQ，肯定事先要对业界流行的MQ进行调研，如果连每种MQ的优缺点都没了解清楚，就拍脑袋依据喜好，用了某种MQ，还是给项目挖坑。 如果面试官问:”你为什么用这种MQ？。”你直接回答”领导决定的。”这种回答就很LOW了。 还是那句话，不要给公司挖坑。 RabbitMQ版本发布比ActiveMq频繁很多。至于RocketMQ和kafka就不带大家看了，总之也比ActiveMQ活跃的多。详情，可自行查阅。 再来一个性能对比表 特性 ActiveMQ RabbitMQ RocketMQ kafka 开发语言 java erlang java scala 单机吞吐量 万级 万级 10万级 10万级 时效性 ms级 us级 ms级 ms级以内 可用性 高(主从架构) 高(主从架构) 非常高(分布式架构) 非常高(分布式架构) 功能特性 成熟的产品，在很多公司得到应用；有较多的文档；各种协议支持较好 基于erlang开发，所以并发能力很强，性能极其好，延时很低;管理界面较丰富 MQ功能比较完备，扩展性佳 只支持主要的MQ功能，像一些消息查询，消息回溯等功能没有提供，毕竟是为大数据准备的，在大数据领域应用广。 综合上面的材料得出以下两点:(1)中小型软件公司，建议选RabbitMQ. 一方面，erlang语言天生具备高并发的特性，而且他的管理界面用起来十分方便。正所谓，成也萧何，败也萧何！他的弊端也在这里，虽然RabbitMQ是开源的，然而国内有几个能定制化开发erlang的程序员呢？所幸，RabbitMQ的社区十分活跃，可以解决开发过程中遇到的bug，这点对于中小型公司来说十分重要。不考虑rocketmq和kafka的原因是，一方面中小型软件公司不如互联网公司，数据量没那么大，选消息中间件，应首选功能比较完备的，所以kafka排除。不考虑rocketmq的原因是，rocketmq是阿里出品，如果阿里放弃维护rocketmq，中小型公司一般抽不出人来进行rocketmq的定制化开发，因此不推荐。(2)大型软件公司，根据具体使用在rocketMq和kafka之间二选一。 一方面，大型软件公司，具备足够的资金搭建分布式环境，也具备足够大的数据量。针对rocketMQ,大型软件公司也可以抽出人手对rocketMQ进行定制化开发，毕竟国内有能力改JAVA源码的人，还是相当多的。至于kafka，根据业务场景选择，如果有日志采集功能，肯定是首选kafka了。具体该选哪个，看使用场景。 如何保证消息队列是高可用的？分析:在第二点说过了，引入消息队列后，系统的可用性下降。在生产中，没人使用单机模式的消息队列。因此，作为一个合格的程序员，应该对消息队列的高可用有很深刻的了解。如果面试的时候，面试官问，你们的消息中间件如何保证高可用的？你的回答只是表明自己只会订阅和发布消息，面试官就会怀疑你是不是只是自己搭着玩，压根没在生产用过。请做一个爱思考，会思考，懂思考的程序员。回答:这问题，其实要对消息队列的集群模式要有深刻了解，才好回答。以rcoketMQ为例，他的集群就有多master 模式、多master多slave异步复制模式、多 master多slave同步双写模式。多master多slave模式部署架构图(网上找的,偷个懒，懒得画): 其实博主第一眼看到这个图，就觉得和kafka好像，只是NameServer集群，在kafka中是用zookeeper代替，都是用来保存和发现master和slave用的。 通信过程如下: Producer 与 NameServer集群中的其中一个节点（随机选择）建立长连接，定期从 NameServer 获取 Topic 路由信息，并向提供 Topic 服务的 Broker Master 建立长连接，且定时向 Broker 发送心跳。 Producer 只能将消息发送到 Broker master，但是 Consumer 则不一样，它同时和提供 Topic 服务的 Master 和 Slave建立长连接，既可以从 Broker Master 订阅消息，也可以从 Broker Slave 订阅消息。 那么kafka呢,为了对比说明直接上kafka的拓补架构图(也是找的，懒得画) 如上图所示，一个典型的Kafka集群中包含若干Producer（可以是web前端产生的Page View，或者是服务器日志，系统CPU、Memory等），若干broker（Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干Consumer Group，以及一个Zookeeper集群。 Kafka通过Zookeeper管理集群配置，选举leader，以及在Consumer Group发生变化时进行rebalance。 Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。 至于rabbitMQ,也有普通集群和镜像集群模式，自行去了解，比较简单，两小时即懂。 要求，在回答高可用的问题时，应该能逻辑清晰的画出自己的MQ集群架构或清晰的叙述出来。 如何保证消息不被重复消费？分析:这个问题其实换一种问法就是，如何保证消息队列的幂等性? 这个问题可以认为是消息队列领域的基本问题。换句话来说，是在考察你的设计能力，这个问题的回答可以根据具体的业务场景来答，没有固定的答案。 回答:先来说一下为什么会造成重复消费? 其实无论是那种消息队列，造成重复消费原因其实都是类似的。 正常情况下，消费者在消费消息时候，消费完毕后，会发送一个确认信息给消息队列，消息队列就知道该消息被消费了，就会将该消息从消息队列中删除。只是不同的消息队列发送的确认信息形式不同 例如RabbitMQ是发送一个ACK确认消息，RocketMQ是返回一个CONSUME_SUCCESS成功标志，kafka实际上有个offset的概念 简单说一下(如果还不懂，出门找一个kafka入门到精通教程),就是每一个消息都有一个offset，kafka消费过消息后，需要提交offset，让消息队列知道自己已经消费过了。 那造成重复消费的原因? 就是因为网络传输等等故障，确认信息没有传送到消息队列，导致消息队列不知道自己已经消费过该消息了，再次将该消息分发给其他的消费者。 如何解决?这个问题针对业务场景来答分以下几点 (1)比如，你拿到这个消息做数据库的insert操作。 那就容易了，给这个消息做一个唯一主键，那么就算出现重复消费的情况，就会导致主键冲突，避免数据库出现脏数据。 (2)再比如，你拿到这个消息做redis的set的操作 那就容易了，不用解决。因为你无论set几次结果都是一样的，set操作本来就算幂等操作。 (3)如果上面两种情况还不行，上大招。 准备一个第三方介质,来做消费记录。以redis为例，给消息分配一个全局id，只要消费过该消息，将以K-V形式写入redis。那消费者开始消费前，先去redis中查询有没消费记录即可。 如何保证消费的可靠性传输?分析:我们在使用消息队列的过程中，应该做到消息不能多消费，也不能少消费。如果无法做到可靠性传输，可能给公司带来千万级别的财产损失。 同样的，如果可靠性传输在使用过程中，没有考虑到，这不是给公司挖坑么，你可以拍拍屁股走了，公司损失的钱，谁承担。 还是那句话，认真对待每一个项目，不要给公司挖坑 回答:其实这个可靠性传输，每种MQ都要从三个角度来分析:生产者弄丢数据、消息队列弄丢数据、消费者弄丢数据 RabbitMQ生产者丢数据从生产者弄丢数据这个角度来看，RabbitMQ提供transaction和confirm模式来确保生产者不丢消息。 transaction机制就是说，发送消息前，开启事物(channel.txSelect())，然后发送消息，如果发送过程中出现什么异常，事物就会回滚(channel.txRollback())，如果发送成功则提交事物(channel.txCommit())。 然而缺点就是吞吐量下降了。因此，按照博主的经验，生产上用confirm模式的居多。 一旦channel进入confirm模式，所有在该信道上面发布的消息都将会被指派一个唯一的ID(从1开始) 一旦消息被投递到所有匹配的队列之后，rabbitMQ就会发送一个Ack给生产者(包含消息的唯一ID) 这就使得生产者知道消息已经正确到达目的队列了.如果rabiitMQ没能处理该消息，则会发送一个Nack消息给你，你可以进行重试操作。 处理Ack和Nack的代码如下所示（说好不上代码的，偷偷上了）: 12345678910111213141516171819channel.addConfirmListener(new ConfirmListener() &#123; @Override public void handleNack(long deliveryTag, boolean multiple) throws IOException &#123; System.out.println("nack: deliveryTag = "+deliveryTag+" multiple: "+multiple); &#125; @Override public void handleAck(long deliveryTag, boolean multiple) throws IOException &#123; System.out.println("ack: deliveryTag = "+deliveryTag+" multiple: "+multiple); &#125;&#125;); 消息队列丢数据处理消息队列丢数据的情况，一般是开启持久化磁盘的配置。 这个持久化配置可以和confirm机制配合使用，你可以在消息持久化磁盘后，再给生产者发送一个Ack信号。 这样，如果消息持久化磁盘之前，rabbitMQ阵亡了，那么生产者收不到Ack信号，生产者会自动重发。 那么如何持久化呢，这里顺便说一下吧，其实也很容易，就下面两步 1、将queue的持久化标识durable设置为true,则代表是一个持久的队列 2、发送消息的时候将deliveryMode=2 这样设置以后，rabbitMQ就算挂了，重启后也能恢复数据 消费者丢数据消费者丢数据一般是因为采用了自动确认消息模式。 这种模式下，消费者会自动确认收到信息。这时rahbitMQ会立即将消息删除，这种情况下如果消费者出现异常而没能处理该消息，就会丢失该消息。 至于解决方案，采用手动确认消息即可。 kafka Producer在发布消息到某个Partition时，先通过ZooKeeper找到该Partition的Leader 然后无论该Topic的Replication Factor为多少（也即该Partition有多少个Replica），Producer只将该消息发送到该Partition的Leader。 Leader会将该消息写入其本地Log。每个Follower都从Leader中pull数据。针对上述情况，得出如下分析 生产者丢数据在kafka生产中，基本都有一个leader和多个follwer。follwer会去同步leader的信息。 因此，为了避免生产者丢数据，做如下两点配置 第一个配置要在producer端设置acks=all。这个配置保证了，follwer同步完成后，才认为消息发送成功。 在producer端设置retries=MAX，一旦写入失败，这无限重试 消息队列丢数据针对消息队列丢数据的情况，无外乎就是，数据还没同步，leader就挂了，这时zookpeer会将其他的follwer切换为leader,那数据就丢失了。 针对这种情况，应该做两个配置。 replication.factor参数，这个值必须大于1，即要求每个partition必须有至少2个副本 min.insync.replicas参数，这个值必须大于1，这个是要求一个leader至少感知到有至少一个follower还跟自己保持联系 这两个配置加上上面生产者的配置联合起来用，基本可确保kafka不丢数据 消费者丢数据这种情况一般是自动提交了offset，然后你处理程序过程中挂了。kafka以为你处理好了。 再强调一次offset是干嘛的 offset：指的是kafka的topic中的每个消费组消费的下标。 简单的来说就是一条消息对应一个offset下标，每次消费数据的时候如果提交offset，那么下次消费就会从提交的offset加一那里开始消费。 比如一个topic中有100条数据，我消费了50条并且提交了，那么此时的kafka服务端记录提交的offset就是49(offset从0开始)，那么下次消费的时候offset就从50开始消费。 解决方案也很简单，改成手动提交即可。 ActiveMQ和RocketMQ大家自行查阅吧 如何保证消息的顺序性？分析:其实并非所有的公司都有这种业务需求，但是还是对这个问题要有所复习。 回答:针对这个问题，通过某种算法，将需要保持先后顺序的消息放到同一个消息队列中(kafka中就是partition,rabbitMq中就是queue)。然后只用一个消费者去消费该队列。 有的人会问:那如果为了吞吐量，有多个消费者去消费怎么办？ 这个问题，没有固定回答的套路。比如我们有一个微博的操作，发微博、写评论、删除微博，这三个异步操作。如果是这样一个业务场景，那只要重试就行。 比如你一个消费者先执行了写评论的操作，但是这时候，微博都还没发，写评论一定是失败的，等一段时间。等另一个消费者，先执行写评论的操作后，再执行，就可以成功。 总之，针对这个问题，我的观点是保证入队有序就行，出队以后的顺序交给消费者自己去保证，没有固定套路。 总结写到这里，希望读者把本文提出的这几个问题，经过深刻的准备后，一般来说，能囊括大部分的消息队列的知识点。 如果面试官不问这几个问题怎么办，简单，自己把几个问题讲清楚，突出以下自己考虑的全面性。 最后，其实我不太提倡这样突击复习，希望大家打好基本功，做一个爱思考，懂思考，会思考的程序员。 参考原文出处：https://www.cnblogs.com/williamjie/p/9481780.html]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java项目应该如何分层？]]></title>
    <url>%2F2019%2F09%2F07%2FJava%E9%A1%B9%E7%9B%AE%E5%BA%94%E8%AF%A5%E5%A6%82%E4%BD%95%E5%88%86%E5%B1%82%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[背景说起应用分层，大部分人都会认为这个不是很简单嘛 就controller，service, mapper三层。看起来简单，很多人其实并没有把他们职责划分开，在很多代码中,controller做的逻辑比service还多,service往往当成透传了，这其实是很多人开发代码都没有注意到的地方，反正功能也能用，至于放哪无所谓呗。这样往往造成后面代码无法复用，层级关系混乱，对后续代码的维护非常麻烦。 的确在这些人眼中分层只是一个形式，前辈们的代码这么写的，其他项目代码这么写的，那么我也这么跟着写。但是在真正的团队开发中每个人的习惯都不同，写出来的代码必然带着自己的标签，有的人习惯controller写大量的业务逻辑，有的人习惯在service中之间调用远程服务，这样就导致了每个人的开发代码风格完全不同，后续其他人修改的时候，一看，我靠这个人写的代码和我平常的习惯完全不同，修改的时候到底是按着自己以前的习惯改，还是跟着前辈们走，这又是个艰难的选择，选择一旦有偏差，你的后辈又维护你的代码的时候，恐怕就要骂人了。 所以一个好的应用分层需要具备以下几点: 方便后续代码进行维护扩展。 分层的效果需要让整个团队都接受 各个层职责边界清晰 如何进行分层阿里规范在阿里的编码规范中约束的分层如下: 开放接口层:可直接封装 Service 方法暴露成 RPC 接口;通过 Web 封装成 http 接口;进行 网关安全控制、流量控制等。 终端显示层:各个端的模板渲染并执行显示的层。当前主要是 velocity 渲染，JS 渲染， JSP 渲染，移动端展示等。 Web 层:主要是对访问控制进行转发，各类基本参数校验，或者不复用的业务简单处理等。 Service 层:相对具体的业务逻辑服务层。 Manager 层:通用业务处理层，它有如下特征:1. 对第三方平台封装的层，预处理返回结果及转化异常信息;2. 对Service层通用能力的下沉，如缓存方案、中间件通用处理;3. 与DAO层交互，对多个DAO的组合复用。 DAO 层:数据访问层，与底层 MySQL、Oracle、Hbase 进行数据交互。 阿里巴巴规约中的分层比较清晰简单明了，但是描述得还是过于简单了，以及service层和manager层有很多同学还是有点分不清楚之间的关系，就导致了很多项目中根本没有Manager层的存在。下面介绍一下具体业务中应该如何实现分层 优化分层从我们的业务开发中总结了一个较为的理想模型,这里要先说明一下由于我们的rpc框架选用的是thrift可能会比其他的一些rpc框架例如dubbo会多出一层,作用和controller层类似 1.最上层controller和TService是我们阿里分层规范里面的第一层:轻业务逻辑，参数校验，异常兜底。通常这种接口可以轻易更换接口类型,所以业务逻辑必须要轻，甚至不做具体逻辑。 2.Service：业务层，复用性较低，这里推荐每一个controller方法都得对应一个service,不要把业务编排放在controller中去做，为什么呢？如果我们把业务编排放在controller层去做的话，如果以后我们要接入thrift,我们这里又需要把业务编排在做一次，这样会导致我们每接入一个入口层这个代码都得重新复制一份如下图所示: 这样大量的重复工作必定会导致我们开发效率下降，所以我们需要把业务编排逻辑都得放进service中去做: 3.Mannager：可复用逻辑层。这里的Mannager可以是单个服务的，比如我们的cache,mq等等，当然也可以是复合的，当你需要调用多个Mannager的时候，这个可以合为一个Mannager，比如逻辑上的连表查询等。如果是httpMannager或rpcMannager需要在这一层做一些数据转换 4.DAO：数据库访问层。主要负责“操作数据库的某张表，映射到某个java对象”，dao应该只允许自己的Service访问，其他Service要访问我的数据必须通过对应的Service。 分层领域模型的转换在阿里巴巴编码规约中列举了下面几个领域模型规约: DO（Data Object）：与数据库表结构一一对应，通过DAO层向上传输数据源对象。 DTO（Data Transfer Object）：数据传输对象，Service或Manager向外传输的对象。 BO（Business Object）：业务对象。由Service层输出的封装业务逻辑的对象。 AO（Application Object）：应用对象。在Web层与Service层之间抽象的复用对象模型，极为贴近展示层，复用度不高。 VO（View Object）：显示层对象，通常是Web向模板渲染引擎层传输的对象。 Query：数据查询对象，各层接收上层的查询请求。注意超过2个参数的查询封装，禁止使用Map类来传输。 层次 领域模型 Controller/TService VO/DTO Service/Manager AO/BO DAO DO 每一个层基本都自己对应的领域模型，这样就导致了有些人过于追求每一层都是用自己的领域模型，这样就导致了一个对象可能会出现3次甚至4次转换在一次请求中，当返回的时候同样也会出现3-4次转换，这样有可能一次完整的请求-返回会出现很多次对象转换。如果在开发中真的按照这么来，恐怕就别写其他的了，一天就光写这个重复无用的逻辑算了吧。 所以我们得采取一个折中的方案: 1.允许Service/Manager可以操作数据领域模型，对于这个层级来说，本来自己做的工作也是做的是业务逻辑处理和数据组装。 2.Controller/TService层的领域模型不允许传入DAO层，这样就不符合职责划分了。 3.同理，不允许DAO层的数据传入到Controller/TService. 总结总的来说业务分层对于代码规范是比较重要，决定着以后的代码是否可复用，是否职责清晰，边界清晰。 当然这种分层其实见仁见智, 团队中的所有人的分层习惯也不同，所以很难权衡出一个标准的准则，总的来说只要满足职责逻辑清晰，后续维护容易，就是好的分层。 最后，如果你的团队有更好的分层，或者上面所描述的有什么错误的地方还请留言指正一下。 参考原文出处：https://juejin.im/post/5b44e62e6fb9a04fc030f216]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[InnoDB一棵B+树可以存放多少行数据？]]></title>
    <url>%2F2019%2F09%2F07%2FInnoDB%E4%B8%80%E6%A3%B5B-%E6%A0%91%E5%8F%AF%E4%BB%A5%E5%AD%98%E6%94%BE%E5%A4%9A%E5%B0%91%E8%A1%8C%E6%95%B0%E6%8D%AE%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[一个问题？InnoDB一棵B+树可以存放多少行数据？这个问题的简单回答是：约2千万。为什么是这么多呢？因为这是可以算出来的，要搞清楚这个问题，我们先从InnoDB索引数据结构、数据组织方式说起。 我们都知道计算机在存储数据的时候，有最小存储单元，这就好比我们今天进行现金的流通最小单位是一毛。在计算机中磁盘存储数据最小单元是扇区，一个扇区的大小是512字节，而文件系统（例如XFS/EXT4）他的最小单元是块，一个块的大小是4k，而对于我们的InnoDB存储引擎也有自己的最小储存单元——页（Page），一个页的大小是16K。 下面几张图可以帮你理解最小存储单元： 文件系统中一个文件大小只有1个字节，但不得不占磁盘上4KB的空间。 innodb的所有数据文件（后缀为ibd的文件），他的大小始终都是16384（16k）的整数倍。 磁盘扇区、文件系统、InnoDB存储引擎都有各自的最小存储单元。 在MySQL中我们的InnoDB页的大小默认是16k，当然也可以通过参数设置： 12345678910111213mysql&gt; show variables like &apos;innodb_page_size&apos;;+------------------+-------+| Variable_name | Value |+------------------+-------+| innodb_page_size | 16384 |+------------------+-------+1 row in set (0.00 sec) 数据表中的数据都是存储在页中的，所以一个页中能存储多少行数据呢？假设一行数据的大小是1k，那么一个页可以存放16行这样的数据。 如果数据库只按这样的方式存储，那么如何查找数据就成为一个问题，因为我们不知道要查找的数据存在哪个页中，也不可能把所有的页遍历一遍，那样太慢了。所以人们想了一个办法，用B+树的方式组织这些数据。如图所示： 我们先将数据记录按主键进行排序，分别存放在不同的页中（为了便于理解我们这里一个页中只存放3条记录，实际情况可以存放很多），除了存放数据的页以外，还有存放键值+指针的页，如图中page number=3的页，该页存放键值和指向数据页的指针，这样的页由N个键值+指针组成。当然它也是排好序的。这样的数据组织形式，我们称为索引组织表。现在来看下，要查找一条数据，怎么查？ 如select * from user where id=5; 这里id是主键,我们通过这棵B+树来查找，首先找到根页，你怎么知道user表的根页在哪呢？其实每张表的根页位置在表空间文件中是固定的，即page number=3的页（这点我们下文还会进一步证明），找到根页后通过二分查找法，定位到id=5的数据应该在指针P5指向的页中，那么进一步去page number=5的页中查找，同样通过二分查询法即可找到id=5的记录： 5 zhao2 27 现在我们清楚了InnoDB中主键索引B+树是如何组织数据、查询数据的，我们总结一下： 1、InnoDB存储引擎的最小存储单元是页，页可以用于存放数据也可以用于存放键值+指针，在B+树中叶子节点存放数据，非叶子节点存放键值+指针。 2、索引组织表通过非叶子节点的二分查找法以及指针确定数据在哪个页中，进而在去数据页中查找到需要的数据； 那么回到我们开始的问题，通常一棵B+树可以存放多少行数据？ 这里我们先假设B+树高为2，即存在一个根节点和若干个叶子节点，那么这棵B+树的存放总记录数为：根节点指针数*单个叶子节点记录行数。 上文我们已经说明单个叶子节点（页）中的记录数=16K/1K=16。（这里假设一行记录的数据大小为1k，实际上现在很多互联网业务数据记录大小通常就是1K左右）。 那么现在我们需要计算出非叶子节点能存放多少指针，其实这也很好算，我们假设主键ID为bigint类型，长度为8字节，而指针大小在InnoDB源码中设置为6字节，这样一共14字节，我们一个页中能存放多少这样的单元，其实就代表有多少指针，即16384/14=1170。那么可以算出一棵高度为2的B+树，能存放1170*16=18720条这样的数据记录。 根据同样的原理我们可以算出一个高度为3的B+树可以存放：1170117016=21902400条这样的记录。所以在InnoDB中B+树高度一般为1-3层，它就能满足千万级的数据存储。在查找数据时一次页的查找代表一次IO，所以通过主键索引查询通常只需要1-3次IO操作即可查找到数据。 怎么得到InnoDB主键索引B+树的高度？上面我们通过推断得出B+树的高度通常是1-3，下面我们从另外一个侧面证明这个结论。在InnoDB的表空间文件中，约定page number为3的代表主键索引的根页，而在根页偏移量为64的地方存放了该B+树的page level。如果page level为1，树高为2，page level为2，则树高为3。即B+树的高度=page level+1；下面我们将从实际环境中尝试找到这个page level。 在实际操作之前，你可以通过InnoDB元数据表确认主键索引根页的page number为3，你也可以从《InnoDB存储引擎》这本书中得到确认。 1234567SELECTb.name, a.name, index_id, type, a.space, a.PAGE_NOFROMinformation_schema.INNODB_SYS_INDEXES a,information_schema.INNODB_SYS_TABLES bWHEREa.table_id = b.table_id AND a.space &lt;&gt; 0; 执行结果： 可以看出数据库dbt3下的customer表、lineitem表主键索引根页的page number均为3，而其他的二级索引page number为4。关于二级索引与主键索引的区别请参考MySQL相关书籍，本文不在此介绍。 下面我们对数据库表空间文件做想相关的解析： 因为主键索引B+树的根页在整个表空间文件中的第3个页开始，所以可以算出它在文件中的偏移量：16384*3=49152（16384为页大小）。 另外根据《InnoDB存储引擎》中描述在根页的64偏移量位置前2个字节，保存了page level的值，因此我们想要的page level的值在整个文件中的偏移量为：16384*3+64=49152+64=49216，前2个字节中。 接下来我们用hexdump工具，查看表空间文件指定偏移量上的数据： linetem表的page level为2，B+树高度为page level+1=3； region表的page level为0，B+树高度为page level+1=1； customer表的page level为2，B+树高度为page level+1=3； 这三张表的数据量如下： 小结： lineitem表的数据行数为600多万，B+树高度为3，customer表数据行数只有15万，B+树高度也为3。可以看出尽管数据量差异较大，这两个表树的高度都是3，换句话说这两个表通过索引查询效率并没有太大差异，因为都只需要做3次IO。那么如果有一张表行数是一千万，那么他的B+树高度依旧是3，查询效率仍然不会相差太大。 region表只有5行数据，当然他的B+树高度为1。 最后回顾一道面试题有一道MySQL的面试题，为什么MySQL的索引要使用B+树而不是其它树形结构?比如B树？ 现在这个问题的复杂版本可以参考本文； 他的简单版本回答是： 因为B树不管叶子节点还是非叶子节点，都会保存数据，这样导致在非叶子节点中能保存的指针数量变少（有些资料也称为扇出），指针少的情况下要保存大量数据，只能增加树的高度，导致IO操作变多，查询性能变低； 总结本文从一个问题出发，逐步介绍了InnoDB索引组织表的原理、查询方式，并结合已有知识，回答该问题，结合实践来证明。当然为了表述简单易懂，文中忽略了一些细枝末节，比如一个页中不可能所有空间都用于存放数据，它还会存放一些少量的其他字段比如page level，index number等等，另外还有页的填充因子也导致一个页不可能全部用于保存数据。关于二级索引数据存取方式可以参考MySQL相关书籍，他的要点是结合主键索引进行回表查询。 参考原文出处：https://www.cnblogs.com/leefreeman/p/8315844.html?from=singlemessage&amp;isappinstalled=0 姜承尧 《MySQL技术内幕:InnoDB存储引擎》 姜承尧 http://www.innomysql.com/查看-innodb表中每个的索引高度/]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Redis的7个问题]]></title>
    <url>%2F2019%2F08%2F11%2F%E5%85%B3%E4%BA%8ERedis%E7%9A%847%E4%B8%AA%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Hello，Redis!我们相处已经很多年了，从模糊的认识到现在我们已经深入结合，你的好我一直都知道也一直都记住 能否再让我多问你的几个问题，让我更加深入的去了解你。 Redis 的通讯协议是什么 Redis 的通讯协议是文本协议，是的，Redis 服务器与客户端通过 RESP(Redis Serialization Protocol)协议通信。 没错，文本协议确实是会浪费流量，不过它的优点在于直观，非常的简单，解析性能极其的好 我们不需要一个特殊的 Redis 客户端仅靠 Telnet 或者是文本流就可以跟 Redis 进行通讯。 客户端的命令格式： 简单字符串 Simple Strings，以 “+”加号开头。 错误 Errors，以”-“减号开头。 整数型 Integer，以 “:” 冒号开头。 大字符串类型 Bulk Strings，以 “$”美元符号开头。 数组类型 Arrays，以 “*”星号开头。 12set hello abc一个简单的文本流就可以是redis的客户端 简单总结：具体可以见： https://redis.io/topics/protocol ， Redis 文档认为简单的实现，快速的解析，直观理解是采用 RESP 文本协议最重要的地方，有可能文本协议会造成一定量的流量浪费，但却在性能上和操作上快速简单，这中间也是一个权衡和协调的过程。 Redis 究竟有没有 ACID 事务 要弄清楚 Redis 有没有事务，其实很简单，上 Rredis 的官网查看文档，发现： Redis 确实是有事务，不过按照传统的事务定义 ACID 来看，Redis 是不是都具备了 ACID 的特性。 ACID 指的是： 原子性 一致性 隔离性 持久性 我们将使用以上 Redis 事务的命令来检验是否 Redis 都具备了 ACID 的各个特征。 原子性事务具备原子性指的是，数据库将事务中多个操作当作一个整体来执行，服务要么执行事务中所有的操作，要么一个操作也不会执行。 事务队列首先弄清楚 Redis 开始事务 multi 命令后，Redis 会为这个事务生成一个队列，每次操作的命令都会按照顺序插入到这个队列中。 这个队列里面的命令不会被马上执行，直到 exec 命令提交事务，所有队列里面的命令会被一次性，并且排他的进行执行。 对应如下图： 从上面的例子可以看出，当执行一个成功的事务，事务里面的命令都是按照队列里面顺序的并且排他的执行。 但原子性又一个特点就是要么全部成功，要么全部失败，也就是我们传统 DB 里面说的回滚。 当我们执行一个失败的事务： 可以发现，就算中间出现了失败，set abc x 这个操作也已经被执行了，并没有进行回滚，从严格的意义上来说 Redis 并不具备原子性。 为何 Redis 不支持回滚这个其实跟 Redis 的定位和设计有关系，先看看为何我们的 MySQL 可以支持回滚 这个还是跟写 Log 有关系，Redis 是完成操作之后才会进行 AOF 日志记录，AOF 日志的定位只是记录操作的指令记录。 而 MySQL 有完善的 Redolog，并且是在事务进行 Commit 之前就会写完成 Redolog，Binlog： 要知道 MySQL 为了能进行回滚是花了不少的代价，Redis 应用的场景更多是对抗高并发具备高性能，所以 Redis 选择更简单，更快速无回滚的方式处理事务也是符合场景。 一致性事务具备一致性指的是，如果数据库在执行事务之前是一致的，那么在事务执行之后，无论事务是否成功，数据库也应该是一致的。 从 Redis 来说可以从 2 个层面看，一个是执行错误是否有确保一致性，另一个是宕机时，Redis 是否有确保一致性的机制。 执行错误是否有确保一致性 依然去执行一个错误的事务，在事务执行的过程中会识别出来并进行错误处理，这些错误并不会对数据库作出修改，也不会对事务的一致性产生影响。 宕机对一致性的影响暂不考虑分布式高可用的 Redis 解决方案，先从单机看宕机恢复是否能满意数据完整性约束。 无论是 RDB 还是 AOF 持久化方案，可以使用 RDB 文件或 AOF 文件进行恢复数据，从而将数据库还原到一个一致的状态。 再议一致性上面执行错误和宕机对一致性的影响的观点摘自黄健宏 《Redis 设计与实现》。 当在读这章的时候还是有一些存疑的点，归根到底 Redis 并非关系型数据库。 如果仅仅就 ACID 的表述上来说，一致性就是从 A 状态经过事务到达 B 状态没有破坏各种约束性，仅就 Redis 而言不谈实现的业务，那显然就是满意一致性。 但如果加上业务去谈一致性，例如，A 转账给 B，A 减少 10 块钱，B 增加 10 块钱，因为 Redis 并不具备回滚，也就不具备传统意义上的原子性，所以 Redis 也应该不具备传统的一致性。 其实，这里只是简单讨论下 Redis 在传统 ACID 上的概念怎么进行对接，或许，有可能是我想多了，用传统关系型数据库的 ACID 去审核 Redis 是没有意义的，Redis 本来就没有意愿去实现 ACID 的事务。 隔离性隔离性指的是，数据库中有多个事务并发的执行，各个事务之间不会相互影响，并且在并发状态下执行的事务和串行执行的事务产生的结果是完全相同的。 Redis 因为是单线程操作，所以在隔离性上有天生的隔离机制，当 Redis 执行事务时，Redis 的服务端保证在执行事务期间不会对事务进行中断，所以，Redis 事务总是以串行的方式运行，事务也具备隔离性。 持久性事务的持久性指的是，当一个事务执行完毕，执行这个事务所得到的结果被保存在持久化的存储中，即使服务器在事务执行完成后停机了，执行的事务的结果也不会被丢失。 Redis 是否具备持久化，这个取决于 Redis 的持久化模式： 纯内存运行，不具备持久化，服务一旦停机，所有数据将丢失。 RDB 模式，取决于 RDB 策略，只有在满足策略才会执行 Bgsave，异步执行并不能保证 Redis 具备持久化。 AOF 模式，只有将 appendfsync 设置为 always，程序才会在执行命令同步保存到磁盘，这个模式下，Redis 具备持久化。(将 appendfsync 设置为 always，只是在理论上持久化可行，但一般不会这么操作) 简单总结： Redis 具备了一定的原子性，但不支持回滚。 Redis 不具备 ACID 中一致性的概念。(或者说 Redis 在设计时就无视这点) Redis 具备隔离性。 Redis 通过一定策略可以保证持久性。 Redis 和 ACID 纯属站在使用者的角度去思想，Redis 设计更多的是追求简单与高性能，不会受制于传统 ACID 的束缚。 Redis 的乐观锁 Watch 是怎么实现的当我们一提到乐观锁就会想起 CAS(Compare And Set)，CAS 操作包含三个操作数： 内存位置的值(V) 预期原值(A) 新值(B) 如果内存位置的值与预期原值相匹配，那么处理器会自动将该位置更新为新值。否则，处理器不做任何操作。 在 Redis 的事务中使用 Watch 实现，Watch 会在事务开始之前盯住 1 个或多个关键变量。 当事务执行时，也就是服务器收到了 exec 指令要顺序执行缓存的事务队列时， Redis 会检查关键变量自 Watch 之后，是否被修改了。 Java 的 AtomicXXX 的乐观锁机制在 Java 中我们也经常的使用到一些乐观锁的参数，例如 AtomicXXX，这些机制的背后是怎么去实现的，是否 Redis 也跟 Java 的 CAS 实现机制一样? 先来看看 Java 的 Atomic 类，我们追一下源码，可以看到它的背后其实是 Unsafe_CompareAndSwapObject： 可以看见 compareAndSwapObject 是 Native 方法，需要在继续追查，可以下载源码或打开 ：http://hg.openjdk.java.net/jdk8u/。 Cmpxchg可以发现追查到最终 CAS，“比较并修改”，本来是两个语意，但是最终确实一条 CPU 指令 Cmpxchg 完成。 Cmpxchg 是一条 CPU 指令的命令而不是多条 CPU 指令，所以它不会被多线程的调度所打断，所以能够保证 CAS 的操作是一个原子操作。 当然 Cmpxchg 的机制其实存在 ABA 还有多次重试的问题，这个不在这里讨论。 Redis 的 Watch 机制Redis 的 Watch 也是使用 Cmpxchg 吗，两者存在相似之处在用法上也有一些不同 Redis 的 Watch 不存在 ABA 问题，也没有多次重试机制，其中有一个重大的不同是：Redis 事务执行其实是串行的。 简单追一下源码：摘录出来的源码可能有些凌乱，不过可以简单总结出来数据结构图和简单的流程图，之后再看源码就会清晰很多。 存储如下图： RedisDb 存放了一个 watched_keys 的 Dcit 结构，每个被 Watch 的 Key 的值是一个链表结构，存放的是一组 Redis 客户端标志。 流程如下图： 每一次 Watch，Multi，Exec 时都会去查询这个 watched_keys 结构进行判断，每次 Touch 到被 Watch 的 Key 时都会标志为 CLIENT_DIRTY_CAS。 因为在 Redis 中所有的事务都是串行的，假设有客户端 A 和客户端 B 都 Watch 同一个 Key。 当客户端 A 进行 Touch 修改或者 A 率先执行完，会把客户端 A 从这个 watched_keys 的这个 Key 的列表删除，然后把这个列表所有的客户端都设置成 CLIENT_DIRTY_CAS。 当后面的客户端 B 开始执行时，判断到自己的状态是 CLIENT_DIRTY_CAS，便 discardTransaction 终止事务。 简单总结：Cmpxchg 的实现主要是利用了 CPU 指令，看似两个操作使用一条 CPU 指令完成，所以不会被多线程进行打断。 而 Redis 的 Watch 机制，更多是利用了 Redis 本身单线程的机制，采用了 watched_keys 的数据结构和串行流程实现了乐观锁机制。 Redis 是如何持久化的Redis 的持久化有两种机制，一个是 RDB，也就是快照，快照就是一次全量的备份，会把所有 Redis 的内存数据进行二进制的序列化存储到磁盘。 另一种是 AOF 日志，AOF 日志记录的是数据操作修改的指令记录日志，可以类比 MySQL 的 Binlog，AOF 日期随着时间的推移只会无限增量。 在对 Redis 进行恢复时，RDB 快照直接读取磁盘即可恢复，而 AOF 需要对所有的操作指令进行重放进行恢复，这个过程有可能非常漫长。 RDBRedis 在进行 RDB 的快照生成有两种方法，一种是 Save，由于 Redis 是单进程单线程，直接使用 Save，Redis 会进行一个庞大的文件 IO 操作。 由于单进程单线程势必会阻塞线上的业务，一般的话不会直接采用 Save，而是采用 Bgsave，之前一直说 Redis 是单进程单线程，其实不然。 在使用 Bgsave 的时候，Redis 会 Fork 一个子进程，快照的持久化就交给子进程去处理，而父进程继续处理线上业务的请求。 Fork 机制想要弄清楚 RDB 快照的生成原理就必须弄清楚 Fork 机制，Fork 机制是 Linux 操作系统的一个进程机制。 当父进程 Fork 出来一个子进程，子进程和父进程拥有共同的内存数据结构，子进程刚刚产生时，它和父进程共享内存里面的代码段和数据段。 一开始两个进程都具备了相同的内存段，子进程在做数据持久化时，不会去修改现在的内存数据，而是会采用 COW(Copy On Write)的方式将数据段页面进行分离。 当父进程修改了某一个数据段时，被共享的页面就会复制一份分离出来，然后父进程再在新的数据段进行修改。 分裂这个过程也成为分裂的过程，本来父子进程都指向很多相同的内存块，但是如果父进程对其中某个内存块进行该修改，就会将其复制出来，进行分裂再在新的内存块上面进行修改。 因为子进程在 Fork 的时候就可以固定内存，这个时间点的数据将不会产生变化。 所以我们可以安心的产生快照不用担心快照的内容受到父进程业务请求的影响。 另外可以想象，如果在 Bgsave 的过程中，Redis 没有任何操作，父进程没有接收到任何业务请求也没有任何的背后例如过期移除等操作，父进程和子进程将会使用相同的内存块。 AOFAOF 是 Redis 操作指令的日志存储，类同于 MySQL 的 Binlog，假设 AOF 从 Redis 创建以来就一直执行，那么 AOF 就记录了所有的 Redis 指令的记录。 如果要恢复 Redis，可以对 AOF 进行指令重放，便可修复整个 Redis 实例。 不过 AOF 日志也有两个比较大的问题： 一个是 AOF 的日志会随着时间递增，如果一个数据量大运行的时间久，AOF 日志量将变得异常庞大。 另一个问题是 AOF 在做数据恢复时，由于重放的量非常庞大，恢复的时间将会非常的长。 AOF 写操作是在 Redis 处理完业务逻辑之后，按照一定的策略才会进行些 AOF 日志存盘，这点跟 MySQL 的 Redolog 和 Binlog 有很大的不同。 也因为此原因，Redis 因为处理逻辑在前而记录操作日志在后，也是导致 Redis 无法进行回滚的一个原因。 bgrewriteaof：针对上述的问题，Redis 在 2.4 之后也使用了 bgrewriteaof 对 AOF 日志进行瘦身。 bgrewriteaof 命令用于异步执行一个 AOF 文件重写操作。重写会创建一个当前 AOF 文件的体积优化版本。 RDB 和 AOF 混合搭配模式在对 Redis 进行恢复的时候，如果我们采用了 RDB 的方式，因为 Bgsave 的策略，可能会导致我们丢失大量的数据。 如果我们采用了 AOF 的模式，通过 AOF 操作日志重放恢复，重放 AOF 日志比 RDB 要长久很多。 Redis 4.0 之后，为了解决这个问题，引入了新的持久化模式，混合持久化，将 RDB 的文件和局部增量的 AOF 文件相结合。 RDB 可以使用相隔较长的时间保存策略，AOF 不需要是全量日志，只需要保存前一次 RDB 存储开始到这段时间增量 AOF 日志即可，一般来说，这个日志量是非常小的。 Redis 在内存使用上是如何开源节流Redis 跟其他传统数据库不同，Redis 是一个纯内存的数据库，并且存储了都是一些数据结构的数据 如果不对内存加以控制的话，Redis 很可能会因为数据量过大导致系统的奔溃。 Ziplist 当最开始尝试开启一个小数据量的 Hash 结构和一个 Zset 结构时，发现他们在 Redis 里面的真正结构类型是一个 Ziplist。 Ziplist 是一个紧凑的数据结构，每一个元素之间都是连续的内存，如果在 Redis 中，Redis 启用的数据结构数据量很小时，Redis 就会切换到使用紧凑存储的形式来进行压缩存储。 例如，上面的例子，我们采用了 Hash 结构进行存储，Hash 结构是一个二维的结构，是一个典型的用空间换取时间的结构。 但是如果使用的数据量很小，使用二维结构反而浪费了空间，在时间的性能上也并没有得到太大的提升，还不如直接使用一维结构进行存储。 在查找的时候，虽然复杂度是 O(n)，但是因为数据量少遍历也非常快，增至比 Hash 结构本身的查询更快。 如果当集合对象的元素不断的增加，或者某个 Value 的值过大，这种小对象存储也会升级生成标准的结构。 Redis 也可以在配置中进行定义紧凑结构和标准结构的转换参数： Quicklist Quicklist 数据结构是 Redis 在 3.2 才引入的一个双向链表的数据结构，确实来说是一个 Ziplist 的双向链表。 Quicklist 的每一个数据节点是一个 Ziplist，Ziplist 本身就是一个紧凑列表。 假使，Quicklist 包含了 5 个 Ziplist 的节点，每个 Ziplist 列表又包含了 5 个数据，那么在外部看来，这个 Quicklist 就包含了 25 个数据项。 Quicklist 的结构设计简单总结起来，是一个空间和时间的折中方案： 双向链表可以在两端进行 Push 和 Pop 操作，但是它在每一个节点除了保存自身的数据外，还要保存两个指针，增加额外的内存开销。 其次是由于每个节点都是独立的，在内存地址上并不连续，节点多了容易产生内存碎片。 Ziplist 本身是一块连续的内存，存储和查询效率很高，但是，它不利于修改操作，每次数据变动时都会引发内存 Realloc，如果 Ziplist 长度很长时，一次 Realloc 会导致大批量数据拷贝。 所以，结合 Ziplist 和双向链表的优点，Quciklist 就孕育而生。 对象共享Redis 在自己的对象系统中构建了一个引用计数方法，通过这个方法程序可以跟踪对象的引用计数信息，除了可以在适当的时候进行对象释放，还可以用来作为对象共享。 举个例子，假使键 A 创建了一个整数值 100 的字符串作为值对象，这个时候键 B 也创建保存同样整数值 100 的字符串对象作为值对象。 那么在 Redis 的操作时： 讲数据库键的指针指向一个现有的值对象。 讲被共享的值对象引用计数加一。 假使，我们的数据库中指向整数值 100 的键不止键 A 和键 B，而是有几百个，那么 Redis 服务器中只需要一个字符串对象的内存就可以保存原本需要几百个字符串对象的内存才能保存的数据。 Redis 是如何实现主从复制 几个定义： runID：服务器运行的 ID。 Offset：主服务器的复制偏移量和从服务器复制的偏移量。 Replication backlog：主服务器的复制积压缓冲区。 在 Redis 2.8 之后，使用 Psync 命令代替 Sync 命令来执行复制的同步操作。 Psync 命令具有完整重同步和部分重同步两种模式： 完整同步用于处理初次复制情况： 完整重同步的执行步骤和 Sync 命令执行步骤一致，都是通过让主服务器创建并发送 RDB 文件，以及向从服务器发送保存在缓冲区的写命令来进行同步。 部分重同步是用于处理断线后重复制情况： 当从服务器在断线后重新连接主服务器时，主服务可以将主从服务器连接断开期间执行的写命令发送给从服务器，从服务器只要接收并执行这些写命令，就可以将数据库更新至主服务器当前所处的状态。 完整重同步： Slave 发送 Psync 给 Master，由于是第一次发送，不带上 runID 和 Offset。 Master 接收到请求，发送 Master 的 runID 和 Offset 给从节点。 Master 生成保存 RDB 文件。 Master 发送 RDB 文件给 Slave。 在发送 RDB 这个操作的同时，写操作会复制到缓冲区 Replication Backlog Buffer 中，并从 Buffer 区发送到 Slave。 Slave 将 RDB 文件的数据装载，并更新自身数据。 如果网络的抖动或者是短时间的断链也需要进行完整同步就会导致大量的开销，这些开销包括了，Bgsave 的时间，RDB 文件传输的时间，Slave 重新加载 RDB 时间，如果 Slave 有 AOF，还会导致 AOF 重写。 这些都是大量的开销，所以在 Redis 2.8 之后也实现了部分重同步的机制。 部分重同步： 网络发生错误，Master 和 Slave 失去连接。 Master 依然向 Buffer 缓冲区写入数据。 Slave 重新连接上 Master。 Slave 向 Master 发送自己目前的 runID 和 Offset。 Master 会判断 Slave 发送给自己的 Offset 是否存在 Buffer 队列中。 如果存在，则发送 Continue 给 Slave;如果不存在，意味着可能错误了太多的数据，缓冲区已经被清空，这个时候就需要重新进行全量的复制。 Master 发送从 Offset 偏移后的缓冲区数据给 Slave。 Slave 获取数据更新自身数据。 Redis 是怎么制定过期删除策略的当一个键处于过期的状态，其实在 Redis 中这个内存并不是实时就被从内存中进行摘除，而是 Redis 通过一定的机制去把一些处于过期键进行移除，进而达到内存的释放 那么当一个键处于过期，Redis 会在什么时候去删除? 几时被删除存在三种可能性，这三种可能性也代表了 Redis 的三种不同的删除策略。 定时删除：在设置键过去的时间同时，创建一个定时器，让定时器在键过期时间来临，立即执行对键的删除操作。 惰性删除：放任键过期不管，但是每次从键空间获取键时，都会检查该键是否过期，如果过期的话，就删除该键。 定期删除：每隔一段时间，程序都要对数据库进行一次检查，删除里面的过期键，至于要删除多少过期键，由算法而定。 定时删除设置键的过期时间，创建定时器，一旦过期时间来临，就立即对键进行操作。 这种对内存是友好的，但是对 CPU 的时间是最不友好的，特别是在业务繁忙，过期键很多的时候，删除过期键这个操作就会占据很大一部分 CPU 的时间。 要知道 Redis 是单线程操作，在内存不紧张而 CPU 紧张的时候，将 CPU 的时间浪费在与业务无关的删除过期键上面，会对 Redis 的服务器的响应时间和吞吐量造成影响。 另外，创建一个定时器需要用到 Redis 服务器中的时间事件，而当前时间事件的实现方式是无序链表，时间复杂度为 O(n)，让服务器大量创建定时器去实现定时删除策略，会产生较大的性能影响 所以，定时删除并不是一种好的删除策略。 惰性删除与定时删除相反，惰性删除策略对 CPU 来说是最友好的，程序只有在取出键的时候才会进行检查，是一种被动的过程。 与此同时，惰性删除对内存来说又是最不友好的，一个键过期，只要不再被取出，这个过期键就不会被删除，它占用的内存也不会被释放。 很明显，惰性删除也不是一个很好的策略，Redis 是非常依赖内存和较好内存的，如果一些长期键长期没有被访问，就会造成大量的内存垃圾，甚至会操成内存的泄漏。 在对执行数据写入时，通过 expireIfNeeded 函数对写入的 Key 进行过期判断。 其中 expireIfNeeded 在内部做了三件事情，分别是： 查看 Key 是否过期。 向 Slave 节点传播执行过去 Key 的动作。 删除过期 Key。 定期删除上面两种删除策略，无论是定时删除和惰性删除，这两种删除方式在单一的使用上都存在明显的缺陷，要么占用太多 CPU 时间，要么浪费太多内存。 定期删除策略是前两种策略的一个整合和折中： 定期删除策略每隔一段时间执行一次删除过期键操作，并通过限制删除操作执行的时间和频率来减少删除操作对 CPU 时间的影响。 通过合理的删除执行的时长和频率，来达到合理的删除过期键。 总结Redis 可谓博大精深，简单的七连问只是盲人摸象，这次只是摸到了一根象鼻子，还应该顺着鼻子向下摸，下次可能摸到了一只象耳朵。 只要愿意往下深入去了解去摸索，而不只应用不思考，总有一天会把 Redis 这只大象给摸透了。 参考原文作者：陈于喆 https://mp.weixin.qq.com/s/LGkS_2wkcXhThfOiRRQC7Q 黄健宏 《Redis 设计与实现》]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>内存</tag>
        <tag>线程</tag>
        <tag>数据结构</tag>
        <tag>Redis</tag>
        <tag>CPU</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[秒杀系统架构分析与实战]]></title>
    <url>%2F2019%2F08%2F10%2F%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90%E4%B8%8E%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[秒杀业务分析正常电子商务流程 （1）查询商品； （2）创建订单； （3）扣减库存； （4）更新订单； （5）付款； （6）卖家发货； 秒杀业务的特性 （1）低廉价格； （2）大幅推广； （3）瞬时售空； （4）一般是定时上架； （5）时间短、瞬时并发量高； 秒杀技术挑战假设某网站秒杀活动只推出一件商品，预计会吸引1万人参加活动，也就说最大并发请求数是10000，秒杀系统需要面对的技术挑战有： 对现有网站业务造成冲击秒杀活动只是网站营销的一个附加活动，这个活动具有时间短，并发访问量大的特点，如果和网站原有应用部署在一起，必然会对现有业务造成冲击，稍有不慎可能导致整个网站瘫痪。 解决方案：将秒杀系统独立部署，甚至使用独立域名，使其与网站完全隔离。 高并发下的应用、数据库负载用户在秒杀开始前，通过不停刷新浏览器页面以保证不会错过秒杀，这些请求如果按照一般的网站应用架构，访问应用服务器、连接数据库，会对应用服务器和数据库服务器造成负载压力。 解决方案：重新设计秒杀商品页面，不使用网站原来的商品详细页面，页面内容静态化，用户请求不需要经过应用服务。 突然增加的网络及服务器带宽假设商品页面大小200K（主要是商品图片大小），那么需要的网络和服务器带宽是2G（200K×10000），这些网络带宽是因为秒杀活动新增的，超过网站平时使用的带宽。 解决方案：因为秒杀新增的网络带宽，必须和运营商重新购买或者租借。为了减轻网站服务器的压力，需要将秒杀商品页面缓存在CDN，同样需要和CDN服务商临时租借新增的出口带宽。 直接下单秒杀的游戏规则是到了秒杀才能开始对商品下单购买，在此时间点之前，只能浏览商品信息，不能下单。而下单页面也是一个普通的URL，如果得到这个URL，不用等到秒杀开始就可以下单了。 解决方案：为了避免用户直接访问下单页面URL，需要将改URL动态化，即使秒杀系统的开发者也无法在秒杀开始前访问下单页面的URL。办法是在下单页面URL加入由服务器端生成的随机数作为参数，在秒杀开始的时候才能得到。 如何控制秒杀商品页面购买按钮的点亮购买按钮只有在秒杀开始的时候才能点亮，在此之前是灰色的。如果该页面是动态生成的，当然可以在服务器端构造响应页面输出，控制该按钮是灰色还是点亮，但是为了减轻服务器端负载压力，更好地利用CDN、反向代理等性能优化手段，该页面被设计为静态页面，缓存在CDN、反向代理服务器上，甚至用户浏览器上。秒杀开始时，用户刷新页面，请求根本不会到达应用服务器。 解决方案：使用JavaScript脚本控制，在秒杀商品静态页面中加入一个JavaScript文件引用，该JavaScript文件中包含秒杀开始标志为否；当秒杀开始的时候生成一个新的JavaScript文件（文件名保持不变，只是内容不一样），更新秒杀开始标志为是，加入下单页面的URL及随机数参数（这个随机数只会产生一个，即所有人看到的URL都是同一个，服务器端可以用redis这种分布式缓存服务器来保存随机数），并被用户浏览器加载，控制秒杀商品页面的展示。这个JavaScript文件的加载可以加上随机版本号（例如xx.js?v=32353823），这样就不会被浏览器、CDN和反向代理服务器缓存。 这个JavaScript文件非常小，即使每次浏览器刷新都访问JavaScript文件服务器也不会对服务器集群和网络带宽造成太大压力。 如何只允许第一个提交的订单被发送到订单子系统由于最终能够成功秒杀到商品的用户只有一个，因此需要在用户提交订单时，检查是否已经有订单提交。如果已经有订单提交成功，则需要更新 JavaScript文件，更新秒杀开始标志为否，购买按钮变灰。事实上，由于最终能够成功提交订单的用户只有一个，为了减轻下单页面服务器的负载压力，可以控制进入下单页面的入口，只有少数用户能进入下单页面，其他用户直接进入秒杀结束页面。 解决方案：假设下单服务器集群有10台服务器，每台服务器只接受最多10个下单请求。在还没有人提交订单成功之前，如果一台服务器已经有十单了，而有的一单都没处理，可能出现的用户体验不佳的场景是用户第一次点击购买按钮进入已结束页面，再刷新一下页面，有可能被一单都没有处理的服务器处理，进入了填写订单的页面，可以考虑通过cookie的方式来应对，符合一致性原则。当然可以采用最少连接的负载均衡算法，出现上述情况的概率大大降低。 如何进行下单前置检查 下单服务器检查本机已处理的下单请求数目： 如果超过10条，直接返回已结束页面给用户； 如果未超过10条，则用户可进入填写订单及确认页面； 检查全局已提交订单数目： 已超过秒杀商品总数，返回已结束页面给用户； 未超过秒杀商品总数，提交到子订单系统； 秒杀一般是定时上架该功能实现方式很多。不过目前比较好的方式是：提前设定好商品的上架时间，用户可以在前台看到该商品，但是无法点击“立即购买”的按钮。但是需要考虑的是，有人可以绕过前端的限制，直接通过URL的方式发起购买，这就需要在前台商品页面，以及bug页面到后端的数据库，都要进行时钟同步。越在后端控制，安全性越高。 定时秒杀的话，就要避免卖家在秒杀前对商品做编辑带来的不可预期的影响。这种特殊的变更需要多方面评估。一般禁止编辑，如需变更，可以走数据订正的流程。 减库存的操作有两种选择，一种是拍下减库存 另外一种是付款减库存；目前采用的“拍下减库存”的方式，拍下就是一瞬间的事，对用户体验会好些。 库存会带来“超卖”的问题：售出数量多于库存数量由于库存并发更新的问题，导致在实际库存已经不足的情况下，库存依然在减，导致卖家的商品卖得件数超过秒杀的预期。方案：采用乐观锁 123update auction_auctions setquantity = #inQuantity#where auction_id = #itemId# and quantity = #dbQuantity# 还有一种方式，会更好些，叫做尝试扣减库存，扣减库存成功才会进行下单逻辑： 123update auction_auctions set quantity = quantity-#count# where auction_id = #itemId# and quantity &gt;= #count# 秒杀器的应对秒杀器一般下单个购买及其迅速，根据购买记录可以甄别出一部分。可以通过校验码达到一定的方法，这就要求校验码足够安全，不被破解，采用的方式有：秒杀专用验证码，电视公布验证码，秒杀答题。 秒杀架构原则尽量将请求拦截在系统上游传统秒杀系统之所以挂，请求都压倒了后端数据层，数据读写锁冲突严重，并发高响应慢，几乎所有请求都超时，流量虽大，下单成功的有效流量甚小【一趟火车其实只有2000张票，200w个人来买，基本没有人能买成功，请求有效率为0】。 读多写少的常用多使用缓存这是一个典型的读多写少的应用场景【一趟火车其实只有2000张票，200w个人来买，最多2000个人下单成功，其他人都是查询库存，写比例只有0.1%，读比例占99.9%】，非常适合使用缓存。 秒杀架构设计秒杀系统为秒杀而设计，不同于一般的网购行为，参与秒杀活动的用户更关心的是如何能快速刷新商品页面，在秒杀开始的时候抢先进入下单页面，而不是商品详情等用户体验细节，因此秒杀系统的页面设计应尽可能简单。 商品页面中的购买按钮只有在秒杀活动开始的时候才变亮，在此之前及秒杀商品卖出后，该按钮都是灰色的，不可以点击。 下单表单也尽可能简单，购买数量只能是一个且不可以修改，送货地址和付款方式都使用用户默认设置，没有默认也可以不填，允许等订单提交后修改；只有第一个提交的订单发送给网站的订单子系统，其余用户提交订单后只能看到秒杀结束页面。 要做一个这样的秒杀系统，业务会分为两个阶段： 第一个阶段是秒杀开始前某个时间到秒杀开始， 这个阶段可以称之为准备阶段，用户在准备阶段等待秒杀； 第二个阶段就是秒杀开始到所有参与秒杀的用户获得秒杀结果， 这个就称为秒杀阶段吧。 前端层设计首先要有一个展示秒杀商品的页面，在这个页面上做一个秒杀活动开始的倒计时，在准备阶段内用户会陆续打开这个秒杀的页面， 并且可能不停的刷新页面。这里需要考虑两个问题： 第一个是秒杀页面的展示我们知道一个html页面还是比较大的，即使做了压缩，http头和内容的大小也可能高达数十K，加上其他的css， js，图片等资源，如果同时有几千万人参与一个商品的抢购，一般机房带宽也就只有1G10G，网络带宽就极有可能成为瓶颈，所以这个页面上各类静态资源首先应分开存放，然后放到cdn节点上分散压力，由于CDN节点遍布全国各地，能缓冲掉绝大部分的压力，而且还比机房带宽便宜 第二个是倒计时出于性能原因这个一般由js调用客户端本地时间，就有可能出现客户端时钟与服务器时钟不一致，另外服务器之间也是有可能出现时钟不一致。客户端与服务器时钟不一致可以采用客户端定时和服务器同步时间，这里考虑一下性能问题，用于同步时间的接口由于不涉及到后端逻辑，只需要将当前web服务器的时间发送给客户端就可以了，因此速度很快，就我以前测试的结果来看，一台标准的web服务器2W+QPS不会有问题，如果100W人同时刷，100W QPS也只需要50台web，一台硬件LB就可以了~，并且web服务器群是可以很容易的横向扩展的(LB+DNS轮询)，这个接口可以只返回一小段json格式的数据，而且可以优化一下减少不必要cookie和其他http头的信息，所以数据量不会很大，一般来说网络不会成为瓶颈，即使成为瓶颈也可以考虑多机房专线连通，加智能DNS的解决方案；web服务器之间时间不同步可以采用统一时间服务器的方式，比如每隔1分钟所有参与秒杀活动的web服务器就与时间服务器做一次时间同步。 浏览器层请求拦截 （1）产品层面，用户点击“查询”或者“购票”后，按钮置灰，禁止用户重复提交请求; （2）JS层面，限制用户在x秒之内只能提交一次请求; 站点层设计前端层的请求拦截，只能拦住小白用户（不过这是99%的用户哟），高端的程序员根本不吃这一套，写个for循环，直接调用你后端的http请求，怎么整？ （1）同一个uid，限制访问频度，做页面缓存，x秒内到达站点层的请求，均返回同一页面 （2）同一个item的查询，例如手机车次，做页面缓存，x秒内到达站点层的请求，均返回同一页面 如此限流，又有99%的流量会被拦截在站点层。 服务层设计站点层的请求拦截，只能拦住普通程序员，高级黑客，假设他控制了10w台肉鸡（并且假设买票不需要实名认证），这下uid的限制不行了吧？怎么整？ （1）大哥，我是服务层，我清楚的知道小米只有1万部手机，我清楚的知道一列火车只有2000张车票，我透10w个请求去数据库有什么意义呢？对于写请求，做请求队列，每次只透过有限的写请求去数据层，如果均成功再放下一批，如果库存不够则队列里的写请求全部返回“已售完”； （2）对于读请求，还用说么？cache来抗，不管是memcached还是redis，单机抗个每秒10w应该都是没什么问题的； 如此限流，只有非常少的写请求，和非常少的读缓存mis的请求会透到数据层去，又有99.9%的请求被拦住了。 用户请求分发模块：使用Nginx或Apache将用户的请求分发到不同的机器上。 用户请求预处理模块：判断商品是不是还有剩余来决定是不是要处理该请求。 用户请求处理模块：把通过预处理的请求封装成事务提交给数据库，并返回是否成功。 数据库接口模块：该模块是数据库的唯一接口，负责与数据库交互，提供RPC接口供查询是否秒杀结束、剩余数量等信息。 用户请求预处理模块经过HTTP服务器的分发后，单个服务器的负载相对低了一些，但总量依然可能很大，如果后台商品已经被秒杀完毕，那么直接给后来的请求返回秒杀失败即可，不必再进一步发送事务了，示例代码可以如下所示： 123456789101112131415161718192021222324252627282930313233package seckill;import org.apache.http.HttpRequest;/** * 预处理阶段，把不必要的请求直接驳回，必要的请求添加到队列中进入下一阶段. */public class PreProcessor &#123; // 商品是否还有剩余 private static boolean reminds = true; private static void forbidden() &#123; // Do something. &#125; public static boolean checkReminds() &#123; if (reminds) &#123; // 远程检测是否还有剩余，该RPC接口应由数据库服务器提供，不必完全严格检查. if (!RPC.checkReminds()) &#123; reminds = false; &#125; &#125; return reminds; &#125; /** * 每一个HTTP请求都要经过该预处理. */ public static void preProcess(HttpRequest request) &#123; if (checkReminds()) &#123; // 一个并发的队列 RequestQueue.queue.add(request); &#125; else &#123; // 如果已经没有商品了，则直接驳回请求即可. forbidden(); &#125; &#125;&#125; 并发队列的选择 Java的并发包提供了三个常用的并发队列实现，分别是：ConcurrentLinkedQueue、LinkedBlockingQueue和ArrayBlockingQueue。 ArrayBlockingQueue是初始容量固定的阻塞队列，我们可以用来作为数据库模块成功竞拍的队列，比如有10个商品，那么我们就设定一个10大小的数组队列。 ConcurrentLinkedQueue使用的是CAS原语无锁队列实现，是一个异步队列，入队的速度很快，出队进行了加锁，性能稍慢。 LinkedBlockingQueue也是阻塞的队列，入队和出队都用了加锁，当队空的时候线程会暂时阻塞。 由于我们的系统入队需求要远大于出队需求，一般不会出现队空的情况，所以我们可以选择ConcurrentLinkedQueue来作为我们的请求队列实现： 1234567package seckill;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.ConcurrentLinkedQueue;import org.apache.http.HttpRequest;public class RequestQueue &#123; public static ConcurrentLinkedQueue&lt;HttpRequest&gt; queue = new ConcurrentLinkedQueue&lt;HttpRequest&gt;();&#125; 用户请求模块123456789101112131415161718192021package seckill;import org.apache.http.HttpRequest;public class Processor &#123; /** * 发送秒杀事务到数据库队列. */ public static void kill(BidInfo info) &#123; DB.bids.add(info); &#125; public static void process() &#123; BidInfo info = new BidInfo(RequestQueue.queue.poll()); if (info != null) &#123; kill(info); &#125; &#125;&#125;class BidInfo &#123; BidInfo(HttpRequest request) &#123; // Do something. &#125;&#125; 数据库模块 数据库主要是使用一个ArrayBlockingQueue来暂存有可能成功的用户请求。 1234567891011121314151617181920212223package seckill;import java.util.concurrent.ArrayBlockingQueue;/** * DB应该是数据库的唯一接口. */public class DB &#123; public static int count = 10; public static ArrayBlockingQueue&lt;BidInfo&gt; bids = new ArrayBlockingQueue&lt;BidInfo&gt;(10); public static boolean checkReminds() &#123; // TODO return true; &#125; // 单线程操作 public static void bid() &#123; BidInfo info = bids.poll(); while (count-- &gt; 0) &#123; // insert into table Bids values(item_id, user_id, bid_date, other) // select count(id) from Bids where item_id = ? // 如果数据库商品数量大约总数，则标志秒杀已完成，设置标志位reminds = false. info = bids.poll(); &#125; &#125;&#125; 数据库设计基本概念概念一“单库” 概念二“分片” 分片解决的是“数据量太大”的问题，也就是通常说的“水平切分”。一旦引入分片，势必有“数据路由”的概念，哪个数据访问哪个库。路由规则通常有3种方法： 范围：range 优点：简单，容易扩展 缺点：各库压力不均（新号段更活跃） 哈希：hash 【大部分互联网公司采用的方案二：哈希分库，哈希路由】 优点：简单，数据均衡，负载均匀 缺点：迁移麻烦（2库扩3库数据要迁移） 路由服务：router-config-server 优点：灵活性强，业务与路由算法解耦 缺点：每次访问数据库前多一次查询 概念三“分组” 分组解决“可用性”问题，分组通常通过主从复制的方式实现。 互联网公司数据库实际软件架构是：又分片，又分组（如下图） 设计思路数据库软件架构师平时设计些什么东西呢？至少要考虑以下四点： 如何保证数据可用性； 如何提高数据库读性能（大部分应用读多写少，读会先成为瓶颈）； 如何保证一致性； 如何提高扩展性； 如何保证数据的可用性？ 解决可用性问题的思路是=&gt;冗余 如何保证站点的可用性？复制站点，冗余站点 如何保证服务的可用性？复制服务，冗余服务 如何保证数据的可用性？复制数据，冗余数据 数据的冗余，会带来一个副作用=&gt;引发一致性问题（先不说一致性问题，先说可用性）。 如何保证数据库“读”高可用？ 冗余读库 冗余读库带来的副作用？读写有延时，可能不一致。 上面这个图是很多互联网公司mysql的架构，写仍然是单点，不能保证写高可用。 如何保证数据库“写”高可用？ 冗余写库 采用双主互备的方式，可以冗余写库带来的副作用？双写同步，数据可能冲突（例如“自增id”同步冲突），如何解决同步冲突，有两种常见解决方案： 两个写库使用不同的初始值，相同的步长来增加id：1写库的id为0,2,4,6…；2写库的id为1,3,5,7…； 不使用数据的id，业务层自己生成唯一的id，保证数据不冲突； 实际中没有使用上述两种架构来做读写的“高可用”，采用的是“双主当主从用”的方式： 仍是双主，但只有一个主提供服务（读+写），另一个主是“shadow-master”，只用来保证高可用，平时不提供服务。 master挂了，shadow-master顶上（vip漂移，对业务层透明，不需要人工介入）。这种方式的好处： 读写没有延时； 读写高可用； 不足： 不能通过加从库的方式扩展读性能； 资源利用率为50%，一台冗余主没有提供服务； 那如何提高读性能呢？进入第二个话题，如何提供读性能。 如何扩展读性能 提高读性能的方式大致有三种，第一种是建立索引。这种方式不展开，要提到的一点是，不同的库可以建立不同的索引。 写库不建立索引； 线上读库建立线上访问索引，例如uid； 线下读库建立线下访问索引，例如time； 第二种扩充读性能的方式是，增加从库，这种方法大家用的比较多，但是，存在两个缺点： 从库越多，同步越慢； 同步越慢，数据不一致窗口越大（不一致后面说，还是先说读性能的提高）； 实际中没有采用这种方法提高数据库读性能（没有从库），采用的是增加缓存。常见的缓存架构如下： 上游是业务应用，下游是主库，从库（读写分离），缓存。实际的玩法：服务+数据库+缓存一套。 业务层不直接面向db和cache，服务层屏蔽了底层db、cache的复杂性。为什么要引入服务层，今天不展开，采用了“服务+数据库+缓存一套”的方式提供数据访问，用cache提高读性能。 不管采用主从的方式扩展读性能，还是缓存的方式扩展读性能，数据都要复制多份（主+从，db+cache），一定会引发一致性问题。 如何保证一致性？ 主从数据库的一致性，通常有两种解决方案： 中间件 如果某一个key有写操作，在不一致时间窗口内，中间件会将这个key的读操作也路由到主库上。这个方案的缺点是，数据库中间件的门槛较高（百度，腾讯，阿里，360等一些公司有）。 强制读主 上面实际用的“双主当主从用”的架构，不存在主从不一致的问题。第二类不一致，是db与缓存间的不一致： 常见的缓存架构如上，此时写操作的顺序是： （1）淘汰cache； （2）写数据库； 读操作的顺序是： （1）读cache，如果cache hit则返回； （2）如果cache miss，则读从库； （3）读从库后，将数据放回cache； 在一些异常时序情况下，有可能从【从库读到旧数据（同步还没有完成），旧数据入cache后】，数据会长期不一致。解决办法是“缓存双淘汰”，写操作时序升级为： （1）淘汰cache； （2）写数据库； （3）在经过“主从同步延时窗口时间”后，再次发起一个异步淘汰cache的请求； 这样，即使有脏数据如cache，一个小的时间窗口之后，脏数据还是会被淘汰。带来的代价是，多引入一次读miss（成本可以忽略）。 除此之外，最佳实践之一是：建议为所有cache中的item设置一个超时时间。 如何提高数据库的扩展性？ 原来用hash的方式路由，分为2个库，数据量还是太大，要分为3个库，势必需要进行数据迁移，有一个很帅气的“数据库秒级扩容”方案。 如何秒级扩容？ 首先，我们不做2库变3库的扩容，我们做2库变4库（库加倍）的扩容（未来4-&gt;8-&gt;16） 服务+数据库是一套（省去了缓存），数据库采用“双主”的模式。 扩容步骤： 第一步，将一个主库提升; 第二步，修改配置，2库变4库（原来MOD2，现在配置修改后MOD4），扩容完成； 原MOD2为偶的部分，现在会MOD4余0或者2；原MOD2为奇的部分，现在会MOD4余1或者3；数据不需要迁移，同时，双主互相同步，一遍是余0，一边余2，两边数据同步也不会冲突，秒级完成扩容！ 最后，要做一些收尾工作： 将旧的双主同步解除； 增加新的双主（双主是保证可用性的，shadow-master平时不提供服务）； 删除多余的数据（余0的主，可以将余2的数据删除掉）； 这样，秒级别内，我们就完成了2库变4库的扩展。 大并发带来的挑战请求接口的合理设计一个秒杀或者抢购页面，通常分为2个部分，一个是静态的HTML等内容，另一个就是参与秒杀的Web后台请求接口。 通常静态HTML等内容，是通过CDN的部署，一般压力不大，核心瓶颈实际上在后台请求接口上。这个后端接口，必须能够支持高并发请求，同时，非常重要的一点，必须尽可能“快”，在最短的时间里返回用户的请求结果。为了实现尽可能快这一点，接口的后端存储使用内存级别的操作会更好一点。仍然直接面向MySQL之类的存储是不合适的，如果有这种复杂业务的需求，都建议采用异步写入。 当然，也有一些秒杀和抢购采用“滞后反馈”，就是说秒杀当下不知道结果，一段时间后才可以从页面中看到用户是否秒杀成功。但是，这种属于“偷懒”行为，同时给用户的体验也不好，容易被用户认为是“暗箱操作”。 高并发的挑战：一定要“快”我们通常衡量一个Web系统的吞吐率的指标是QPS（Query Per Second，每秒处理请求数），解决每秒数万次的高并发场景，这个指标非常关键。举个例子，我们假设处理一个业务请求平均响应时间为100ms，同时，系统内有20台Apache的Web服务器，配置MaxClients为500个（表示Apache的最大连接数目）。 那么，我们的Web系统的理论峰值QPS为（理想化的计算方式）： 120*500/0.1 = 100000 （10万QPS） 咦？我们的系统似乎很强大，1秒钟可以处理完10万的请求，5w/s的秒杀似乎是“纸老虎”哈。实际情况，当然没有这么理想。在高并发的实际场景下，机器都处于高负载的状态，在这个时候平均响应时间会被大大增加。 就Web服务器而言，Apache打开了越多的连接进程，CPU需要处理的上下文切换也越多，额外增加了CPU的消耗，然后就直接导致平均响应时间增加。因此上述的MaxClient数目，要根据CPU、内存等硬件因素综合考虑，绝对不是越多越好。可以通过Apache自带的abench来测试一下，取一个合适的值。然后，我们选择内存操作级别的存储的Redis，在高并发的状态下，存储的响应时间至关重要。网络带宽虽然也是一个因素，不过，这种请求数据包一般比较小，一般很少成为请求的瓶颈。负载均衡成为系统瓶颈的情况比较少，在这里不做讨论哈。 那么问题来了，假设我们的系统，在5w/s的高并发状态下，平均响应时间从100ms变为250ms（实际情况，甚至更多）： 120*500/0.25 = 40000 （4万QPS） 于是，我们的系统剩下了4w的QPS，面对5w每秒的请求，中间相差了1w。 然后，这才是真正的恶梦开始。举个例子，高速路口，1秒钟来5部车，每秒通过5部车，高速路口运作正常。突然，这个路口1秒钟只能通过4部车，车流量仍然依旧，结果必定出现大塞车。（5条车道忽然变成4条车道的感觉）。 同理，某一个秒内，20*500个可用连接进程都在满负荷工作中，却仍然有1万个新来请求，没有连接进程可用，系统陷入到异常状态也是预期之内。 其实在正常的非高并发的业务场景中，也有类似的情况出现，某个业务请求接口出现问题，响应时间极慢，将整个Web请求响应时间拉得很长，逐渐将Web服务器的可用连接数占满，其他正常的业务请求，无连接进程可用。 更可怕的问题是，是用户的行为特点，系统越是不可用，用户的点击越频繁，恶性循环最终导致“雪崩”（其中一台Web机器挂了，导致流量分散到其他正常工作的机器上，再导致正常的机器也挂，然后恶性循环），将整个Web系统拖垮。 重启与过载保护如果系统发生“雪崩”，贸然重启服务，是无法解决问题的。最常见的现象是，启动起来后，立刻挂掉。这个时候，最好在入口层将流量拒绝，然后再将重启。如果是redis/memcache这种服务也挂了，重启的时候需要注意“预热”，并且很可能需要比较长的时间。 秒杀和抢购的场景，流量往往是超乎我们系统的准备和想象的。这个时候，过载保护是必要的。如果检测到系统满负载状态，拒绝请求也是一种保护措施。在前端设置过滤是最简单的方式，但是，这种做法是被用户“千夫所指”的行为。更合适一点的是，将过载保护设置在CGI入口层，快速将客户的直接请求返回。 作弊的手段：进攻与防守秒杀和抢购收到了“海量”的请求，实际上里面的水分是很大的。不少用户，为了“抢“到商品，会使用“刷票工具”等类型的辅助工具，帮助他们发送尽可能多的请求到服务器。还有一部分高级用户，制作强大的自动请求脚本。这种做法的理由也很简单，就是在参与秒杀和抢购的请求中，自己的请求数目占比越多，成功的概率越高。 这些都是属于“作弊的手段”，不过，有“进攻”就有“防守”，这是一场没有硝烟的战斗哈。 同一个账号，一次性发出多个请求部分用户通过浏览器的插件或者其他工具，在秒杀开始的时间里，以自己的账号，一次发送上百甚至更多的请求。实际上，这样的用户破坏了秒杀和抢购的公平性。 这种请求在某些没有做数据安全处理的系统里，也可能造成另外一种破坏，导致某些判断条件被绕过。例如一个简单的领取逻辑，先判断用户是否有参与记录，如果没有则领取成功，最后写入到参与记录中。这是个非常简单的逻辑，但是，在高并发的场景下，存在深深的漏洞。多个并发请求通过负载均衡服务器，分配到内网的多台Web服务器，它们首先向存储发送查询请求，然后，在某个请求成功写入参与记录的时间差内，其他的请求获查询到的结果都是“没有参与记录”。这里，就存在逻辑判断被绕过的风险。 应对方案： 在程序入口处，一个账号只允许接受1个请求，其他请求过滤。不仅解决了同一个账号，发送N个请求的问题，还保证了后续的逻辑流程的安全。实现方案，可以通过Redis这种内存缓存服务，写入一个标志位（只允许1个请求写成功，结合watch的乐观锁的特性），成功写入的则可以继续参加。 或者，自己实现一个服务，将同一个账号的请求放入一个队列中，处理完一个，再处理下一个。 多个账号，一次性发送多个请求很多公司的账号注册功能，在发展早期几乎是没有限制的，很容易就可以注册很多个账号。因此，也导致了出现了一些特殊的工作室，通过编写自动注册脚本，积累了一大批“僵尸账号”，数量庞大，几万甚至几十万的账号不等，专门做各种刷的行为（这就是微博中的“僵尸粉“的来源）。举个例子，例如微博中有转发抽奖的活动，如果我们使用几万个“僵尸号”去混进去转发，这样就可以大大提升我们中奖的概率。 这种账号，使用在秒杀和抢购里，也是同一个道理。例如，iPhone官网的抢购，火车票黄牛党。 应对方案： 这种场景，可以通过检测指定机器IP请求频率就可以解决，如果发现某个IP请求频率很高，可以给它弹出一个验证码或者直接禁止它的请求： 弹出验证码，最核心的追求，就是分辨出真实用户。因此，大家可能经常发现，网站弹出的验证码，有些是“鬼神乱舞”的样子，有时让我们根本无法看清。他们这样做的原因，其实也是为了让验证码的图片不被轻易识别，因为强大的“自动脚本”可以通过图片识别里面的字符，然后让脚本自动填写验证码。实际上，有一些非常创新的验证码，效果会比较好，例如给你一个简单问题让你回答，或者让你完成某些简单操作（例如百度贴吧的验证码）。 直接禁止IP，实际上是有些粗暴的，因为有些真实用户的网络场景恰好是同一出口IP的，可能会有“误伤“。但是这一个做法简单高效，根据实际场景使用可以获得很好的效果。 多个账号，不同IP发送不同请求所谓道高一尺，魔高一丈。有进攻，就会有防守，永不休止。这些“工作室”，发现你对单机IP请求频率有控制之后，他们也针对这种场景，想出了他们的“新进攻方案”，就是不断改变IP。 有同学会好奇，这些随机IP服务怎么来的。有一些是某些机构自己占据一批独立IP，然后做成一个随机代理IP的服务，有偿提供给这些“工作室”使用。还有一些更为黑暗一点的，就是通过木马黑掉普通用户的电脑，这个木马也不破坏用户电脑的正常运作，只做一件事情，就是转发IP包，普通用户的电脑被变成了IP代理出口。通过这种做法，黑客就拿到了大量的独立IP，然后搭建为随机IP服务，就是为了挣钱。 应对方案： 说实话，这种场景下的请求，和真实用户的行为，已经基本相同了，想做分辨很困难。再做进一步的限制很容易“误伤“真实用户，这个时候，通常只能通过设置业务门槛高来限制这种请求了，或者通过账号行为的”数据挖掘“来提前清理掉它们。 僵尸账号也还是有一些共同特征的，例如账号很可能属于同一个号码段甚至是连号的，活跃度不高，等级低，资料不全等等。根据这些特点，适当设置参与门槛，例如限制参与秒杀的账号等级。通过这些业务手段，也是可以过滤掉一些僵尸号。 高并发下的数据安全我们知道在多线程写入同一个文件的时候，会存现“线程安全”的问题（多个线程同时运行同一段代码，如果每次运行结果和单线程运行的结果是一样的，结果和预期相同，就是线程安全的）。如果是MySQL数据库，可以使用它自带的锁机制很好的解决问题，但是，在大规模并发的场景中，是不推荐使用MySQL的。秒杀和抢购的场景中，还有另外一个问题，就是“超发”，如果在这方面控制不慎，会产生发送过多的情况。我们也曾经听说过，某些电商搞抢购活动，买家成功拍下后，商家却不承认订单有效，拒绝发货。这里的问题，也许并不一定是商家奸诈，而是系统技术层面存在超发风险导致的。 超发的原因假设某个抢购场景中，我们一共只有100个商品，在最后一刻，我们已经消耗了99个商品，仅剩最后一个。这个时候，系统发来多个并发请求，这批请求读取到的商品余量都是99个，然后都通过了这一个余量判断，最终导致超发。 在上面的这个图中，就导致了并发用户B也“抢购成功”，多让一个人获得了商品。这种场景，在高并发的情况下非常容易出现。 悲观锁思路解决线程安全的思路很多，可以从“悲观锁”的方向开始讨论。 悲观锁，也就是在修改数据的时候，采用锁定状态，排斥外部请求的修改。遇到加锁的状态，就必须等待。 虽然上述的方案的确解决了线程安全的问题，但是，别忘记，我们的场景是“高并发”。也就是说，会很多这样的修改请求，每个请求都需要等待“锁”，某些线程可能永远都没有机会抢到这个“锁”，这种请求就会死在那里。同时，这种请求会很多，瞬间增大系统的平均响应时间，结果是可用连接数被耗尽，系统陷入异常。 FIFO队列思路那好，那么我们稍微修改一下上面的场景，我们直接将请求放入队列中的，采用FIFO（First Input First Output，先进先出），这样的话，我们就不会导致某些请求永远获取不到锁。看到这里，是不是有点强行将多线程变成单线程的感觉哈。 然后，我们现在解决了锁的问题，全部请求采用“先进先出”的队列方式来处理。那么新的问题来了，高并发的场景下，因为请求很多，很可能一瞬间将队列内存“撑爆”，然后系统又陷入到了异常状态。或者设计一个极大的内存队列，也是一种方案，但是，系统处理完一个队列内请求的速度根本无法和疯狂涌入队列中的数目相比。也就是说，队列内的请求会越积累越多，最终Web系统平均响应时候还是会大幅下降，系统还是陷入异常。 乐观锁思路这个时候，我们就可以讨论一下“乐观锁”的思路了。乐观锁，是相对于“悲观锁”采用更为宽松的加锁机制，大都是采用带版本号（Version）更新。实现就是，这个数据所有请求都有资格去修改，但会获得一个该数据的版本号，只有版本号符合的才能更新成功，其他的返回抢购失败。这样的话，我们就不需要考虑队列的问题，不过，它会增大CPU的计算开销。但是，综合来说，这是一个比较好的解决方案。 有很多软件和服务都“乐观锁”功能的支持，例如Redis中的watch就是其中之一。通过这个实现，我们保证了数据的安全。 总结互联网正在高速发展，使用互联网服务的用户越多，高并发的场景也变得越来越多。电商秒杀和抢购，是两个比较典型的互联网高并发场景。虽然我们解决问题的具体技术方案可能千差万别，但是遇到的挑战却是相似的，因此解决问题的思路也异曲同工。 参考原文出处：https://www.jianshu.com/p/df4fbecb1a4b]]></content>
      <categories>
        <category>系统架构</category>
      </categories>
      <tags>
        <tag>一致性</tag>
        <tag>并发</tag>
        <tag>线程</tag>
        <tag>Redis</tag>
        <tag>缓存</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java NIO面试题剖析]]></title>
    <url>%2F2019%2F08%2F07%2FJava-NIO%E9%9D%A2%E8%AF%95%E9%A2%98%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[首先我们分别画图来看看，BIO、NIO、AIO，分别是什么？ BIO传统的网络通讯模型，就是BIO，同步阻塞IO 它其实就是服务端创建一个ServerSocket， 然后就是客户端用一个Socket去连接服务端的那个ServerSocket， ServerSocket接收到了一个的连接请求就创建一个Socket和一个线程去跟那个Socket进行通讯。 接着客户端和服务端就进行阻塞式的通信，客户端发送一个请求，服务端Socket进行处理后返回响应。 在响应返回前，客户端那边就阻塞等待，上门事情也做不了。 这种方式的缺点：每次一个客户端接入，都需要在服务端创建一个线程来服务这个客户端 这样大量客户端来的时候，就会造成服务端的线程数量可能达到了几千甚至几万，这样就可能会造成服务端过载过高，最后崩溃死掉。 BIO模型图 Acceptor 传统的IO模型的网络服务的设计模式中有俩种比较经典的设计模式：一个是多线程， 一种是依靠线程池来进行处理。 如果是基于多线程的模式来的话，就是这样的模式，这种也是Acceptor线程模型。 NIONIO是一种同步非阻塞IO, 基于Reactor模型来实现的。 其实相当于就是一个线程处理大量的客户端的请求，通过一个线程轮询大量的channel，每次就获取一批有事件的channel，然后对每个请求启动一个线程处理即可。 这里的核心就是非阻塞，就那个selector一个线程就可以不停轮询channel，所有客户端请求都不会阻塞，直接就会进来，大不了就是等待一下排着队而已。 这里面优化BIO的核心就是，一个客户端并不是时时刻刻都有数据进行交互，没有必要死耗着一个线程不放，所以客户端选择了让线程歇一歇，只有客户端有相应的操作的时候才发起通知，创建一个线程来处理请求。 NIO：模型图 Reactor模型 AIOAIO：异步非阻塞IO，基于Proactor模型实现。 每个连接发送过来的请求，都会绑定一个Buffer，然后通知操作系统去完成异步的读，这个时间你就可以去做其他的事情 等到操作系统完成读之后，就会调用你的接口，给你操作系统异步读完的数据。这个时候你就可以拿到数据进行处理，将数据往回写 在往回写的过程，同样是给操作系统一个Buffer，让操作系统去完成写，写完了来通知你。 这俩个过程都有buffer存在，数据都是通过buffer来完成读写。 这里面的主要的区别在于将数据写入的缓冲区后，就不去管它，剩下的去交给操作系统去完成。 操作系统写回数据也是一样，写到Buffer里面，写完后通知客户端来进行读取数据。 AIO：模型图 聊完了BIO，NIO，AIO的区别之后，现在我们再结合这三个模型来说下同步和阻塞的一些问题。 同步阻塞为什么说BIO是同步阻塞的呢？其实这里说的不是针对网络通讯模型而言，而是针对磁盘文件读写IO操作来说的。 因为用BIO的流读写文件，例如FileInputStrem，是说你发起个IO请求直接hang死，卡在那里，必须等着搞完了这次IO才能返回。 同步非阻塞:为什么说NIO为啥是同步非阻塞？因为无论多少客户端都可以接入服务端，客户端接入并不会耗费一个线程，只会创建一个连接然后注册到selector上去，这样你就可以去干其他你想干的其他事情了 一个selector线程不断的轮询所有的socket连接，发现有事件了就通知你，然后你就启动一个线程处理一个请求即可，这个过程的话就是非阻塞的。 但是这个处理的过程中，你还是要先读取数据，处理，再返回的，这是个同步的过程。 异步非阻塞为什么说AIO是异步非阻塞？通过AIO发起个文件IO操作之后，你立马就返回可以干别的事儿了，接下来你也不用管了，操作系统自己干完了IO之后，告诉你说ok了 当你基于AIO的api去读写文件时， 当你发起一个请求之后，剩下的事情就是交给了操作系统 当读写完成后， 操作系统会来回调你的接口， 告诉你操作完成 在这期间不需要等待， 也不需要去轮询判断操作系统完成的状态，你可以去干其他的事情。 同步就是自己还得主动去轮询操作系统，异步就是操作系统反过来通知你。所以来说， AIO就是异步非阻塞的。 NIO核心组件详细讲解学习NIO先来搞清楚一些相关的概念 NIO通讯有哪些相关组件，对应的作用都是什么，之间有哪些联系？多路复用机制实现Selector 首先我们来了解下传统的Socket网络通讯模型。 传统Socket通讯原理图 为什么传统的socket不支持海量连接？每次一个客户端接入，都是要在服务端创建一个线程来服务这个客户端的 这会导致大量的客户端的时候，服务端的线程数量可能达到几千甚至几万，几十万，这会导致服务器端程序负载过高，不堪重负，最终系统崩溃死掉。 接着来看下NIO是如何基于Selector实现多路复用机制支持的海量连接。 NIO原理图 多路复用机制是如何支持海量连接？NIO的线程模型对Socket发起的连接不需要每个都创建一个线程，完全可以使用一个Selector来多路复用监听N多个Channel是否有请求，该请求是对应的连接请求，还是发送数据的请求 这里面是基于操作系统底层的Select通知机制的，一个Selector不断的轮询多个Channel，这样避免了创建多个线程 只有当莫个Channel有对应的请求的时候才会创建线程，可能说1000个请求， 只有100个请求是有数据交互的 这个时候可能server端就提供10个线程就能够处理这些请求。这样的话就可以避免了创建大量的线程。 NIO如何通过Buffer来缓冲数据的 NIO中的Buffer是个什么东西 ？ 学习NIO，首当其冲就是要了解所谓的Buffer缓冲区，这个东西是NIO里比较核心的一个部分 一般来说，如果你要通过NIO写数据到文件或者网络，或者是从文件和网络读取数据出来此时就需要通过Buffer缓冲区来进行。Buffer的使用一般有如下几个步骤： 写入数据到Buffer，调用flip()方法，从Buffer中读取数据，调用clear()方法或者compact()方法。 Buffer中对应的Position， Mark， Capacity，Limit都啥？ capacity：缓冲区容量的大小，就是里面包含的数据大小。 limit：对buffer缓冲区使用的一个限制，从这个index开始就不能读取数据了。 position：代表着数组中可以开始读写的index， 不能大于limit。 mark：是类似路标的东西，在某个position的时候，设置一下mark，此时就可以设置一个标记 后续调用reset()方法可以把position复位到当时设置的那个mark上。去把position或limit调整为小于mark的值时，就丢弃这个mark 如果使用的是Direct模式创建的Buffer的话，就会减少中间缓冲直接使用DirectorBuffer来进行数据的存储。 如何通过Channel和FileChannel读取Buffer数据写入磁盘 NIO中，Channel是什么？Channel是NIO中的数据通道，类似流，但是又有些不同 Channel既可从中读取数据，又可以从写数据到通道中，但是流的读写通常是单向的。 Channel可以异步的读写。Channel中的数据总是要先读到一个Buffer中，或者从缓冲区中将数据写到通道中。 FileChannel的作用是什么？Buffer有不同的类型，同样Channel也有好几个类型。 FileChannel DatagramChannel SocketChannel ServerSocketChannel 这些通道涵盖了UDP 和 TCP 网络IO，以及文件IO。而FileChannel就是文件IO对应的管道， 在读取文件的时候会用到这个管道。 总结:​ 通过本篇文章，主要是分析了常见的NIO的一些问题： BIO， NIO， AIO各自的特点 什么同步阻塞，同步非阻塞，异步非阻塞 为什么NIO能够应对支持海量的请求 NIO相关组件的原理 NIO通讯的简单案例 本文仅仅是介绍了一下网络通讯的一些原理 参考本文转载自： 石杉的架构笔记 https://mp.weixin.qq.com/s/YIcXaH7AWLJbPjnTUwnlyQ]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用户表分库分表方案]]></title>
    <url>%2F2019%2F07%2F21%2F%E7%94%A8%E6%88%B7%E8%A1%A8%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[再次抛出笔者的观点，在能满足业务场景的情况下，单表&gt;分区&gt;单库分表&gt;分库分表，推荐优先级从左到右逐渐降低。 本篇文章主要讲用户表（或者类似这种业务属性的表）的分表方案，至于订单表，流水表等，本文的方案可能不是很合适，可以参考另一篇文章《分库分表技术演进&amp;最佳实践-修订篇》。 我们首先来看一下分表时主要需要做的事情： 选定分片键：既然是用户表那分片键非用户ID莫属； 修改代码：以sharding-jdbc这种client模式的中间件为例，主要是引入依赖，然后新增一些配置。业务代码并不怎么需要改动。 存量数据迁移； 业务发展超过容量评估后需要开发和运维介入扩容； 做过分库分表的都知道，第3步最麻烦，而且非常不好验证迁前后数据一致性（目前业界主流的迁移方案是存量数据迁移+利用binlog进行增量数据同步，待两边的数据持平后，将业务代码中的开关切到分表模式）。 第4步同样麻烦，业务增长完全超过当初分表设计的容量评估是很常见的事情，这也成为业务高速发展的一个隐患。而且互联网类型的业务都希望能做到7x24小时不停服务，这样就给扩容带来了更大的挑战。笔者看过比较好的方案就是58沈剑提出的成倍扩容方案。如下图所示，假设现在已经有2张表：tb_user_1，tb_user_2。且有两个库是主备关系，并且分表算法是hash(user_id)%2： 现在要扩容到4张表，做法是将两个库的主从关系切断。然后slave晋升为master，这样就有两个主库：master-1，master-2。新的分表算法是： 库选择算法为：hash(user_id)%4的结果为1或者2，就选master-1库，hash(user_id)%4的结果为3或者0，就选master-2库； 表的选择算法为：hash(user_id)%2的结果为1则选tb_user_1表，hash(user_id)%2的结果为0则选tb_user_2表。 如此以来，两个库中总计4张表，都冗余了1倍的数据：master-1中tb_user_1冗余了3、7、11…，master-1中tb_user_2冗余了4、8、12…，master-2中tb_user_1冗余了1、5、9…，master-2中tb_user_2冗余了2、6、10…。将这些冗余数据删掉后，库、表、数据示意图如下所示： 即使这样方案，还是避免不了分表时的存量数据迁移，以及分表后业务发展到一定时期后的繁琐扩容。那么有没有一种很好的方案，能够一劳永逸，分表时不需要存量数据迁移，用户量无论如何增长，扩容时都不需要迁移存量数据，只需要新增一个数据库示例，修改一下配置即可。软件开发行业，一个方案能撑过3~5年就是一个很优秀的方案，我们现在YY的是整个生命周期内都不用改动的完美的方案。没错，我们在寻找银弹。 这个方案笔者在两个地方都接触到了： 某V厂面试时，部门老大提出的方案； 和美团大牛普架讨论了解到的CAT存储方案； 说明：CAT是美团点评开源的APM，目前在Github上的star已经破万（Github地址：https://github.com/dianping/cat），比skywalking和pinpoint还快，如果你正在选型APM，而且能接受代码侵入，那么CAT是一个不错的选择。 CAT存储方案是按照写入时间顺序存储，假设每小时写入量是千万级别，那么分表就按照小时维度。也就是说，2019年7月18号10点数据写入到表tb_catdata_2019071810中，2019年7月18号12点数据写入到表tb_catdata_2019071812中，2019年7月20号14点数据写入到表tb_catdata_2019072014中。这样做的优点如下： 历史数据不用迁移； 扩容非常简单； 缺点如下： 读写热点集中，所有写操作全部打在最新的表上。 有没有发现，这个方案的优点就是我们需要的。BINGO，要的就是这样的方案。那么对应到用户表上来具体的分表方案非常类似：按照range切分。需要说明的是，这个方案的前提是用户ID一定要趋势递增，最好严格递增。笔者给出3种用户ID递增的方案： 自增ID 假设存量数据用户表的id最大值是960W，那么分表算法是这样的，表序号只需要根据user_id/10000000就能得到： 用户ID在范围[1, 10000000)中分到tb_user_0中（需要将tb_user重命名为tb_user_0）； 用户ID在范围[10000000, 20000000)中分到tb_user_1中； 用户ID在范围[20000000, 30000000)中分到tb_user_2中； 用户ID在范围[30000000, 40000000)中分到tb_user_3中； 以此类推。 如果你的tb_user本来就有自增主键，那这种方案就比较好。但是需要注意几点，由于用户ID是自增的，所以这个ID不能通过HTTP暴露出去，否则可以通过新注册一个用户后，就能得到你的真实用户数，这是比较危险的。其次，存量数据在单表中可以通过自增ID生成，但是当切换分表后，用户ID如果还是用自增生成，需要注意在创建新表时设置AUTO_INCREMENT，例如创建表tb_user_2时，设置AUTO_INCREMENT=10000000，DDL如下： 12345678CREATE TABLE if not exists `tb_user_2` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT PRIMARY KEY, `username` varchar(16) NOT NULL COMMENT &apos;用户名&apos;, `remark` varchar(16) NOT NULL COMMENT &apos;备注&apos;) ENGINE=InnoDB AUTO_INCREMENT=10000000;- 这样的话，当新增用户时，用户ID就会从10000000开始，而不会与之前的用户ID冲突insert into tb_user_2 values(null, &apos;afei&apos;, &apos;afei&apos;); Redis incr 第二种方案就是利用Redis的incr命令。将之前最大的ID保存到Redis中，接下来新增用户的ID值都通过incr命令得到。然后insert到表tb_user中。这种方案需要注意Redis主从切换后，晋升为主的Redis节点中的ID可能由于同步时间差不是最新ID的问题。这样的话，可能会导致插入记录到tb_user失败。需要对这种异常特殊处理一下即可。 利用雪花算法生成 采用类雪花算法生成用户ID，这种方式不太好精确掌握切分表的时机。因为没有高效获取tb_user表数据量的办法，也就不知道什么时候表数据量达到1000w级别，也就不知道什么时候需要往新表中插入数据（select count(*) from tb_user无论怎么优化性能都不会很高，除非是MyISAM引擎）。而且如果利用雪花算法生成用户ID，那么还需要一张表保存用户ID和分表关系： 笔者推荐第一种方案，即利用表自增ID生成用户ID：方案越简单，可靠性越高。其他两种方案，或者其他方案或多或少需要引入一些中间件或者介质，从而增加方案的复杂度。新方案效果图如下： 回顾总结我们回头看一下这种用户表方案，满足了存量数据不需要做任何迁移（除非是存量数据远远超过单表承受能力）。而且，无论用户规模增长到多大量级，1亿，10亿，50亿，后面都不需要做数据迁移。而且也不再需要开发和运维介入。因为整个方案，会自己往新表中插入数据。我们唯一需要做的就是，根据硬件性能，约定一个库允许保存的用户表数量即可。假如一个库保存64张表，那么当扩容到第65张表时，程序会自动往第二个库的第一张表中写入。 参考原文出处：微信公众号：阿飞的博客 https://mp.weixin.qq.com/s/vZn0Q8sHS0gZnlRPP-BKdw]]></content>
      <categories>
        <category>系统架构</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式锁:Redis、Zookeeper]]></title>
    <url>%2F2019%2F07%2F21%2F%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81-Redis%E3%80%81Zookeeper%2F</url>
    <content type="text"><![CDATA[为什么用分布式锁？在讨论这个问题之前，我们先来看一个业务场景： 系统A是一个电商系统，目前是一台机器部署，系统中有一个用户下订单的接口，但是用户下订单之前一定要去检查一下库存，确保库存足够了才会给用户下单。 由于系统有一定的并发，所以会预先将商品的库存保存在redis中，用户下单的时候会更新redis的库存。 此时系统架构如下： 但是这样一来会产生一个问题：假如某个时刻，redis里面的某个商品库存为1，此时两个请求同时到来，其中一个请求执行到上图的第3步，更新数据库的库存为0，但是第4步还没有执行。 而另外一个请求执行到了第2步，发现库存还是1，就继续执行第3步。 这样的结果，是导致卖出了2个商品，然而其实库存只有1个。 很明显不对啊！这就是典型的库存超卖问题 此时，我们很容易想到解决方案：用锁把2、3、4步锁住，让他们执行完之后，另一个线程才能进来执行第2步。 按照上面的图，在执行第2步时，使用Java提供的synchronized或者ReentrantLock来锁住，然后在第4步执行完之后才释放锁。 这样一来，2、3、4 这3个步骤就被“锁”住了，多个线程之间只能串行化执行。 但是好景不长，整个系统的并发飙升，一台机器扛不住了。现在要增加一台机器，如下图： 增加机器之后，系统变成上图所示，我的天！ 假设此时两个用户的请求同时到来，但是落在了不同的机器上，那么这两个请求是可以同时执行了，还是会出现库存超卖的问题。 为什么呢？因为上图中的两个A系统，运行在两个不同的JVM里面，他们加的锁只对属于自己JVM里面的线程有效，对于其他JVM的线程是无效的。 因此，这里的问题是：Java提供的原生锁机制在多机部署场景下失效了 这是因为两台机器加的锁不是同一个锁(两个锁在不同的JVM里面)。 那么，我们只要保证两台机器加的锁是同一个锁，问题不就解决了吗？ 此时，就该分布式锁隆重登场了，分布式锁的思路是： 在整个系统提供一个全局、唯一的获取锁的“东西”，然后每个系统在需要加锁时，都去问这个“东西”拿到一把锁，这样不同的系统拿到的就可以认为是同一把锁。 至于这个“东西”，可以是Redis、Zookeeper，也可以是数据库。 文字描述不太直观，我们来看下图： 通过上面的分析，我们知道了库存超卖场景在分布式部署系统的情况下使用Java原生的锁机制无法保证线程安全，所以我们需要用到分布式锁的方案。 那么，如何实现分布式锁呢？接着往下看！ 基于Redis实现分布式锁上面分析为啥要使用分布式锁了，这里我们来具体看看分布式锁落地的时候应该怎么样处理。 最常见的一种方案就是使用Redis做分布式锁 使用Redis做分布式锁的思路大概是这样的：在redis中设置一个值表示加了锁，然后释放锁的时候就把这个key删除。 具体代码是这样的： 12345678910// 获取锁// NX是指如果key不存在就成功，key存在返回false，PX可以指定过期时间SET anyLock unique_value NX PX 30000// 释放锁：通过执行一段lua脚本// 释放锁涉及到两条指令，这两条指令不是原子性的// 需要用到redis的lua脚本支持特性，redis执行lua脚本是原子性的if redis.call("get",KEYS[1]) == ARGV[1] then return redis.call("del",KEYS[1])else return 0end 这种方式有几大要点： 一定要用SET key value NX PX milliseconds 命令 如果不用，先设置了值，再设置过期时间，这个不是原子性操作，有可能在设置过期时间之前宕机，会造成死锁(key永久存在) value要具有唯一性 这个是为了在解锁的时候，需要验证value是和加锁的一致才删除key。 这是避免了一种情况：假设A获取了锁，过期时间30s，此时35s之后，锁已经自动释放了，A去释放锁，但是此时可能B获取了锁。A客户端就不能删除B的锁了。 除了要考虑客户端要怎么实现分布式锁之外，还需要考虑redis的部署问题。 redis有3种部署方式： 单机模式 master-slave + sentinel选举模式 redis cluster模式 使用redis做分布式锁的缺点在于：如果采用单机部署模式，会存在单点问题，只要redis故障了。加锁就不行了。 采用master-slave模式，加锁的时候只对一个节点加锁，即便通过sentinel做了高可用，但是如果master节点故障了，发生主从切换，此时就会有可能出现锁丢失的问题。 基于以上的考虑，其实redis的作者也考虑到这个问题，他提出了一个RedLock的算法，这个算法的意思大概是这样的： 假设redis的部署模式是redis cluster，总共有5个master节点，通过以下步骤获取一把锁： 获取当前时间戳，单位是毫秒 轮流尝试在每个master节点上创建锁，过期时间设置较短，一般就几十毫秒 尝试在大多数节点上建立一个锁，比如5个节点就要求是3个节点（n / 2 +1） 客户端计算建立好锁的时间，如果建立锁的时间小于超时时间，就算建立成功了 要是锁建立失败了，那么就依次删除这个锁 只要别人建立了一把分布式锁，你就得不断轮询去尝试获取锁 但是这样的这种算法还是颇具争议的，可能还会存在不少的问题，无法保证加锁的过程一定正确。 另一种方式：Redisson此外，实现Redis的分布式锁，除了自己基于redis client原生api来实现之外，还可以使用开源框架：Redission Redisson是一个企业级的开源Redis Client，也提供了分布式锁的支持。我也非常推荐大家使用，为什么呢？ 回想一下上面说的，如果自己写代码来通过redis设置一个值，是通过下面这个命令设置的。 SET anyLock unique_value NX PX 30000 这里设置的超时时间是30s，假如我超过30s都还没有完成业务逻辑的情况下，key会过期，其他线程有可能会获取到锁。 这样一来的话，第一个线程还没执行完业务逻辑，第二个线程进来了也会出现线程安全问题。所以我们还需要额外的去维护这个过期时间，太麻烦了~ 我们来看看redisson是怎么实现的？先感受一下使用redission的爽： 12345678Config config = new Config();config.useClusterServers().addNodeAddress("redis://192.168.31.101:7001") .addNodeAddress("redis://192.168.31.101:7002") .addNodeAddress("redis://192.168.31.101:7003") .addNodeAddress("redis://192.168.31.102:7001") .addNodeAddress("redis://192.168.31.102:7002") .addNodeAddress("redis://192.168.31.102:7003");RedissonClient redisson = Redisson.create(config);RLock lock = redisson.getLock("anyLock");lock.lock();lock.unlock(); 就是这么简单，我们只需要通过它的api中的lock和unlock即可完成分布式锁，他帮我们考虑了很多细节： redisson所有指令都通过lua脚本执行，redis支持lua脚本原子性执行 redisson设置一个key的默认过期时间为30s,如果某个客户端持有一个锁超过了30s怎么办？ redisson中有一个watchdog的概念，翻译过来就是看门狗，它会在你获取锁之后，每隔10秒帮你把key的超时时间设为30s 这样的话，就算一直持有锁也不会出现key过期了，其他线程获取到锁的问题了。 redisson的“看门狗”逻辑保证了没有死锁发生。 (如果机器宕机了，看门狗也就没了。此时就不会延长key的过期时间，到了30s之后就会自动过期了，其他线程可以获取到锁) 这里稍微贴出来其实现代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// 加锁逻辑private &lt;T&gt; RFuture&lt;Long&gt; tryAcquireAsync(long leaseTime, TimeUnit unit, final long threadId) &#123; if (leaseTime != -1) &#123; return tryLockInnerAsync(leaseTime, unit, threadId, RedisCommands.EVAL_LONG); &#125;// 调用一段lua脚本，设置一些key、过期时间 RFuture&lt;Long&gt; ttlRemainingFuture = tryLockInnerAsync(commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout(), TimeUnit.MILLISECONDS, threadId, RedisCommands.EVAL_LONG); ttlRemainingFuture.addListener(new FutureListener&lt;Long&gt;() &#123; @Override public void operationComplete(Future&lt;Long&gt; future) throws Exception &#123; if (!future.isSuccess()) &#123; return; &#125; Long ttlRemaining = future.getNow(); // lock acquired if (ttlRemaining == null) &#123; // 看门狗逻辑 scheduleExpirationRenewal(threadId); &#125; &#125; &#125;); return ttlRemainingFuture;&#125;&lt;T&gt; RFuture&lt;T&gt; tryLockInnerAsync(long leaseTime, TimeUnit unit, long threadId, RedisStrictCommand&lt;T&gt; command) &#123; internalLockLeaseTime = unit.toMillis(leaseTime); return commandExecutor.evalWriteAsync(getName(), LongCodec.INSTANCE, command, "if (redis.call('exists', KEYS[1]) == 0) then " + "redis.call('hset', KEYS[1], ARGV[2], 1); " + "redis.call('pexpire', KEYS[1], ARGV[1]); " + "return nil; " + "end; " + "if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then " + "redis.call('hincrby', KEYS[1], ARGV[2], 1); " + "redis.call('pexpire', KEYS[1], ARGV[1]); " + "return nil; " + "end; " + "return redis.call('pttl', KEYS[1]);", Collections.&lt;Object&gt;singletonList(getName()), internalLockLeaseTime, getLockName(threadId));&#125;// 看门狗最终会调用了这里private void scheduleExpirationRenewal(final long threadId) &#123; if (expirationRenewalMap.containsKey(getEntryName())) &#123; return; &#125; // 这个任务会延迟10s执行 Timeout task = commandExecutor.getConnectionManager().newTimeout(new TimerTask() &#123; @Override public void run(Timeout timeout) throws Exception &#123; // 这个操作会将key的过期时间重新设置为30s RFuture&lt;Boolean&gt; future = renewExpirationAsync(threadId); future.addListener(new FutureListener&lt;Boolean&gt;() &#123; @Override public void operationComplete(Future&lt;Boolean&gt; future) throws Exception &#123; expirationRenewalMap.remove(getEntryName()); if (!future.isSuccess()) &#123; log.error("Can't update lock " + getName() + " expiration", future.cause()); return; &#125; if (future.getNow()) &#123; // reschedule itself // 通过递归调用本方法，无限循环延长过期时间 scheduleExpirationRenewal(threadId); &#125; &#125; &#125;); &#125; &#125;, internalLockLeaseTime / 3, TimeUnit.MILLISECONDS); if (expirationRenewalMap.putIfAbsent(getEntryName(), new ExpirationEntry(threadId, task)) != null) &#123; task.cancel(); &#125;&#125; 另外，redisson还提供了对redlock算法的支持, 它的用法也很简单： 1234567RedissonClient redisson = Redisson.create(config);RLock lock1 = redisson.getFairLock("lock1");RLock lock2 = redisson.getFairLock("lock2");RLock lock3 = redisson.getFairLock("lock3");RedissonRedLock multiLock = new RedissonRedLock(lock1, lock2, lock3);multiLock.lock();multiLock.unlock(); 小结：本节分析了使用redis作为分布式锁的具体落地方案 以及其一些局限性 然后介绍了一个redis的客户端框架redisson， 这也是我推荐大家使用的， 比自己写代码实现会少care很多细节。 基于zookeeper实现分布式锁常见的分布式锁实现方案里面，除了使用redis来实现之外，使用zookeeper也可以实现分布式锁。 在介绍zookeeper(下文用zk代替)实现分布式锁的机制之前，先粗略介绍一下zk是什么东西： Zookeeper是一种提供配置管理、分布式协同以及命名的中心化服务。 zk的模型是这样的：zk包含一系列的节点，叫做znode，就好像文件系统一样每个znode表示一个目录，然后znode有一些特性： 有序节点：假如当前有一个父节点为/lock，我们可以在这个父节点下面创建子节点； zookeeper提供了一个可选的有序特性，例如我们可以创建子节点“/lock/node-”并且指明有序，那么zookeeper在生成子节点时会根据当前的子节点数量自动添加整数序号 也就是说，如果是第一个创建的子节点，那么生成的子节点为/lock/node-0000000000，下一个节点则为/lock/node-0000000001，依次类推。 临时节点：客户端可以建立一个临时节点，在会话结束或者会话超时后，zookeeper会自动删除该节点。 事件监听：在读取数据时，我们可以同时对节点设置事件监听，当节点数据或结构变化时，zookeeper会通知客户端。当前zookeeper有如下四种事件： 节点创建 节点删除 节点数据修改 子节点变更 基于以上的一些zk的特性，我们很容易得出使用zk实现分布式锁的落地方案： 使用zk的临时节点和有序节点，每个线程获取锁就是在zk创建一个临时有序的节点，比如在/lock/目录下。 创建节点成功后，获取/lock目录下的所有临时节点，再判断当前线程创建的节点是否是所有的节点的序号最小的节点 如果当前线程创建的节点是所有节点序号最小的节点，则认为获取锁成功。 如果当前线程创建的节点不是所有节点序号最小的节点，则对节点序号的前一个节点添加一个事件监听。 比如当前线程获取到的节点序号为/lock/003,然后所有的节点列表为[/lock/001,/lock/002,/lock/003],则对/lock/002这个节点添加一个事件监听器。 如果锁释放了，会唤醒下一个序号的节点，然后重新执行第3步，判断是否自己的节点序号是最小。 比如/lock/001释放了，/lock/002监听到时间，此时节点集合为[/lock/002,/lock/003],则/lock/002为最小序号节点，获取到锁。 整个过程如下： 具体的实现思路就是这样，至于代码怎么写，这里比较复杂就不贴出来了。 Curator介绍Curator是一个zookeeper的开源客户端，也提供了分布式锁的实现。 他的使用方式也比较简单： 123InterProcessMutex interProcessMutex = new InterProcessMutex(client,"/anyLock");interProcessMutex.acquire();interProcessMutex.release(); 其实现分布式锁的核心源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950private boolean internalLockLoop(long startMillis, Long millisToWait, String ourPath) throws Exception&#123; boolean haveTheLock = false; boolean doDelete = false; try &#123; if ( revocable.get() != null ) &#123; client.getData().usingWatcher(revocableWatcher).forPath(ourPath); &#125; while ( (client.getState() == CuratorFrameworkState.STARTED) &amp;&amp; !haveTheLock ) &#123; // 获取当前所有节点排序后的集合 List&lt;String&gt; children = getSortedChildren(); // 获取当前节点的名称 String sequenceNodeName = ourPath.substring(basePath.length() + 1); // +1 to include the slash // 判断当前节点是否是最小的节点 PredicateResults predicateResults = driver.getsTheLock(client, children, sequenceNodeName, maxLeases); if ( predicateResults.getsTheLock() ) &#123; // 获取到锁 haveTheLock = true; &#125; else &#123; // 没获取到锁，对当前节点的上一个节点注册一个监听器 String previousSequencePath = basePath + "/" + predicateResults.getPathToWatch(); synchronized(this)&#123; Stat stat = client.checkExists().usingWatcher(watcher).forPath(previousSequencePath); if ( stat != null )&#123; if ( millisToWait != null ) &#123; millisToWait -= (System.currentTimeMillis() - startMillis); startMillis = System.currentTimeMillis(); if ( millisToWait &lt;= 0 ) &#123; doDelete = true; // timed out - delete our node break; &#125; wait(millisToWait); &#125;else&#123; wait(); &#125; &#125; &#125; // else it may have been deleted (i.e. lock released). Try to acquire again &#125; &#125; &#125; catch ( Exception e ) &#123; doDelete = true; throw e; &#125; finally&#123; if ( doDelete )&#123; deleteOurPath(ourPath); &#125; &#125; return haveTheLock;&#125; 其实curator实现分布式锁的底层原理和上面分析的是差不多的。这里我们用一张图详细描述其原理： 小结：本节介绍了zookeeperr实现分布式锁的方案以及zk的开源客户端的基本使用，简要的介绍了其实现原理。 两种方案的优缺点比较学完了两种分布式锁的实现方案之后，本节需要讨论的是redis和zk的实现方案中各自的优缺点。 对于redis的分布式锁而言，它有以下缺点： 它获取锁的方式简单粗暴，获取不到锁直接不断尝试获取锁，比较消耗性能。 另外来说的话，redis的设计定位决定了它的数据并不是强一致性的，在某些极端情况下，可能会出现问题。锁的模型不够健壮 即便使用redlock算法来实现，在某些复杂场景下，也无法保证其实现100%没有问题，关于redlock的讨论可以看How to do distributed locking redis分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能。 但是另一方面使用redis实现分布式锁在很多企业中非常常见，而且大部分情况下都不会遇到所谓的“极端复杂场景” 所以使用redis作为分布式锁也不失为一种好的方案，最重要的一点是redis的性能很高，可以支撑高并发的获取、释放锁操作。 对于zk分布式锁而言: zookeeper天生设计定位就是分布式协调，强一致性。锁的模型健壮、简单易用、适合做分布式锁。 如果获取不到锁，只需要添加一个监听器就可以了，不用一直轮询，性能消耗较小。 但是zk也有其缺点：如果有较多的客户端频繁的申请加锁、释放锁，对于zk集群的压力会比较大。 小结：综上所述，redis和zookeeper都有其优缺点。我们在做技术选型的时候可以根据这些问题作为参考因素。 作者的一些建议通过前面的分析，实现分布式锁的两种常见方案：redis和zookeeper，他们各有千秋。应该如何选型呢？ 就个人而言的话，我比较推崇zk实现的锁： 因为redis是有可能存在隐患的，可能会导致数据不对的情况。但是，怎么选用要看具体在公司的场景了。 如果公司里面有zk集群条件，优先选用zk实现，但是如果说公司里面只有redis集群，没有条件搭建zk集群。 那么其实用redis来实现也可以，另外还可能是系统设计者考虑到了系统已经有redis，但是又不希望再次引入一些外部依赖的情况下，可以选用redis。 这个是要系统设计者基于架构的考虑了 参考原文出处：微信公众号： 石杉的架构笔记 https://mp.weixin.qq.com/s/gUDMP5FVPfS7Id4IyHp02A]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>分布式</tag>
        <tag>并发</tag>
        <tag>线程</tag>
        <tag>Redis</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大型网站的页面静态化]]></title>
    <url>%2F2019%2F07%2F20%2F%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E7%9A%84%E9%A1%B5%E9%9D%A2%E9%9D%99%E6%80%81%E5%8C%96%2F</url>
    <content type="text"><![CDATA[前言我们小伙伴们在访问淘宝、网易等大型网站时有没有考虑到，网站首页、商品详情页以及新闻详情页面是如何处理的？怎么能够支撑这么大流量的访问呢？ 很多小伙伴们就会提出他们都采用了静态化的方案，这样用户请求直接获取静态数据html，就不需要访问数据库了，性能就会大大提高；而且提高网站SEO优化。 那今天就带着大家聊一下静态化。把之前工作场景中静态化方案遇到的问题，以及如何演变的，分享给小伙伴。 方案一：网页静态HTML化这个方案是最早使用的方案，我们就拿CMS系统举例，类似网易的新闻网站 核心流程图: 上图的核心思想： 1）管理后台调用新闻服务创建文章成功后，发送消息到消息队列 2）静态服务监听消息，把文章静态化，也就是生成html文件 3）在静态服务器上面安装一个文件同步工具，此工具的功能可以做到只同步有变动的文件，即做增量同步（老顾用久没用了，忘了工具的名称） 4）通过同步工具把html文件同步到所有的web服务器上面 这样的话就达到了，用户访问一些变化不大的页面时，是直接访问的html文件，直接在web服务器那边直接返回，不需要在访问数据库了，系统吞吐量比较高。 这个方案的问题： 1、网页布局样式僵化，无法修改 如果产品经理觉得新闻详情页面的布局要调整一下，现在的不够美观，或者加个其他模块，那就坑爹了，我们需要把所有的已经静态html化的文章全部重新静态化。这个是不现实的，因为像网易这么大的体量，新闻量是很大的，会被搞死。 2、页面会出现暂时间不一致 会出现用户刚刚再看最新的新闻，刷新一下又不存在了。这个是因为同步工具在同步到web服务器是要有时间的，同步到web服务器A上面了，但web服务器B还没有来得及同步。用户在访问的时候通过nginx进行负载均衡，随机把请求分配给web服务器的导致的。当然可以调整nginx负载均衡策略去解决。 3、Html文件太多，无法维护 这个是很明显的问题，html文件会越来越多，对存储空间要求很大，而且每台web服务器都一样，浪费磁盘空间；将来迁移维护也会带来很大的麻烦。 4、同步工具的不稳定 因为文件一旦多之后，同步工具稳定性就出现了问题 这个方案应该是比较传统的（不推荐） 方案二：伪静态化什么是伪静态？ 举个例子：我们一般访问一个文章，一般的链接地址为：http://www.xxx.com/news?id=1代表请求id为1的文章。**不过这种链接方式对SEO不是太友好（SEO对网站来说太重要了）**；所以一般进行改造：http://www.xxx.com/news/1.html 这样看上去就是个静态页面。一般我们可以采用nginx对url进行rewrite。小伙伴如何有兴趣可以自行了解，比较简单。 之所以是伪静态其实也是需要动态处理的。 针对方案一上面问题，方案进一步的演化，如下图 此方案的核心思想 1）管理后台调用新闻服务创建文章成功后，发送消息到消息队列 2）缓存服务监听消息，把文章内容缓存到缓存服务器上面 3）用户发起请求，web服务器根据id，直接查询缓存服务器 4）获取数据返回给用户 此方案就解决了方案一的一个大问题，就是html文件多的问题，因为不需要生成html，而且用缓存的方式，解决不需要访问数据库，提升系统吞吐量。 不过此方案的问题： 1、网页布局样式维护成本比较高，因为此方案照样是把所有的内容放到了缓存中，如果需要修改布局，需要重新设置缓存。 2、分布式缓存压力比较大，一旦缓存故障就导致所有请求会查询数据库，导致系统崩溃 还有个小问题，就是实时数据处理，就是页面中如价格，库存需要到后台读取的。当然小伙伴也许就会说，也可以处理啊，用户把商品内容请求到后，然后在用浏览器发送异步的ajax请求获得商品数量就好了啊。这样就是无形的增加了一次请求。（此问题可以忽略） 此方案类似很多公司都在使用，如：同程旅游等 方案三：布局样式模板化针对方案二的问题，我们可以采用openresty技术方案进行，利用http模板插件lua脚本进行解决，这里老顾不会介绍openresty+lua技术，有兴趣的小伙伴，可以到访问https://www.roncoo.com/view/139 这个视频课程。 如下图： 这里说明一下上图中我们小伙伴不需要全部都要了解，这个是比较全的商品详情页的解决方案，涉及到了三级缓存这个概念，在这里老顾就不深入讲三级缓存了。 我们主要看的是上面怎么会有两层ngnix，分发层和应用层，这个是什么意思？ 应用层nginx老顾先介绍一下应用层nginx是什么意思？nginx一般被用做负载均衡，其实nginx还有很多的功能，尤其他的openresty扩展 + lua脚本语言结合起来可以完成很多功能，小伙伴可以理解为lua脚本语言就是类似java语言，可以动态处理业务，如：本地缓存处理，远程http访问，访问redis等。 应用层nginx就是利用了http模板 + 缓存通过lua脚本完成的网页渲染 http模板 1）应用层nginx通过lua脚本语言先获取本地商品数据，然后和http模板进行渲染，形成最终商品详情页返回给用户 2）如果应用层nginx本地的缓存没有此商品数据，就通过lua脚本发起http请求访问web服务器，获取商品数据。 3）web服务器会向redis或本机的ehcache请求商品数据（这里涉及三级缓存概念），如果存在此商品数据，直接返回给用户；如果不存在则请求微服务访问数据库 这个思路就是通过http模板，解决了方案二中的布局样式的问题，如果需要调整布局，只要改一下模板就行了，非常方便。也解决了实时性问题。这边涉及到的nginx本地缓存其实就是为了保证不需要访问数据库，提升系统吞吐量。小伙伴只要了解一下思路，如果不了解openresty和lua可以自行上网了解，也可以联系老顾。 分发层ngnix为什么上面还有一层分发层呢？这个是因为大型网站的商品数太多了，应用层nginx的本地缓存是有限的，不可能把所有的商品数据缓存在同一个服务器的本地缓存；一台应用层nginx只能缓存部分商品数据，说到这里小伙伴是不是应该就知道为什么了吧？就是利用hash一致性算法，根据商品id路由分发到同一个应用层ngnix服务器。 分发层ngnix的作用就是hash策略的负载均衡，保证了商品id路由到固定的应用层服务器。 三级缓存保证了系统的稳定性，即使redis缓存崩溃，还有其他2个缓存保障。 总结： 方案三是比较完整的方案，很多大厂都在使用，能够承受亿级流量，但系统比较复杂。 如果对实时性要求不高，布局样式调整不频繁，可以考虑方案二，系统比较简单 参考原文出处： https://www.toutiao.com/i6671093883025228301/?group_id=6671093883025228301]]></content>
      <categories>
        <category>系统架构</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>缓存</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息中间件的面试四连炮]]></title>
    <url>%2F2019%2F07%2F20%2F%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E7%9A%84%E9%9D%A2%E8%AF%95%E5%9B%9B%E8%BF%9E%E7%82%AE%2F</url>
    <content type="text"><![CDATA[概述大家平时也有用到一些消息中间件(MQ)，但是对其理解可能仅停留在会使用API能实现生产消息、消费消息就完事了。 对MQ更加深入的问题，可能很多人没怎么思考过。 比如，你跳槽面试时，如果面试官看到你简历上写了，熟练掌握消息中间件，那么很可能给你发起如下 4 个面试连环炮！ 为什么要使用MQ？ 使用了MQ之后有什么优缺点？ 怎么保证MQ消息不丢失？ 怎么保证MQ的高可用性？ 本文将通过一些场景，配合着通俗易懂的语言和多张手绘彩图，讨论一下这些问题。 为什么要使用MQ？相信大家也听过这样的一句话：好的架构不是设计出来的，是演进出来的。 这句话在引入MQ的场景同样适用，使用MQ必定有其道理，是用来解决实际问题的。而不是看见别人用了，我也用着玩儿一下。 其实使用MQ的场景有挺多的，但是比较核心的有3个： 异步、解耦、削峰填谷 异步我们通过实际案例说明：假设A系统接收一个请求，需要在自己本地写库执行SQL，然后需要调用BCD三个系统的接口。 假设自己本地写库要3ms，调用BCD三个系统分别要300ms、450ms、200ms。 那么最终请求总延时是3 + 300 + 450 + 200 = 953ms，接近1s，可能用户会感觉太慢了。 此时整个系统大概是这样的： 但是一旦使用了MQ之后，系统A只需要发送3条消息到MQ中的3个消息队列，然后就返回给用户了。 假设发送消息到MQ中耗时20ms，那么用户感知到这个接口的耗时仅仅是20 + 3 = 23ms，用户几乎无感知，倍儿爽！ 此时整个系统结构大概是这样的： 可以看到，通过MQ的异步功能，可以大大提高接口的性能。 解耦假设A系统在用户发生某个操作的时候，需要把用户提交的数据同时推送到B、C两个系统的时候。 这个时候负责A系统的哥们想：没事啊，B、C两个系统给我提供一个Http接口或者RPC接口，我把数据推送过去不就完事了吗。负责A系统的哥们美滋滋。 如下图所示： 一切看起来很美好，但是随着业务快速迭代，这个时候系统D也想要这个数据。那既然这样，A系统的开发同学就改咯，在发送数据给BC的同时加上一个D。 但是，越到后面越发现，麻烦来了。。。 整个系统好像不止这个数据要发送给BCD、还有第二、第三个数据要发送给BCD。甚至有时候又加入了E、F等等系统，他们也要这个数据。 并且有时候可能B系统突然又不要这个数据了，A系统该来改去，A系统的开发哥们头皮发麻。 更复杂的场景是，数据通过接口传给其他系统有时候还要考虑重试、超时等一些异常情况，真是头发都白了呀。。。 来看下图，体会一下这无助的现场： 这个时候，就该我们的MQ粉墨登场了！ 这种情况下使用MQ来解耦是在合适不过了，因为负责A系统的哥们只需要把消息扔到MQ就行了，其他系统按需来订阅消息就好了。 就算某个系统不需要这个数据了，也不会需要A系统改动代码。 看看加入MQ解耦的下图，是不是清爽了很多！ 削峰填谷举个例子，比如我们的订单系统，在下单的时候就会往数据库写数据。但是数据库只能支撑每秒1000左右的并发写入，并发量再高就容易宕机。 低峰期的时候并发也就100多个，但是在高峰期时候，并发量会突然激增到5000以上，这个时候数据库肯定死了。 如下图，来感受一下数据库被打死的绝望： 但是使用了MQ之后，情况就变了！ 消息被MQ保存起来了，然后系统就可以按照自己的消费能力来消费，比如每秒1000个数据，这样慢慢写入数据库，这样就不会打死数据库了： 整个过程，如下图所示： 至于为什么叫做削峰填谷呢?来看看这个图： 如果没有用MQ的情况下，并发量高峰期的时候是有一个“顶峰”的，然后高峰期过后又是一个低并发的“谷”。 但是使用了MQ之后，限制消费消息的速度为1000，但是这样一来，高峰期产生的数据势必会被积压在MQ中，高峰就被“削”掉了。 但是因为消息积压，在高峰期过后的一段时间内，消费消息的速度还是会维持在1000QPS，直到消费完积压的消息,这就叫做“填谷” 通过上面的分析，大家就可以知道为什么要使用MQ，以及使用了MQ有什么好处。知其所以然，明白了自己的系统为什么要使用MQ。 这样以后别人问你为啥要用MQ，就不会出现 “我们组长要用MQ我们就用了” 这样尴尬的回答了。 使用了MQ之后有什么优缺点？看到这个问题蒙圈了，用了就用了嘛！优点上面已经说了，但是这个缺点是啥啊。好像没啥缺点啊。 如果你这样想，就大错特错了，在设计系统的过程中，除了要清楚的知道为什么要用这个东西，还要思考一下用了之后有什么坏处。这样才能心里有底，防范于未然。 接下来我们就讨论一下，用MQ会有什么缺点把？ 系统可用性降低大家想想一下，上面的说解耦的场景，本来A系统的哥们要把系统关键数据发送给BC系统的，现在突然加入了一个MQ了，现在BC系统接收数据要通过MQ来接收。 但是大家有没有考虑过一个问题，万一MQ挂了怎么办？这就引出一个问题，加入了MQ之后，系统的可用性是不是就降低了？ 因为多了一个风险因素：MQ可能会挂掉。只要MQ挂了，数据没了，系统运行就不对了。 系统复杂度提高本来我的系统通过接口调用一下就能完事的，但是加入一个MQ之后，需要考虑消息重复消费、消息丢失、甚至消息顺序性的问题 为了解决这些问题，又需要引入很多复杂的机制，这样一来是不是系统的复杂度提高了。 数据一致性问题本来好好的，A系统调用BC系统接口，如果BC系统出错了，会抛出异常，返回给A系统让A系统知道，这样的话就可以做回滚操作了 但是使用了MQ之后，A系统发送完消息就完事了，认为成功了。而刚好C系统写数据库的时候失败了，但是A认为C已经成功了？这样一来数据就不一致了。 通过分析引入MQ的优缺点之后，就明白了使用MQ有很多优点，但是会发现它带来的缺点又会需要你做各种额外的系统设计来弥补 最后你可能会发现整个系统复杂了好几倍，所以设计系统的时候要基于这些考虑做出取舍，很多时候你会发现该用的还是要用的。。。 怎么保证MQ消息不丢失？使用了MQ之后，还要关心消息丢失的问题。这里我们挑RabbitMQ来说明一下吧。 生产者弄丢了数据RabbitMQ生产者将数据发送到rabbitmq的时候,可能数据在网络传输中搞丢了，这个时候RabbitMQ收不到消息，消息就丢了。 RabbitMQ提供了两种方式来解决这个问题： 事务方式： 在生产者发送消息之前，通过channel.txSelect开启一个事务，接着发送消息 如果消息没有成功被RabbitMQ接收到，生产者会收到异常，此时就可以进行事务回滚channel.txRollback然后重新发送。假如RabbitMQ收到了这个消息，就可以提交事务channel.txCommit。 但是这样一来，生产者的吞吐量和性能都会降低很多，现在一般不这么干。 另外一种方式就是通过confirm机制： 这个confirm模式是在生产者哪里设置的，就是每次写消息的时候会分配一个唯一的id，然后RabbitMQ收到之后会回传一个ack，告诉生产者这个消息ok了。 如果rabbitmq没有处理到这个消息，那么就回调一个nack的接口，这个时候生产者就可以重发。 事务机制和cnofirm机制最大的不同在于事务机制是同步的，提交一个事务之后会阻塞在那儿 但是confirm机制是异步的，发送一个消息之后就可以发送下一个消息，然后那个消息rabbitmq接收了之后会异步回调你一个接口通知你这个消息接收到了。 所以一般在生产者这块避免数据丢失，都是用confirm机制的。 Rabbitmq弄丢了数据RabbitMQ集群也会弄丢消息，这个问题在官方文档的教程中也提到过，就是说在消息发送到RabbitMQ之后，默认是没有落地磁盘的，万一RabbitMQ宕机了，这个时候消息就丢失了。 所以为了解决这个问题，RabbitMQ提供了一个持久化的机制，消息写入之后会持久化到磁盘 这样哪怕是宕机了，恢复之后也会自动恢复之前存储的数据，这样的机制可以确保消息不会丢失。 设置持久化有两个步骤： 第一个是创建queue的时候将其设置为持久化的，这样就可以保证rabbitmq持久化queue的元数据，但是不会持久化queue里的数据 第二个是发送消息的时候将消息的deliveryMode设置为2，就是将消息设置为持久化的，此时rabbitmq就会将消息持久化到磁盘上去。 但是这样一来可能会有人说：万一消息发送到RabbitMQ之后，还没来得及持久化到磁盘就挂掉了，数据也丢失了，怎么办？ 对于这个问题，其实是配合上面的confirm机制一起来保证的，就是在消息持久化到磁盘之后才会给生产者发送ack消息。 万一真的遇到了那种极端的情况，生产者是可以感知到的，此时生产者可以通过重试发送消息给别的RabbitMQ节点 消费端弄丢了数据RabbitMQ消费端弄丢了数据的情况是这样的：在消费消息的时候，刚拿到消息，结果进程挂了，这个时候RabbitMQ就会认为你已经消费成功了，这条数据就丢了。 对于这个问题，要先说明一下RabbitMQ消费消息的机制：在消费者收到消息的时候，会发送一个ack给RabbitMQ，告诉RabbitMQ这条消息被消费到了，这样RabbitMQ就会把消息删除。 但是默认情况下这个发送ack的操作是自动提交的，也就是说消费者一收到这个消息就会自动返回ack给RabbitMQ，所以会出现丢消息的问题。 所以针对这个问题的解决方案就是：关闭RabbitMQ消费者的自动提交ack,在消费者处理完这条消息之后再手动提交ack。 这样即使遇到了上面的情况，RabbitMQ也不会把这条消息删除，会在你程序重启之后，重新下发这条消息过来。 怎么保证MQ的高可用性性？使用了MQ之后，我们肯定是希望MQ有高可用特性，因为不可能接受机器宕机了，就无法收发消息的情况。 这一块我们也是基于RabbitMQ这种经典的MQ来说明一下： RabbitMQ是比较有代表性的，因为是基于主从做高可用性的，我们就以他为例子讲解第一种MQ的高可用性怎么实现。 rabbitmq有三种模式：单机模式，普通集群模式，镜像集群模式 单机模式单机模式就是demo级别的，就是说只有一台机器部署了一个RabbitMQ程序。 这个会存在单点问题，宕机就玩完了，没什么高可用性可言。一般就是你本地启动了玩玩儿的，没人生产用单机模式。 普通集群模式这个模式的意思就是在多台机器上启动多个rabbitmq实例。类似的master-slave模式一样。 但是创建的queue，只会放在一个master rabbtimq实例上，其他实例都同步那个接收消息的RabbitMQ元数据。 在消费消息的时候，如果你连接到的RabbitMQ实例不是存放Queue数据的实例，这个时候RabbitMQ就会从存放Queue数据的实例上拉去数据，然后返回给客户端。 总的来说，这种方式有点麻烦，没有做到真正的分布式，每次消费者连接一个实例后拉取数据，如果连接到不是存放queue数据的实例，这个时候会造成额外的性能开销。如果从放Queue的实例拉取，会导致单实例性能瓶颈。 如果放queue的实例宕机了，会导致其他实例无法拉取数据，这个集群都无法消费消息了，没有做到真正的高可用。 所以这个事儿就比较尴尬了，这就没有什么所谓的高可用性可言了，这方案主要是提高吞吐量的，就是说让集群中多个节点来服务某个queue的读写操作。 镜像集群模式镜像集群模式才是真正的rabbitmq的高可用模式，跟普通集群模式不一样的是：创建的queue无论元数据还是queue里的消息都会存在于多个实例上， 每次写消息到queue的时候，都会自动把消息到多个实例的queue里进行消息同步。 这样的话任何一个机器宕机了别的实例都可以用提供服务，这样就做到了真正的高可用了。 但是也存在着不好之处： 性能开销过高，消息需要同步所有机器，会导致网络带宽压力和消耗很重 扩展性低：无法解决某个queue数据量特别大的情况，导致queue无法线性拓展。 就算加了机器，那个机器也会包含queue的所有数据，queue的数据没有做到分布式存储。 对于RabbitMQ的高可用一般的做法都是开启镜像集群模式，这样起码来说做到了高可用，一个节点宕机了，其他节点可以继续提供服务。 总结通过本篇文章，分析了对于MQ的一些常规问题： 为什么使用MQ？ 使用MQ有什么优缺点 如何保证消息不丢失？ 如何保证MQ高可用性？ 但是，这些问题仅仅是使用MQ的其中一部分需要考虑的问题，事实上，还有其他更加复杂的问题需要我们去解决， 比如：如何保证消息的顺序性？消息队列如何选型？消息积压问题如何解决? 本文仅仅是针对RabbitMQ的场景举例子。还有其他比较的消息队列，比如RocketMQ、Kafka 参考原文出处：微信公众号： 石杉的架构笔记 https://mp.weixin.qq.com/s/6LdjhwRGvwYozTt2sr72PQ]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为了追求极致的性能，Kafka掌控了这11项要领！]]></title>
    <url>%2F2019%2F07%2F20%2F%E4%B8%BA%E4%BA%86%E8%BF%BD%E6%B1%82%E6%9E%81%E8%87%B4%E7%9A%84%E6%80%A7%E8%83%BD%EF%BC%8CKafka%E6%8E%8C%E6%8E%A7%E4%BA%86%E8%BF%9911%E9%A1%B9%E8%A6%81%E9%A2%86%EF%BC%81%2F</url>
    <content type="text"><![CDATA[很多同学私信问我Kafka在性能优化方面做了哪些举措，对于相关问题的答案其实我早就写过了，就是没有系统的整理一篇，最近思考着花点时间来整理一下，下次再有同学问我相关的问题我就可以潇洒的甩个链接了。这个问题也是Kafka面试的时候的常见问题，面试官问你这个问题也不算刁难你。在网上也有很多相关的文章开讲解这个问题，比如之前各大公众号转载的“为什么Kafka这么快？”，这些文章我看了，写的不错，问题在于只是罗列了部分的要领，没有全部的详述出来。本文所罗列的要领会比你们网上搜寻到的都多，如果你在看完本篇文章之后，在面试的时候遇到相关问题，相信你一定能让面试官眼前一亮。 批量处理传统消息中间件的消息发送和消费整体上是针对单条的。对于生产者而言，它先发一条消息，然后broker返回ACK表示已接收，这里产生2次rpc；对于消费者而言，它先请求接受消息，然后broker返回消息，最后发送ACK表示已消费，这里产生了3次rpc（有些消息中间件会优化一下，broker返回的时候返回多条消息）。而Kafka采用了批量处理：生产者聚合了一批消息，然后再做2次rpc将消息存入broker，这原本是需要很多次的rpc才能完成的操作。假设需要发送1000条消息，每条消息大小1KB，那么传统的消息中间件需要2000次rpc，而Kafka可能会把这1000条消息包装成1个1MB的消息，采用2次rpc就完成了任务。这一改进举措一度被认为是一种“作弊”的行为，然而在微批次理念盛行的今日，其它消息中间件也开始纷纷效仿。 客户端优化这里接着批量处理的概念继续来说，新版生产者客户端摒弃了以往的单线程，而采用了双线程：主线程和Sender线程。主线程负责将消息置入客户端缓存，Sender线程负责从缓存中发送消息，而这个缓存会聚合多个消息为一个批次。有些消息中间件会把消息直接扔到broker。 日志格式Kafka从0.8版本开始日志格式历经了三次变革：v0、v1、v2。在之前发过的一篇文章《一文看懂Kafka消息格式的演变》中详细介绍了Kafka日志格式，Kafka的日志格式越来越利于批量消息的处理，有兴趣的同学可以阅读一下这篇文章以作了解。 日志编码如果了解了Kafka具体的日志格式（可以参考上图），那么你应该了解日志（Record，或者称之为消息）本身除了基本的key和value之外，还有一些其它的字段，原本这些附加字段按照固定的大小占用一定的篇幅（参考上图左），而Kafka最新的版本中采用了变成字段Varints和ZigZag编码，有效地降低了这些附加字段的占用大小。日志（消息）尽可能变小了，那么网络传输的效率也会变高，日志存盘的效率也会提升，从而整理的性能也会有所提升。 消息压缩Kafka支持多种消息压缩方式（gzip、snappy、lz4）。对消息进行压缩可以极大地减少网络传输 量、降低网络 I/O，从而提高整体的性能。消息压缩是一种使用时间换空间的优化方式，如果对 时延有一定的要求，则不推荐对消息进行压缩。 建立索引，方便快速定位查询每个日志分段文件对应了两个索引文件，主要用来提高查找消息的效率，这也是提升性能的一种方式。（具体的内容在书中的第5章有详细的讲解，公众号里好像忘记发表了，找了一圈没找到） 分区很多人会忽略掉这个因素，其实分区也是提升性能的一种非常有效的方式，这种方式所带来的效果会比前面所说的日志编码、消息压缩等更加的明显。分区在其他分布式组件中也有大量涉及，至于为什么分区能够提升性能这种基本知识在这里就不在赘述了。不过需要注意，一昧地增加分区并不能一直带来性能的提升，有兴趣的同学可以看一下这篇《Kafka主题中的分区数越多吞吐量就越高？》。 一致性绝大多数的资料在讲述Kafka性能优化的举措之时是不会提及一致性的东西的。我们所了解的通用的一致性协议如Paxos、Raft、Gossip等，而Kafka另辟蹊径采用类似PacificA的做法不是“拍大腿”拍出来的，采用这种模型会提升整理的效率。具体的细节后面会整理一篇，类似《在Kafka中使用Raft替换PacificA的可行性分析及优缺点》。 顺序写盘操作系统可以针对线性读写做深层次的优化，比如预读(read-ahead，提前将一个比较大的磁盘块读入内存) 和后写(write-behind，将很多小的逻辑写操作合并起来组成一个大的物理写操作)技术。Kafka 在设计时采用了文件追加的方式来写入消息，即只能在日志文件的尾部追加新的消 息，并且也不允许修改已写入的消息，这种方式属于典型的顺序写盘的操作，所以就算 Kafka 使用磁盘作为存储介质，它所能承载的吞吐量也不容小觑。 页缓存为什么Kafka性能这么高？当遇到这个问题的时候很多人都会想到上面的顺序写盘这一点。其实在顺序斜盘前面还有页缓存（PageCache）这一层的优化。 页缓存是操作系统实现的一种主要的磁盘缓存，以此用来减少对磁盘 I/O 的操作。具体 来说，就是把磁盘中的数据缓存到内存中，把对磁盘的访问变为对内存的访问。为了弥补性 能上的差异，现代操作系统越来越“激进地”将内存作为磁盘缓存，甚至会非常乐意将所有 可用的内存用作磁盘缓存，这样当内存回收时也几乎没有性能损失，所有对于磁盘的读写也 将经由统一的缓存。 当一个进程准备读取磁盘上的文件内容时，操作系统会先查看待读取的数据所在的页 (page)是否在页缓存(pagecache)中，如果存在(命中)则直接返回数据，从而避免了对物 理磁盘的 I/O 操作;如果没有命中，则操作系统会向磁盘发起读取请求并将读取的数据页存入 页缓存，之后再将数据返回给进程。同样，如果一个进程需要将数据写入磁盘，那么操作系统也会检测数据对应的页是否在页缓存中，如果不存在，则会先在页缓存中添加相应的页，最后将数据写入对应的页。被修改过后的页也就变成了脏页，操作系统会在合适的时间把脏页中的 数据写入磁盘，以保持数据的一致性。 对一个进程而言，它会在进程内部缓存处理所需的数据，然而这些数据有可能还缓存在操 作系统的页缓存中，因此同一份数据有可能被缓存了两次。并且，除非使用 Direct I/O 的方式， 否则页缓存很难被禁止。此外，用过 Java 的人一般都知道两点事实:对象的内存开销非常大， 通常会是真实数据大小的几倍甚至更多，空间使用率低下;Java 的垃圾回收会随着堆内数据的 增多而变得越来越慢。基于这些因素，使用文件系统并依赖于页缓存的做法明显要优于维护一 个进程内缓存或其他结构，至少我们可以省去了一份进程内部的缓存消耗，同时还可以通过结构紧凑的字节码来替代使用对象的方式以节省更多的空间。如此，我们可以在 32GB 的机器上使用 28GB 至 30GB 的内存而不用担心 GC 所带来的性能问题。此外，即使 Kafka 服务重启， 页缓存还是会保持有效，然而进程内的缓存却需要重建。这样也极大地简化了代码逻辑，因为 维护页缓存和文件之间的一致性交由操作系统来负责，这样会比进程内维护更加安全有效。 Kafka 中大量使用了页缓存，这是 Kafka 实现高吞吐的重要因素之一。虽然消息都是先被写入页缓存，然后由操作系统负责具体的刷盘任务的。 零拷贝我在很久之前就之前就发过一篇《什么是Zero Copy》,如果对Zero Copy不了解的同学可以翻阅一下。Kafka使用了Zero Copy技术提升了消费的效率。前面所说的Kafka将消息先写入页缓存，如果消费者在读取消息的时候如果在页缓存中可以命中，那么可以直接从页缓存中读取，这样又节省了一次从磁盘到页缓存的copy开销。另外对于读写的概念可以进一步了解一下什么是写放大和读放大。 附一个磁盘IO流程可以参考下图： 具体解析参考《Linux IO磁盘篇整理小记》。 写在最后本文罗列了一些Kafka的在性能优化方面的要领。本文中的所有内容都在《深入理解Kafka》一书中有讲解，只是散落在各处而已，按照既定的顺序编排，力求从易入难。如果在书中再采用篇幅去罗列类似主题的话，会出现知识讲解的冗余，故没有在书中再次整理赘述，不过这些内容会在公众号里发表出来，前面已经按照其它维度整理过好几篇了。如果需要新的维度内容，可以在公众号里留言，诉求很大的话我会对此整理一篇的，这篇文章就是这么来的。 参考原文出处：微信公众号： 朱小厮的博客 https://mp.weixin.qq.com/s/JyQaohyDPndFJDrw4AOWww]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解Docker容器和镜像]]></title>
    <url>%2F2019%2F06%2F18%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Docker%E5%AE%B9%E5%99%A8%E5%92%8C%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[这篇文章希望能够帮助读者深入理解Docker的命令，还有容器（container）和镜像（image）之间的区别，并深入探讨容器和运行中的容器之间的区别。 当我对Docker技术还是一知半解的时候，我发现理解Docker的命令非常困难。于是，我花了几周的时间来学习Docker的工作原理，更确切地说，是关于Docker统一文件系统（the union file system）的知识，然后回过头来再看Docker的命令，一切变得顺理成章，简单极了。 题外话：就我个人而言，掌握一门技术并合理使用它的最好办法就是深入理解这项技术背后的工作原理。通常情况下，一项新技术的诞生常常会伴随着媒体的大肆宣传和炒作，这使得用户很难看清技术的本质。更确切地说，新技术总是会发明一些新的术语或者隐喻词来帮助宣传，这在初期是非常有帮助的，但是这给技术的原理蒙上了一层砂纸，不利于用户在后期掌握技术的真谛。 Git就是一个很好的例子。我之前不能够很好的使用Git，于是我花了一段时间去学习Git的原理，直到这时，我才真正明白了Git的用法。我坚信只有真正理解Git内部原理的人才能够掌握这个工具。 Image Definition镜像（Image）就是一堆只读层（read-only layer）的统一视角，也许这个定义有些难以理解，下面的这张图能够帮助读者理解镜像的定义。 从左边我们看到了多个只读层，它们重叠在一起。除了最下面一层，其它层都会有一个指针指向下一层。这些层是Docker内部的实现细节，并且能够在主机（译者注：运行Docker的机器）的文件系统上访问到。统一文件系统（union file system）技术能够将不同的层整合成一个文件系统，为这些层提供了一个统一的视角，这样就隐藏了多层的存在，在用户的角度看来，只存在一个文件系统。我们可以在图片的右边看到这个视角的形式。 你可以在你的主机文件系统上找到有关这些层的文件。需要注意的是，在一个运行中的容器内部，这些层是不可见的。在我的主机上，我发现它们存于/var/lib/docker/aufs目录下。 1sudo tree -L 1 /var/lib/docker//var/lib/docker/ Container Definition容器（container）的定义和镜像（image）几乎一模一样，也是一堆层的统一视角，唯一区别在于容器的最上面那一层是可读可写的。 细心的读者可能会发现，容器的定义并没有提及容器是否在运行，没错，这是故意的。正是这个发现帮助我理解了很多困惑。 要点：容器 = 镜像 + 读写层。并且容器的定义并没有提及是否要运行容器。 接下来，我们将会讨论运行态容器。 Running Container Definition一个运行态容器（running container）被定义为一个可读写的统一文件系统加上隔离的进程空间和包含其中的进程。下面这张图片展示了一个运行中的容器。 正是文件系统隔离技术使得Docker成为了一个前途无量的技术。一个容器中的进程可能会对文件进行修改、删除、创建，这些改变都将作用于可读写层（read-write layer）。下面这张图展示了这个行为。 我们可以通过运行以下命令来验证我们上面所说的： 1docker run ubuntu touch happiness.txt 即便是这个ubuntu容器不再运行，我们依旧能够在主机的文件系统上找到这个新文件。 1find / -name happiness.txt Image Layer Definition为了将零星的数据整合起来，我们提出了镜像层（image layer）这个概念。下面的这张图描述了一个镜像层，通过图片我们能够发现一个层并不仅仅包含文件系统的改变，它还能包含了其他重要信息。 元数据（metadata）就是关于这个层的额外信息，它不仅能够让Docker获取运行和构建时的信息，还包括父层的层次信息。需要注意，只读层和读写层都包含元数据。 除此之外，每一层都包括了一个指向父层的指针。如果一个层没有这个指针，说明它处于最底层。 Metadata Location:我发现在我自己的主机上，镜像层（image layer）的元数据被保存在名为”json”的文件中，比如说： 1/var/lib/docker/graph/e809f156dc985.../json e809f156dc985…就是这层的id。 一个容器的元数据好像是被分成了很多文件，但或多或少能够在/var/lib/docker/containers/目录下找到，就是一个可读层的id。这个目录下的文件大多是运行时的数据，比如说网络，日志等等。 全局理解（Tying It All Together）现在，让我们结合上面提到的实现细节来理解Docker的命令。 1docker create &lt;image-id&gt; docker create 命令为指定的镜像（image）添加了一个可读层，构成了一个新的容器。注意，这个容器并没有运行。 1docker start &lt;container-id&gt; Docker start命令为容器文件系统创建了一个进程隔离空间。注意，每一个容器只能够有一个进程隔离空间。 1docker run &lt;image-id&gt; 看到这个命令，读者通常会有一个疑问：docker start 和 docker run命令有什么区别。 从图片可以看出，docker run 命令先是利用镜像创建了一个容器，然后运行这个容器。这个命令非常的方便，并且隐藏了两个命令的细节，但从另一方面来看，这容易让用户产生误解。 题外话：继续我们之前有关于Git的话题，我认为docker run命令类似于git pull命令。git pull命令就是git fetch 和 git merge两个命令的组合，同样的，docker run就是docker create和docker start两个命令的组合。 1docker ps docker ps 命令会列出所有运行中的容器。这隐藏了非运行态容器的存在，如果想要找出这些容器，我们需要使用下面这个命令。 1docker ps –a docker ps –a命令会列出所有的容器，不管是运行的，还是停止的。 1docker images docker images命令会列出了所有顶层（top-level）镜像。实际上，在这里我们没有办法区分一个镜像和一个只读层，所以我们提出了top-level镜像。只有创建容器时使用的镜像或者是直接pull下来的镜像能被称为顶层（top-level）镜像，并且每一个顶层镜像下面都隐藏了多个镜像层。 1docker images –a docker images –a命令列出了所有的镜像，也可以说是列出了所有的可读层。如果你想要查看某一个image-id下的所有层，可以使用docker history来查看。 1docker stop &lt;container-id&gt; docker stop命令会向运行中的容器发送一个SIGTERM的信号，然后停止所有的进程。 1docker kill &lt;container-id&gt; docker kill 命令向所有运行在容器中的进程发送了一个不友好的SIGKILL信号。 1docker pause &lt;container-id&gt; docker stop和docker kill命令会发送UNIX的信号给运行中的进程，docker pause命令则不一样，它利用了cgroups的特性将运行中的进程空间暂停。具体的内部原理你可以在这里找到：https://www.kernel.org/doc/Documentation/cgroups/freezer-subsystem.txt，但是这种方式的不足之处在于发送一个SIGTSTP信号对于进程来说不够简单易懂，以至于不能够让所有进程暂停。 1docker rm &lt;container-id&gt; docker rm命令会移除构成容器的可读写层。注意，这个命令只能对非运行态容器执行。 1docker rmi &lt;image-id&gt; docker rmi 命令会移除构成镜像的一个只读层。你只能够使用docker rmi来移除最顶层（top level layer）（也可以说是镜像），你也可以使用-f参数来强制删除中间的只读层。 1docker commit &lt;container-id&gt; docker commit命令将容器的可读写层转换为一个只读层，这样就把一个容器转换成了不可变的镜像。 1docker build docker build命令非常有趣，它会反复的执行多个命令。 我们从上图可以看到，build命令根据Dockerfile文件中的FROM指令获取到镜像，然后重复地1）run（create和start）、2）修改、3）commit。在循环中的每一步都会生成一个新的层，因此许多新的层会被创建。 1docker exec &lt;running-container-id&gt; docker exec 命令会在运行中的容器执行一个新进程。 1docker inspect &lt;container-id&gt; or &lt;image-id&gt; docker inspect命令会提取出容器或者镜像最顶层的元数据。 1docker save &lt;image-id&gt; docker save命令会创建一个镜像的压缩文件，这个文件能够在另外一个主机的Docker上使用。和export命令不同，这个命令为每一个层都保存了它们的元数据。这个命令只能对镜像生效。 1docker export &lt;container-id&gt; docker export命令创建一个tar文件，并且移除了元数据和不必要的层，将多个层整合成了一个层，只保存了当前统一视角看到的内容（译者注：expoxt后的容器再import到Docker中，通过docker images –tree命令只能看到一个镜像；而save后的镜像则不同，它能够看到这个镜像的历史镜像）。 1docker history &lt;image-id&gt; docker history命令递归地输出指定镜像的历史镜像。 参考http://dockone.io/article/783 http://sina.lt/gfmf]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里淘宝的高并发分布式架构演进之路]]></title>
    <url>%2F2019%2F06%2F17%2F%E9%98%BF%E9%87%8C%E6%B7%98%E5%AE%9D%E7%9A%84%E9%AB%98%E5%B9%B6%E5%8F%91%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B%E4%B9%8B%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[本文以淘宝作为例子，介绍从一百个并发到千万级并发情况下服务端的架构的演进过程，同时列举出每个演进阶段会遇到的相关技术，让大家对架构的演进有一个整体的认知，文章最后汇总了一些架构设计的原则。 基本概念在介绍架构之前，为了避免部分读者对架构设计中的一些概念不了解，下面对几个最基础的概念进行介绍： 分布式系统中的多个模块在不同服务器上部署，即可称为分布式系统，如Tomcat和数据库分别部署在不同的服务器上，或两个相同功能的Tomcat分别部署在不同服务器上 高可用系统中部分节点失效时，其他节点能够接替它继续提供服务，则可认为系统具有高可用性 集群一个特定领域的软件部署在多台服务器上并作为一个整体提供一类服务，这个整体称为集群。如Zookeeper中的Master和Slave分别部署在多台服务器上，共同组成一个整体提供集中配置服务。在常见的集群中，客户端往往能够连接任意一个节点获得服务，并且当集群中一个节点掉线时，其他节点往往能够自动的接替它继续提供服务，这时候说明集群具有高可用性 负载均衡请求发送到系统时，通过某些方式把请求均匀分发到多个节点上，使系统中每个节点能够均匀的处理请求负载，则可认为系统是负载均衡的 正向代理和反向代理系统内部要访问外部网络时，统一通过一个代理服务器把请求转发出去，在外部网络看来就是代理服务器发起的访问，此时代理服务器实现的是正向代理；当外部请求进入系统时，代理服务器把该请求转发到系统中的某台服务器上，对外部请求来说，与之交互的只有代理服务器，此时代理服务器实现的是反向代理。简单来说，正向代理是代理服务器代替系统内部来访问外部网络的过程，反向代理是外部请求访问系统时通过代理服务器转发到内部服务器的过程。 架构演进单机架构 以淘宝作为例子。在网站最初时，应用数量与用户数都较少，可以把Tomcat和数据库部署在同一台服务器上。浏览器往www.taobao.com发起请求时，首先经过DNS服务器（域名系统）把域名转换为实际IP地址10.102.4.1，浏览器转而访问该IP对应的Tomcat。 随着用户数的增长，Tomcat和数据库之间竞争资源，单机性能不足以支撑业务 第一次演进：Tomcat与数据库分开部署 Tomcat和数据库分别独占服务器资源，显著提高两者各自性能。 随着用户数的增长，并发读写数据库成为瓶颈 第二次演进：引入本地缓存和分布式缓存 在Tomcat同服务器上或同JVM中增加本地缓存，并在外部增加分布式缓存，缓存热门商品信息或热门商品的html页面等。通过缓存能把绝大多数请求在读写数据库前拦截掉，大大降低数据库压力。其中涉及的技术包括：使用memcached作为本地缓存，使用Redis作为分布式缓存，还会涉及缓存一致性、缓存穿透/击穿、缓存雪崩、热点数据集中失效等问题。 缓存抗住了大部分的访问请求，随着用户数的增长，并发压力主要落在单机的Tomcat上，响应逐渐变慢 第三次演进：引入反向代理实现负载均衡 在多台服务器上分别部署Tomcat，使用反向代理软件（Nginx）把请求均匀分发到每个Tomcat中。此处假设Tomcat最多支持100个并发，Nginx最多支持50000个并发，那么理论上Nginx把请求分发到500个Tomcat上，就能抗住50000个并发。其中涉及的技术包括：Nginx、HAProxy，两者都是工作在网络第七层的反向代理软件，主要支持http协议，还会涉及session共享、文件上传下载的问题。 反向代理使应用服务器可支持的并发量大大增加，但并发量的增长也意味着更多请求穿透到数据库，单机的数据库最终成为瓶颈 第四次演进：数据库读写分离 把数据库划分为读库和写库，读库可以有多个，通过同步机制把写库的数据同步到读库，对于需要查询最新写入数据场景，可通过在缓存中多写一份，通过缓存获得最新数据。其中涉及的技术包括：Mycat，它是数据库中间件，可通过它来组织数据库的分离读写和分库分表，客户端通过它来访问下层数据库，还会涉及数据同步，数据一致性的问题。 业务逐渐变多，不同业务之间的访问量差距较大，不同业务直接竞争数据库，相互影响性能 第五次演进：数据库按业务分库 把不同业务的数据保存到不同的数据库中，使业务之间的资源竞争降低，对于访问量大的业务，可以部署更多的服务器来支撑。这样同时导致跨业务的表无法直接做关联分析，需要通过其他途径来解决，但这不是本文讨论的重点，有兴趣的可以自行搜索解决方案。 随着用户数的增长，单机的写库会逐渐会达到性能瓶颈 第六次演进：把大表拆分为小表 比如针对评论数据，可按照商品ID进行hash，路由到对应的表中存储；针对支付记录，可按照小时创建表，每个小时表继续拆分为小表，使用用户ID或记录编号来路由数据。只要实时操作的表数据量足够小，请求能够足够均匀的分发到多台服务器上的小表，那数据库就能通过水平扩展的方式来提高性能。其中前面提到的Mycat也支持在大表拆分为小表情况下的访问控制。 这种做法显著的增加了数据库运维的难度，对DBA的要求较高。数据库设计到这种结构时，已经可以称为分布式数据库，但是这只是一个逻辑的数据库整体，数据库里不同的组成部分是由不同的组件单独来实现的，如分库分表的管理和请求分发，由Mycat实现，SQL的解析由单机的数据库实现，读写分离可能由网关和消息队列来实现，查询结果的汇总可能由数据库接口层来实现等等，这种架构其实是MPP（大规模并行处理）架构的一类实现。 目前开源和商用都已经有不少MPP数据库，开源中比较流行的有Greenplum、TiDB、Postgresql XC、HAWQ等，商用的如南大通用的GBase、睿帆科技的雪球DB、华为的LibrA等等，不同的MPP数据库的侧重点也不一样，如TiDB更侧重于分布式OLTP场景，Greenplum更侧重于分布式OLAP场景，这些MPP数据库基本都提供了类似Postgresql、Oracle、MySQL那样的SQL标准支持能力，能把一个查询解析为分布式的执行计划分发到每台机器上并行执行，最终由数据库本身汇总数据进行返回，也提供了诸如权限管理、分库分表、事务、数据副本等能力，并且大多能够支持100个节点以上的集群，大大降低了数据库运维的成本，并且使数据库也能够实现水平扩展。 数据库和Tomcat都能够水平扩展，可支撑的并发大幅提高，随着用户数的增长，最终单机的Nginx会成为瓶颈 第七次演进：使用LVS或F5来使多个Nginx负载均衡 由于瓶颈在Nginx，因此无法通过两层的Nginx来实现多个Nginx的负载均衡。图中的LVS和F5是工作在网络第四层的负载均衡解决方案，其中LVS是软件，运行在操作系统内核态，可对TCP请求或更高层级的网络协议进行转发，因此支持的协议更丰富，并且性能也远高于Nginx，可假设单机的LVS可支持几十万个并发的请求转发；F5是一种负载均衡硬件，与LVS提供的能力类似，性能比LVS更高，但价格昂贵。由于LVS是单机版的软件，若LVS所在服务器宕机则会导致整个后端系统都无法访问，因此需要有备用节点。可使用keepalived软件模拟出虚拟IP，然后把虚拟IP绑定到多台LVS服务器上，浏览器访问虚拟IP时，会被路由器重定向到真实的LVS服务器，当主LVS服务器宕机时，keepalived软件会自动更新路由器中的路由表，把虚拟IP重定向到另外一台正常的LVS服务器，从而达到LVS服务器高可用的效果。 此处需要注意的是，上图中从Nginx层到Tomcat层这样画并不代表全部Nginx都转发请求到全部的Tomcat，在实际使用时，可能会是几个Nginx下面接一部分的Tomcat，这些Nginx之间通过keepalived实现高可用，其他的Nginx接另外的Tomcat，这样可接入的Tomcat数量就能成倍的增加。 由于LVS也是单机的，随着并发数增长到几十万时，LVS服务器最终会达到瓶颈，此时用户数达到千万甚至上亿级别，用户分布在不同的地区，与服务器机房距离不同，导致了访问的延迟会明显不同 第八次演进：通过DNS轮询实现机房间的负载均衡 在DNS服务器中可配置一个域名对应多个IP地址，每个IP地址对应到不同的机房里的虚拟IP。当用户访问www.taobao.com时，DNS服务器会使用轮询策略或其他策略，来选择某个IP供用户访问。此方式能实现机房间的负载均衡，至此，系统可做到机房级别的水平扩展，千万级到亿级的并发量都可通过增加机房来解决，系统入口处的请求并发量不再是问题。 随着数据的丰富程度和业务的发展，检索、分析等需求越来越丰富，单单依靠数据库无法解决如此丰富的需求 第九次演进：引入NoSQL数据库和搜索引擎等技术 当数据库中的数据多到一定规模时，数据库就不适用于复杂的查询了，往往只能满足普通查询的场景。对于统计报表场景，在数据量大时不一定能跑出结果，而且在跑复杂查询时会导致其他查询变慢，对于全文检索、可变数据结构等场景，数据库天生不适用。因此需要针对特定的场景，引入合适的解决方案。如对于海量文件存储，可通过分布式文件系统HDFS解决，对于key value类型的数据，可通过HBase和Redis等方案解决，对于全文检索场景，可通过搜索引擎如ElasticSearch解决，对于多维分析场景，可通过Kylin或Druid等方案解决。 当然，引入更多组件同时会提高系统的复杂度，不同的组件保存的数据需要同步，需要考虑一致性的问题，需要有更多的运维手段来管理这些组件等。 引入更多组件解决了丰富的需求，业务维度能够极大扩充，随之而来的是一个应用中包含了太多的业务代码，业务的升级迭代变得困难 第十次演进：大应用拆分为小应用 按照业务板块来划分应用代码，使单个应用的职责更清晰，相互之间可以做到独立升级迭代。这时候应用之间可能会涉及到一些公共配置，可以通过分布式配置中心Zookeeper来解决。 不同应用之间存在共用的模块，由应用单独管理会导致相同代码存在多份，导致公共功能升级时全部应用代码都要跟着升级 第十一次演进：复用的功能抽离成微服务 如用户管理、订单、支付、鉴权等功能在多个应用中都存在，那么可以把这些功能的代码单独抽取出来形成一个单独的服务来管理，这样的服务就是所谓的微服务，应用和服务之间通过HTTP、TCP或RPC请求等多种方式来访问公共服务，每个单独的服务都可以由单独的团队来管理。此外，可以通过Dubbo、SpringCloud等框架实现服务治理、限流、熔断、降级等功能，提高服务的稳定性和可用性。 不同服务的接口访问方式不同，应用代码需要适配多种访问方式才能使用服务，此外，应用访问服务，服务之间也可能相互访问，调用链将会变得非常复杂，逻辑变得混乱 第十二次演进：引入企业服务总线ESB屏蔽服务接口的访问差异 通过ESB统一进行访问协议转换，应用统一通过ESB来访问后端服务，服务与服务之间也通过ESB来相互调用，以此降低系统的耦合程度。这种单个应用拆分为多个应用，公共服务单独抽取出来来管理，并使用企业消息总线来解除服务之间耦合问题的架构，就是所谓的SOA（面向服务）架构，这种架构与微服务架构容易混淆，因为表现形式十分相似。个人理解，微服务架构更多是指把系统里的公共服务抽取出来单独运维管理的思想，而SOA架构则是指一种拆分服务并使服务接口访问变得统一的架构思想，SOA架构中包含了微服务的思想。 业务不断发展，应用和服务都会不断变多，应用和服务的部署变得复杂，同一台服务器上部署多个服务还要解决运行环境冲突的问题，此外，对于如大促这类需要动态扩缩容的场景，需要水平扩展服务的性能，就需要在新增的服务上准备运行环境，部署服务等，运维将变得十分困难 第十三次演进：引入容器化技术实现运行环境隔离与动态服务管理 目前最流行的容器化技术是Docker，最流行的容器管理服务是Kubernetes(K8S)，应用/服务可以打包为Docker镜像，通过K8S来动态分发和部署镜像。Docker镜像可理解为一个能运行你的应用/服务的最小的操作系统，里面放着应用/服务的运行代码，运行环境根据实际的需要设置好。把整个“操作系统”打包为一个镜像后，就可以分发到需要部署相关服务的机器上，直接启动Docker镜像就可以把服务起起来，使服务的部署和运维变得简单。 在大促的之前，可以在现有的机器集群上划分出服务器来启动Docker镜像，增强服务的性能，大促过后就可以关闭镜像，对机器上的其他服务不造成影响（在3.14节之前，服务运行在新增机器上需要修改系统配置来适配服务，这会导致机器上其他服务需要的运行环境被破坏）。 使用容器化技术后服务动态扩缩容问题得以解决，但是机器还是需要公司自身来管理，在非大促的时候，还是需要闲置着大量的机器资源来应对大促，机器自身成本和运维成本都极高，资源利用率低 第十四次演进：以云平台承载系统 系统可部署到公有云上，利用公有云的海量机器资源，解决动态硬件资源的问题，在大促的时间段里，在云平台中临时申请更多的资源，结合Docker和K8S来快速部署服务，在大促结束后释放资源，真正做到按需付费，资源利用率大大提高，同时大大降低了运维成本。 所谓的云平台，就是把海量机器资源，通过统一的资源管理，抽象为一个资源整体，在之上可按需动态申请硬件资源（如CPU、内存、网络等），并且之上提供通用的操作系统，提供常用的技术组件（如Hadoop技术栈，MPP数据库等）供用户使用，甚至提供开发好的应用，用户不需要关系应用内部使用了什么技术，就能够解决需求（如音视频转码服务、邮件服务、个人博客等）。在云平台中会涉及如下几个概念： IaaS：基础设施即服务。对应于上面所说的机器资源统一为资源整体，可动态申请硬件资源的层面； PaaS：平台即服务。对应于上面所说的提供常用的技术组件方便系统的开发和维护； SaaS：软件即服务。对应于上面所说的提供开发好的应用或服务，按功能或性能要求付费。 至此，以上所提到的从高并发访问问题，到服务的架构和系统实施的层面都有了各自的解决方案，但同时也应该意识到，在上面的介绍中，其实是有意忽略了诸如跨机房数据同步、分布式事务实现等等的实际问题，这些问题以后有机会再拿出来单独讨论 架构设计总结 架构的调整是否必须按照上述演变路径进行？不是的，以上所说的架构演变顺序只是针对某个侧面进行单独的改进，在实际场景中，可能同一时间会有几个问题需要解决，或者可能先达到瓶颈的是另外的方面，这时候就应该按照实际问题实际解决。如在政府类的并发量可能不大，但业务可能很丰富的场景，高并发就不是重点解决的问题，此时优先需要的可能会是丰富需求的解决方案。 对于将要实施的系统，架构应该设计到什么程度？对于单次实施并且性能指标明确的系统，架构设计到能够支持系统的性能指标要求就足够了，但要留有扩展架构的接口以便不备之需。对于不断发展的系统，如电商平台，应设计到能满足下一阶段用户量和性能指标要求的程度，并根据业务的增长不断的迭代升级架构，以支持更高的并发和更丰富的业务。 服务端架构和大数据架构有什么区别？所谓的“大数据”其实是海量数据采集清洗转换、数据存储、数据分析、数据服务等场景解决方案的一个统称，在每一个场景都包含了多种可选的技术，如数据采集有Flume、Sqoop、Kettle等，数据存储有分布式文件系统HDFS、FastDFS，NoSQL数据库HBase、MongoDB等，数据分析有Spark技术栈、机器学习算法等。总的来说大数据架构就是根据业务的需求，整合各种大数据组件组合而成的架构，一般会提供分布式存储、分布式计算、多维分析、数据仓库、机器学习算法等能力。而服务端架构更多指的是应用组织层面的架构，底层能力往往是由大数据架构来提供。 有没有一些架构设计的原则？ N+1设计。系统中的每个组件都应做到没有单点故障； 回滚设计。确保系统可以向前兼容，在系统升级时应能有办法回滚版本； 禁用设计。应该提供控制具体功能是否可用的配置，在系统出现故障时能够快速下线功能； 监控设计。在设计阶段就要考虑监控的手段； 多活数据中心设计。若系统需要极高的高可用，应考虑在多地实施数据中心进行多活，至少在一个机房断电的情况下系统依然可用； 采用成熟的技术。刚开发的或开源的技术往往存在很多隐藏的bug，出了问题没有商业支持可能会是一个灾难； 资源隔离设计。应避免单一业务占用全部资源； 架构应能水平扩展。系统只有做到能水平扩展，才能有效避免瓶颈问题； 非核心则购买。非核心功能若需要占用大量的研发资源才能解决，则考虑购买成熟的产品； 使用商用硬件。商用硬件能有效降低硬件故障的机率； 快速迭代。系统应该快速开发小功能模块，尽快上线进行验证，早日发现问题大大降低系统交付的风险； 无状态设计。服务接口应该做成无状态的，当前接口的访问不依赖于接口上次访问的状态。 参考原文出处：http://t.cn/Ai98XycJ]]></content>
      <categories>
        <category>系统架构</category>
      </categories>
      <tags>
        <tag>一致性</tag>
        <tag>数据库</tag>
        <tag>分布式</tag>
        <tag>并发</tag>
        <tag>缓存</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分库分表技术演进暨最佳实践]]></title>
    <url>%2F2019%2F06%2F14%2F%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E6%8A%80%E6%9C%AF%E6%BC%94%E8%BF%9B%E6%9A%A8%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[每个优秀的程序员和架构师都应该掌握分库分表，移动互联网时代，海量的用户每天产生海量的数量，比如： 用户表 订单表 交易流水表 我们以支付宝为例，支付宝用户是8亿；微信用户更是10亿。而订单表更夸张，比如美团外卖，每天都是几千万的订单。淘宝的历史订单总量应该百亿，甚至千亿级别，这些海量数据远不是一张表能Hold住的。 事实上，MySQL单表可以存储10亿级数据，只是这时候性能比较差。业界公认MySQL单表容量在1千万以下是最佳状态，因为这时它的BTREE索引树高在3~5之间。 既然一张表无法搞定，那么就想办法将数据放到多个地方，目前比较普遍的方案有3个： 分区； 分库分表； NoSQL / NewSQL； 说明一下：只分库，或者只分表，或者分库分表融合方案都统一认为是分库分表方案。因为分库，或者分表只是一种特殊的分库分表而已。NoSQL比较具有代表性的是MongoDB，es。NewSQL比较具有代表性的是TiDB。 Why Not NoSQL / NewSQL?首先，为什么不选择第三种方案NoSQL/NewSQL，我认为主要是RDBMS有以下几个优点： RDBMS生态完善； RDBMS绝对稳定； RDBMS的事务特性； NoSQL/NewSQL作为新生儿，在我们把可靠性当做首要考察对象时，它是无法与RDBMS相提并论的。RDBMS发展几十年，只要有软件的地方，它都是核心存储的首选。 目前绝大部分公司的核心数据都是：以 RDBMS 存储为主，NoSQL / NewSQL存储为辅！ 互联网公司又以MySQL为主，国企 &amp; 银行等不差钱的企业以Oracle / DB2为主！NoSQL/NewSQL宣传的无论多牛逼，就现在各大公司对它的定位，都是RDBMS的补充，而不是取而代之！ Why Not 分区?我们再看分区表方案。了解这个方案之前，先了解它的原理： 分区表是由多个相关的底层表实现，这些底层表也是由句柄对象表示，所以我们也可以直接访问各个分区。 存储引擎管理分区的各个底层表和管理普通表一样（所有的底层表都必须使用相同的存储引擎），分区表的索引只是在各个底层表上各自加上一个相同的索引。 从存储引擎的角度来看，底层表和一个普通表没有任何不同，存储引擎也无须知道这是一个普通表还是一个分区表的一部分。 事实上，这个方案也不错，它对用户屏蔽了sharding的细节，即使查询条件没有sharding column，它也能正常工作（只是这时候性能一般）。 不过它的缺点很明显：很多的资源都受到单机的限制，例如连接数，网络吞吐等！ 虽然每个分区可以独立存储，但是分区表的总入口还是一个MySQL示例。从而导致它的并发能力非常一般，远远达不到互联网高并发的要求！ 至于网上提到的一些其他缺点比如：无法使用外键，不支持全文索引。我认为这都不算缺点，21世纪的项目如果还是使用外键和数据库的全文索引，我都懒得吐槽了！ 所以，如果使用分区表，你的业务应该具备如下两个特点： 数据不是海量（分区数有限，存储能力就有限）； 并发能力要求不高； Why 分库分表?最后要介绍的就是目前互联网行业处理海量数据的通用方法：分库分表。 虽然大家都是采用分库分表方案来处理海量核心数据，但是还没有一个一统江湖的中间件，笔者这里列举一些有一定知名度的分库分表中间件： 阿里的TDDL，DRDS和cobar 开源社区的sharding-jdbc（3.x已经更名为sharding-sphere） 民间组织的MyCAT 360的Atlas； 美团的zebra 备注：sharding-jdbc 的作者张亮大神原来在当当，现在在京东金融。但是sharding-jdbc的版权属于开源社区，不是公司的，也不是张亮个人的！ 其他比如网易，58，京东等公司都有自研的中间件。总之各自为战，也可以说是百花齐放。 但是这么多的分库分表中间件全部可以归结为两大类型： CLIENT模式 PROXY模式 CLIENT模式代表有阿里的TDDL，开源社区的sharding-jdbc（sharding-jdbc的3.x版本即sharding-sphere已经支持了proxy模式） 架构如下： PROXY模式代表有阿里的cobar，民间组织的MyCAT，架构如下： 但是，无论是CLIENT模式，还是PROXY模式。几个核心的步骤是一样的：SQL解析，重写，路由，执行，结果归并。 笔者比较倾向于CLIENT模式，架构简单，性能损耗较小，运维成本低。 接下来，以几个常见的大表为案例，说明分库分表如何落地！ 实战案例分库分表第一步也是最重要的一步，即sharding column的选取，sharding column 选择的好坏将直接决定整个分库分表方案最终是否成功。 而sharding column的选取跟业务强相关，笔者认为选择sharding column的方法最主要分析你的API流量，优先考虑流量大的API，将流量比较大的API对应的SQL提取出来，将这些SQL共同的条件作为sharding column。 例如一般的OLTP系统都是对用户提供服务，这些API对应的SQL都有条件用户ID，那么，用户ID就是非常好的sharding column。 这里列举分库分表的几种主要处理思路： 只选取一个sharding column进行分库分表 ； 多个sharding column多个分库分表； sharding column分库分表 + es； 再以几张实际表为例，说明如何分库分表。 订单表订单表几个核心字段一般如下： 以阿里订单系统为例（参考《企业IT架构转型之道：阿里巴巴中台战略思想与架构实现》），它选择了三个column作为三个独立的sharding column。 即：order_id，user_id，merchant_code 其中，user_id和merchant_code就是买家ID和卖家ID，因为阿里的订单系统中买家和卖家的查询流量都比较大，并且查询对实时性要求都很高。而根据order_id进行分库分表，应该是根据order_id的查询也比较多。 这里还有一点需要提及，多个sharding-column的分库分表是冗余全量还是只冗余关系索引表，需要我们自己权衡。 冗余全量的情况如下图，每个sharding列对应的表的数据都是全量的，这样做的优点是不需要二次查询，性能更好，缺点是比较浪费存储空间（浅绿色字段就是sharding-column）： 冗余关系索引表的情况如下图，只有一个sharding column的分库分表的数据是全量的，其他分库分表只是与这个sharding column的关系表。 这样做的优点是节省空间，缺点是除了第一个sharding column的查询，其他sharding column的查询都需要二次查询。 这三张表的关系如下图所示（浅绿色字段就是sharding column）： 冗余全量表 PK 冗余关系表 速度对比：冗余全量表速度更快，冗余关系表需要二次查询，即使有引入缓存，还是多一次网络开销； 存储成本：冗余全量表需要几倍于冗余关系表的存储成本； 维护代价：冗余全量表维护代价更大，涉及到数据变更时，多张表都要进行修改。 *总结：选择冗余全量表还是索引关系表，这是一种架构上的trade off，两者的优缺点明显，阿里的订单表是冗余全量表。* 用户表用户表几个核心字段一般如下： 一般用户登录场景既可以通过mobile_no，又可以通过email，还可以通过username进行登录。 但是一些用户相关的API，又都包含user_id，那么可能需要根据这4个column都进行分库分表，即4个列都是sharding-column。 账户表账户表几个核心字段一般如下： 与账户表相关的API，一般条件都有account_no，所以以account_no作为sharding-column即可。 复杂查询上面提到的都是条件中有sharding column的SQL执行。但是，总有一些查询条件是不包含sharding column的，同时，我们也不可能为了这些请求量并不高的查询，无限制的冗余分库分表。 那么这些条件中没有sharding column的SQL怎么处理？ 以sharding-jdbc为例，有多少个分库分表，就要并发路由到多少个分库分表中执行，然后对结果进行合并。具体如何合并，可以看笔者sharding-jdbc系列文章，有分析源码讲解合并原理。 这种条件查询相对于有sharding column的条件查询性能很明显会下降很多。如果有几十个，甚至上百个分库分表，只要某个表的执行由于某些因素变慢，就会导致整个SQL的执行响应变慢，这非常符合木桶理论。 更有甚者，那些运营系统中的模糊条件查询，或者上十个条件筛选。这种情况下，即使单表都不好创建索引，更不要说分库分表的情况下。 那么怎么办呢？这个时候大名鼎鼎的elasticsearch，即es就派上用场了。将分库分表所有数据全量冗余到es中，将那些复杂的查询交给es处理。 淘宝我的所有订单页面如下，筛选条件有多个，且商品标题可以模糊匹配，这即使是单表都解决不了的问题（索引满足不了这种场景），更不要说分库分表了： 所以，以订单表为例，整个架构如下： 具体情况具体分析：多sharding column不到万不得已的情况下最好不要使用，成本较大，上面提到的用户表笔者就不太建议使用。 因为用户表有一个很大的特点就是它的上限是肯定的，即使全球70亿人全是你的用户，这点数据量也不大，所以笔者更建议采用单sharding column + es的模式简化架构。 es+HBase简要这里需要提前说明的是，solr + HBase结合的方案在社区中出现的频率可能更高，本篇文章为了保持一致性，所有全文索引方案选型都是es。 至于es+HBase和solr+HBase孰优孰劣，或者说es和solr孰优孰劣，不是本文需要讨论的范畴，事实上也没有太多讨论的意义。 es和solr本就是两个非常优秀且旗鼓相当的中间件。最近几年es更火爆： 如果抛开选型过程中所有历史包袱，单论es+HBase和solr+HBase的优劣，很明显后者是更好的选择。 solr+HBase高度集成，引入索引服务后我们最关心，也是最重要的索引一致性问题，solr+HBase已经有了非常成熟的解决方案一一Lily HBase Indexer。 延伸阅读阿里云上的云数据库HBase版也是借助solr实现全文索引，有兴趣的同学可以戳链接了解更多： https://help.aliyun.com/product/49055.html?spm=5176.124785.631202.con1.603452c0cz7bj2 es+HBase原理刚刚讨论到上面的以MySQL为核心，分库分表+es的方案，随着数据量越来越来，虽然分库分表可以继续成倍扩容，但是这时候压力又落到了es这里，这个架构也会慢慢暴露出问题！ 一般订单表，积分明细表等需要分库分表的核心表都会有好几十列，甚至上百列（假设有50列），但是整个表真正需要参与条件索引的可能就不到10个条件（假设有10列）。 这时候把50个列所有字段的数据全量索引到es中，对es集群有很大的压力，后面的es分片故障恢复也会需要很长的时间。 这个时候我们可以考虑减少es的压力，让es集群有限的资源尽可能保存条件检索时最需要的最有价值的数据 即只把可能参与条件检索的字段索引到es中，这样整个es集群压力减少到原来的1/5（核心表50个字段，只有10个字段参与条件） 而50个字段的全量数据保存到HBase中，这就是经典的es+HBase组合方案，即索引与数据存储隔离的方案 Hadoop体系下的HBase存储能力我们都知道是海量的，而且根据它的rowkey查询性能那叫一个快如闪电，而es的多条件检索能力非常强大。 这个方案把es和HBase的优点发挥的淋漓尽致，同时又规避了它们的缺点，可以说是一个扬长避免的最佳实践。 它们之间的交互大概是这样的：先根据用户输入的条件去es查询获取符合过滤条件的rowkey值，然后用rowkey值去HBase查询，后面这一查询步骤的时间几乎可以忽略，因为这是HBase最擅长的场景。 交互图如下所示： HBase检索能力扩展 图片来源于HBase技术社区-HBase应用实践专场-HBase for Solr 总结最后，对几种方案总结如下（sharding column简称为sc）： - 单个sc 多个sc sc+es sc+es+HBase 适用场景 单一 一般 比较广泛 非常广泛 查询及时性 及时 及时 比较及时 比较及时 存储能力 一般 一般 较大 海量 代码成本 很小 较大 一般 一般 架构复杂度 简单 一般 较难 非常复杂 总之，对于海量数据，且有一定的并发量的分库分表，绝不是引入某一个分库分表中间件就能解决问题，而是一项系统的工程。需要分析整个表相关的业务，让合适的中间件做它最擅长的事情。例如有sharding column的查询走分库分表，一些模糊查询，或者多个不固定条件筛选则走es，海量存储则交给HBase。 做了这么多事情后，后面还会有很多的工作要做，比如数据同步的一致性问题，还有运行一段时间后，某些表的数据量慢慢达到单表瓶颈，这时候还需要做冷数据迁移。总之，分库分表是一项非常复杂的系统工程。任何海量数据的处理，都不是简单的事情，做好战斗的准备吧！ 参考https://www.jianshu.com/p/f29e73b97794 公众号：技术琐话 公众号：HBase技术社区]]></content>
      <categories>
        <category>系统架构</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>索引</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[热点缓存集群架构设计]]></title>
    <url>%2F2019%2F06%2F11%2F%E7%83%AD%E7%82%B9%E7%BC%93%E5%AD%98%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[面对一个热点缓存，咱们的攻城狮兄弟应该如何设计系统架构，才能抗住瞬间高峰的粉丝流量！ 也希望能借着这种热点话题，帮大家重新复习一下热点缓存架构设计相关的技术要点！ 话不多说，进入正题！ 为什么要用缓存集群其实使用缓存集群的时候，最怕的就是热key、大value这两种情况，那啥叫热key大value呢？ 简单来说，热key，就是你的缓存集群中的某个key瞬间被数万甚至十万的并发请求打爆。大value，就是你的某个key对应的value可能有GB级的大小，导致查询value的时候导致网络相关的故障问题。 我们先来看看下面一幅图，假设你手头有个系统，他本身是集群部署的，然后后面有一套缓存集群，这个集群不管你用redis cluster，还是memcached，或者是公司自研缓存集群，都可以。 那么，这套系统用缓存集群干什么呢？ 很简单，在缓存里放一些平时不怎么变动的数据，然后用户在查询大量的平时不怎么变动的数据的时候，不就可以直接从缓存里走了吗？ 缓存集群的并发能力是很强的，而且读缓存的性能是很高的。举个例子，假设你每秒有2万请求，但是其中90%都是读请求，那么每秒1.8万请求都是在读一些不太变化的数据，而不是写数据。 那此时你把这些数据都放在数据库里，然后每秒发送2万请求到数据库上读写数据，你觉得合适吗？ 当然不合适了，如果你要用数据库承载每秒2万请求的话，那么不好意思，你很可能就得搞分库分表 + 读写分离。 比如你得分3个主库，承载每秒2000的写入请求，然后每个主库挂3个从库，一共9个从库承载每秒1.8万的读请求。 这样的话，你可能就需要一共是12台高配置的数据库服务器，这是很耗费钱的，成本非常高，很不合适。 大家看看下面的图，来体会下这种情况。 因此，我们完全可以把平时不太变化的数据放在缓存集群里，缓存集群可以采用2主2从，主节点用来写入缓存，从节点用来读缓存。 以缓存集群的性能，2个从节点完全可以用来承载每秒1.8万的大量读请求，然后3个数据库主库就是承载每秒2000的写请求和少量其他读请求就OK了。 这样一来，你耗费的机器瞬间变成了4台缓存机器 + 3台数据库机器 = 7台机器，是不是比之前的12台机器减少了很大的资源开销？ 没错，缓存其实在系统架构里是非常重要的组成部分。很多时候，对于那些很少变化但是大量高并发读的数据，通过缓存集群来抗高并发读，是非常合适的。 我们看看下面的图，体会一下这个过程。 需要说明的是，这里所有的机器数量、并发请求量都是一个示例，大家主要是体会一下这个意思就好，其目的主要是给一些不太熟悉缓存相关技术的同学一点背景性的阐述，让这些同学能够理解在系统里用缓存集群承载读请求是什么意思。 20万用户同时访问一个热点缓存好了，背景已经给大家解释清楚，现在就可以给大家说说今天重点要讨论的问题：热点缓存。 我们来做一个假设，现在有10个缓存节点来抗大量的读请求。正常情况下，读请求应该是均匀的落在10个缓存节点上的，对吧！ 这10个缓存节点，每秒承载1万请求是差不多的。 然后我们再做一个假设，你一个节点承载2万请求是极限，所以一般你就限制一个节点正常承载1万请求就ok了，稍微留一点buffer出来。 好，所谓的热点缓存问题是什么意思呢？很简单，就是突然因为莫名的原因，出现大量的用户访问同一条缓存数据。 比如林志玲突然宣布结婚，这时是不是会引发短时间内每秒都数十万用户去查看这条热点新闻？ 假设这条新闻就是一个缓存，对应一个缓存key，就存在一台缓存机器上，此时瞬时假设有20万请求奔向那一台机器上的一个key。 此时会如何？我们看看下面的图，来体会一下这种绝望的感受。 很明显了，我们刚才假设的是一个缓存Slave节点最多每秒就是2万的请求，当然实际缓存单机承载5万~10万读请求也是可能的，这里就是一个假设。 结果每秒突然奔过来20万请求到这台机器上，会怎么样？很简单，上面图里那台被20万请求指向的缓存机器会过度操劳而宕机的。 那么如果缓存集群开始出现机器的宕机，此时会如何？ 此时读请求发现读不到数据，会从数据库里提取原始数据，然后放入剩余的其他缓存机器里去。但是接踵而来的每秒20万请求，会再次压垮其他的缓存机器。 以此类推，最终导致缓存集群全盘崩溃，引发系统整体宕机。 咱们看看下面的图，再感受一下这个恐怖的现场。 基于流式计算技术的缓存热点自动发现其实这里关键的一点，就是对于这种热点缓存，你的系统需要能够在热点缓存突然发生的时候，直接发现他，然后瞬间立马实现毫秒级的自动负载均衡。 那么我们就先来说说，你如何自动发现热点缓存问题？ 首先你要知道，一般出现缓存热点的时候，你的每秒并发肯定是很高的，可能每秒都几十万甚至上百万的请求量过来，这都是有可能的。 所以，此时完全可以基于大数据领域的流式计算技术来进行实时数据访问次数的统计，比如storm、spark streaming、flink。 一旦在实时数据访问次数统计的过程中，比如发现一秒之内，某条数据突然访问次数超过了1000，就直接立马把这条数据判定为是热点数据，可以将这个发现出来的热点数据写入比如zookeeper中。 当然，你的系统如何判定热点数据，可以根据自己的业务还有经验值来就可以了。 大家看看下面这张图，看看整个流程是如何进行的。 这里肯定有人会问，那你的流式计算系统在进行数据访问次数统计的时候，会不会也存在说单台机器被请求每秒几十万次的问题呢？ 答案是：否 因为流式计算技术，尤其是storm这种系统，他可以做到同一条数据的请求过来，先分散在很多机器里进行本地计算，最后再汇总局部计算结果到一台机器进行全局汇总。 所以几十万请求可以先分散在比如100台机器上，每台机器统计了这条数据的几千次请求。 然后100条局部计算好的结果汇总到一台机器做全局计算即可，所以基于流式计算技术来进行统计是不会有热点问题的。 热点缓存自动加载为JVM本地缓存我们自己的系统可以对zookeeper指定的热点缓存对应的znode进行监听，如果有变化他立马就可以感知到了。 此时系统层就可以立马把相关的缓存数据从数据库加载出来，然后直接放在自己系统内部的本地缓存里即可。 这个本地缓存，你用ehcache、hashmap，其实都可以，一切看自己的业务需求。我们这里主要说的就是将缓存集群里的集中式缓存，直接变成每个系统自己本地实现缓存即可，每个系统本地是无法缓存过多数据的。 因为一般这种普通系统单实例部署机器可能就一个4核8G的机器，留给本地缓存的空间是很少的，所以用来放这种热点数据的本地缓存是最合适的，刚刚好。 假设你的系统层集群部署了100台机器，那么好了，此时你100台机器瞬间在本地都会有一份热点缓存的副本。 然后接下来对热点缓存的读操作，直接系统本地缓存读出来就给返回了，不用再走缓存集群了。 这样的话，也不可能允许每秒20万的读请求到达缓存机器的一台机器上读一个热点缓存了，而是变成100台机器每台机器承载数千请求，那么那数千请求就直接从机器本地缓存返回数据了，这是没有问题的。 我们再来画一幅图，一起来看看这个过程： 限流熔断保护除此之外，在每个系统内部，其实还应该专门加一个对热点数据访问的限流熔断保护措施。 每个系统实例内部，都可以加一个熔断保护机制，假设缓存集群最多每秒承载4万读请求，那么你一共有100个系统实例。 你自己就该限制好，每个系统实例每秒最多请求缓存集群读操作不超过400次，一超过就可以熔断掉，不让请求缓存集群，直接返回一个空白信息，然后用户稍后会自行再次重新刷新页面之类的。 通过系统层自己直接加限流熔断保护措施，可以很好的保护后面的缓存集群、数据库集群之类的不要被打死。 再来一幅图，一起来看看： 本文总结具体要不要在系统里实现这种复杂的缓存热点优化架构呢？这个还要看你们自己的系统有没有这种场景了。 如果你的系统有热点缓存问题，那么就要实现类似本文的复杂热点缓存支撑架构。但是如果没有的话，那么也别过度设计，其实你的系统可能根本不需要这么复杂的架构。 如果是后者，那么大伙儿就权当看看本文，了解一下对应的架构思想好了。 参考原文出处：微信公众号：石杉的架构笔记 https://mp.weixin.qq.com/s/31mCDLZip4LbeiU0E_E4tw]]></content>
      <categories>
        <category>系统架构</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>Redis</tag>
        <tag>缓存</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[epoll 的本质]]></title>
    <url>%2F2019%2F06%2F02%2Fepoll-%E7%9A%84%E6%9C%AC%E8%B4%A8%2F</url>
    <content type="text"><![CDATA[从事服务端开发，少不了要接触网络编程。epoll 作为 Linux 下高性能网络服务器的必备技术至关重要，nginx、Redis、Skynet 和大部分游戏服务器都使用到这一多路复用技术。 epoll 很重要，但是 epoll 与 select 的区别是什么呢？epoll 高效的原因是什么？ 网上虽然也有不少讲解 epoll 的文章，但要么是过于浅显，或者陷入源码解析，很少能有通俗易懂的。笔者于是决定编写此文，让缺乏专业背景知识的读者也能够明白 epoll 的原理。 本文核心思想是：要让读者清晰明白 epoll 为什么性能好。 文章会从网卡接收数据的流程讲起，串联起 CPU 中断、操作系统进程调度等知识；再一步步分析阻塞接收数据、select 到 epoll 的进化过程；最后探究 epoll 的实现细节。 从网卡接收数据说起下边是一个典型的计算机结构图，计算机由 CPU、存储器（内存）与网络接口等部件组成，了解 epoll 本质的第一步，要从硬件的角度看计算机怎样接收网络数据。 下图展示了网卡接收数据的过程。 在 ① 阶段，网卡收到网线传来的数据； 经过 ② 阶段的硬件电路的传输； 最终 ③ 阶段将数据写入到内存中的某个地址上。 这个过程涉及到 DMA 传输、IO 通路选择等硬件有关的知识，但我们只需知道：网卡会把接收到的数据写入内存。 网卡接收数据的过程 通过硬件传输，网卡接收的数据存放到内存中，操作系统就可以去读取它们。 如何知道接收了数据？了解 epoll 本质的第二步，要从 CPU 的角度来看数据接收。理解这个问题，要先了解一个概念——中断。 计算机执行程序时，会有优先级的需求。比如，当计算机收到断电信号时，它应立即去保存数据，保存数据的程序具有较高的优先级（电容可以保存少许电量，供 CPU 运行很短的一小段时间）。 一般而言，由硬件产生的信号需要 CPU 立马做出回应，不然数据可能就丢失了，所以它的优先级很高。CPU 理应中断掉正在执行的程序，去做出响应；当 CPU 完成对硬件的响应后，再重新执行用户程序。中断的过程如下图，它和函数调用差不多，只不过函数调用是事先定好位置，而中断的位置由“信号”决定。 中断程序调用 以键盘为例，当用户按下键盘某个按键时，键盘会给 CPU 的中断引脚发出一个高电平，CPU 能够捕获这个信号，然后执行键盘中断程序。下图展示了各种硬件通过中断与 CPU 交互的过程。 CPU 中断（图片来源：net.pku.edu.cn） 现在可以回答“如何知道接收了数据？”这个问题了：当网卡把数据写入到内存后，网卡向 CPU 发出一个中断信号，操作系统便能得知有新数据到来，再通过网卡中断程序去处理数据。 进程阻塞为什么不占用 CPU 资源？了解 epoll 本质的第三步，要从操作系统进程调度的角度来看数据接收。阻塞是进程调度的关键一环，指的是进程在等待某事件（如接收到网络数据）发生之前的等待状态，recv、select 和 epoll 都是阻塞方法。下边分析一下进程阻塞为什么不占用 CPU 资源？ 为简单起见，我们从普通的 recv 接收开始分析，先看看下面代码： 123456789101112//创建socketint s = socket(AF_INET, SOCK_STREAM, 0); //绑定bind(s, ...)//监听listen(s, ...)//接受客户端连接int c = accept(s, ...)//接收客户端数据recv(c, ...);//将数据打印出来printf(...) 这是一段最基础的网络编程代码，先新建 socket 对象，依次调用 bind、listen 与 accept，最后调用 recv 接收数据。recv 是个阻塞方法，当程序运行到 recv 时，它会一直等待，直到接收到数据才往下执行。 那么阻塞的原理是什么？ 工作队列 操作系统为了支持多任务，实现了进程调度的功能，会把进程分为“运行”和“等待”等几种状态。运行状态是进程获得 CPU 使用权，正在执行代码的状态；等待状态是阻塞状态，比如上述程序运行到 recv 时，程序会从运行状态变为等待状态，接收到数据后又变回运行状态。操作系统会分时执行各个运行状态的进程，由于速度很快，看上去就像是同时执行多个任务。 下图的计算机中运行着 A、B 与 C 三个进程，其中进程 A 执行着上述基础网络程序，一开始，这 3 个进程都被操作系统的工作队列所引用，处于运行状态，会分时执行。 工作队列中有 A、B 和 C 三个进程 等待队列 当进程 A 执行到创建 socket 的语句时，操作系统会创建一个由文件系统管理的 socket 对象（如下图）。这个 socket 对象包含了发送缓冲区、接收缓冲区与等待队列等成员。等待队列是个非常重要的结构，它指向所有需要等待该 socket 事件的进程。 创建 socket 当程序执行到 recv 时，操作系统会将进程 A 从工作队列移动到该 socket 的等待队列中（如下图）。由于工作队列只剩下了进程 B 和 C，依据进程调度，CPU 会轮流执行这两个进程的程序，不会执行进程 A 的程序。所以进程 A 被阻塞，不会往下执行代码，也不会占用 CPU 资源。 socket 的等待队列 注：操作系统添加等待队列只是添加了对这个“等待中”进程的引用，以便在接收到数据时获取进程对象、将其唤醒，而非直接将进程管理纳入自己之下。上图为了方便说明，直接将进程挂到等待队列之下。 唤醒进程 当 socket 接收到数据后，操作系统将该 socket 等待队列上的进程重新放回到工作队列，该进程变成运行状态，继续执行代码。同时由于 socket 的接收缓冲区已经有了数据，recv 可以返回接收到的数据。 内核接收网络数据全过程这一步，贯穿网卡、中断与进程调度的知识，叙述阻塞 recv 下，内核接收数据的全过程。 如下图所示，进程在 recv 阻塞期间，计算机收到了对端传送的数据（步骤①），数据经由网卡传送到内存（步骤②），然后网卡通过中断信号通知 CPU 有数据到达，CPU 执行中断程序（步骤③）。 此处的中断程序主要有两项功能，先将网络数据写入到对应 socket 的接收缓冲区里面（步骤④），再唤醒进程 A（步骤⑤），重新将进程 A 放入工作队列中。 内核接收数据全过程 唤醒进程的过程如下图所示： 唤醒进程 以上是内核接收数据全过程，这里我们可能会思考两个问题： 其一，操作系统如何知道网络数据对应于哪个 socket？ 其二，如何同时监视多个 socket 的数据？ 第一个问题：因为一个 socket 对应着一个端口号，而网络数据包中包含了 ip 和端口的信息，内核可以通过端口号找到对应的 socket。当然，为了提高处理速度，操作系统会维护端口号到 socket 的索引结构，以快速读取。 第二个问题是多路复用的重中之重，也正是本文后半部分的重点。 同时监视多个 socket 的简单方法服务端需要管理多个客户端连接，而 recv 只能监视单个 socket，这种矛盾下，人们开始寻找监视多个 socket 的方法。epoll 的要义就是高效地监视多个 socket。 从历史发展角度看，必然先出现一种不太高效的方法，人们再加以改进，正如 select 之于 epoll。 先理解不太高效的 select，才能够更好地理解 epoll 的本质。 假如能够预先传入一个 socket 列表，如果列表中的 socket 都没有数据，挂起进程，直到有一个 socket 收到数据，唤醒进程。这种方法很直接，也是 select 的设计思想。 为方便理解，我们先复习 select 的用法。在下边的代码中，先准备一个数组 fds，让 fds 存放着所有需要监视的 socket。然后调用 select，如果 fds 中的所有 socket 都没有数据，select 会阻塞，直到有一个 socket 接收到数据，select 返回，唤醒进程。用户可以遍历 fds，通过 FD_ISSET 判断具体哪个 socket 收到数据，然后做出处理。 1234567891011int s = socket(AF_INET, SOCK_STREAM, 0); bind(s, ...);listen(s, ...);int fds[] = 存放需要监听的socket;while(1)&#123; int n = select(..., fds, ...) for(int i=0; i &lt; fds.count; i++)&#123; if(FD_ISSET(fds[i], ...))&#123; //fds[i]的数据处理 &#125; &#125;&#125; select 的流程 select 的实现思路很直接，假如程序同时监视如下图的 sock1、sock2 和 sock3 三个 socket，那么在调用 select 之后，操作系统把进程 A 分别加入这三个 socket 的等待队列中。 操作系统把进程 A 分别加入这三个 socket 的等待队列中 当任何一个 socket 收到数据后，中断程序将唤起进程。下图展示了 sock2 接收到了数据的处理流程： 注：recv 和 select 的中断回调可以设置成不同的内容。 sock2 接收到了数据，中断程序唤起进程 A 所谓唤起进程，就是将进程从所有的等待队列中移除，加入到工作队列里面，如下图所示： 将进程 A 从所有等待队列中移除，再加入到工作队列里面 经由这些步骤，当进程 A 被唤醒后，它知道至少有一个 socket 接收了数据。程序只需遍历一遍 socket 列表，就可以得到就绪的 socket。 这种简单方式行之有效，在几乎所有操作系统都有对应的实现。 但是简单的方法往往有缺点，主要是： 其一，每次调用 select 都需要将进程加入到所有监视 socket 的等待队列，每次唤醒都需要从每个队列中移除。这里涉及了两次遍历，而且每次都要将整个 fds 列表传递给内核，有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定 select 的最大监视数量，默认只能监视 1024 个 socket。 其二，进程被唤醒后，程序并不知道哪些 socket 收到数据，还需要遍历一次。 那么，有没有减少遍历的方法？有没有保存就绪 socket 的方法？这两个问题便是 epoll 技术要解决的。 补充说明： 本节只解释了 select 的一种情形。当程序调用 select 时，内核会先遍历一遍 socket，如果有一个以上的 socket 接收缓冲区有数据，那么 select 直接返回，不会阻塞。这也是为什么 select 的返回值有可能大于 1 的原因之一。如果没有 socket 有数据，进程才会阻塞。 epoll 的设计思路epoll 是在 select 出现 N 多年后才被发明的，是 select 和 poll（poll 和 select 基本一样，有少量改进）的增强版本。epoll 通过以下一些措施来改进效率： 措施一：功能分离 select 低效的原因之一是将“维护等待队列”和“阻塞进程”两个步骤合二为一。如下图所示，每次调用 select 都需要这两步操作，然而大多数应用场景中，需要监视的 socket 相对固定，并不需要每次都修改。epoll 将这两个操作分开，先用 epoll_ctl 维护等待队列，再调用 epoll_wait 阻塞进程。显而易见地，效率就能得到提升。 相比 select，epoll 拆分了功能 为方便理解后续的内容，我们先了解一下 epoll 的用法。如下的代码中，先用 epoll_create 创建一个 epoll 对象 epfd，再通过 epoll_ctl 将需要监视的 socket 添加到 epfd 中，最后调用 epoll_wait 等待数据： 12345678910111213int s = socket(AF_INET, SOCK_STREAM, 0); bind(s, ...)listen(s, ...)int epfd = epoll_create(...);epoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中while(1)&#123; int n = epoll_wait(...) for(接收到数据的socket)&#123; //处理 &#125;&#125; 功能分离，使得 epoll 有了优化的可能。 措施二：就绪列表 select 低效的另一个原因在于程序不知道哪些 socket 收到数据，只能一个个遍历。如果内核维护一个“就绪列表”，引用收到数据的 socket，就能避免遍历。如下图所示，计算机共有三个 socket，收到数据的 sock2 和 sock3 被就绪列表 rdlist 所引用。当进程被唤醒后，只要获取 rdlist 的内容，就能够知道哪些 socket 收到数据。 就绪列表示意图 epoll 的原理与工作流程本节会以示例和图表来讲解 epoll 的原理和工作流程。 创建 epoll 对象 如下图所示，当某个进程调用 epoll_create 方法时，内核会创建一个 eventpoll 对象（也就是程序中 epfd 所代表的对象）。eventpoll 对象也是文件系统中的一员，和 socket 一样，它也会有等待队列。 内核创建 eventpoll 对象 创建一个代表该 epoll 的 eventpoll 对象是必须的，因为内核要维护“就绪列表”等数据，“就绪列表”可以作为 eventpoll 的成员。 维护监视列表 创建 epoll 对象后，可以用 epoll_ctl 添加或删除所要监听的 socket。以添加 socket 为例，如下图，如果通过 epoll_ctl 添加 sock1、sock2 和 sock3 的监视，内核会将 eventpoll 添加到这三个 socket 的等待队列中。 添加所要监听的 socket 当 socket 收到数据后，中断程序会操作 eventpoll 对象，而不是直接操作进程。 接收数据 当 socket 收到数据后，中断程序会给 eventpoll 的“就绪列表”添加 socket 引用。如下图展示的是 sock2 和 sock3 收到数据后，中断程序让 rdlist 引用这两个 socket。 给就绪列表添加引用 eventpoll 对象相当于 socket 和进程之间的中介，socket 的数据接收并不直接影响进程，而是通过改变 eventpoll 的就绪列表来改变进程状态。 当程序执行到 epoll_wait 时，如果 rdlist 已经引用了 socket，那么 epoll_wait 直接返回，如果 rdlist 为空，阻塞进程。 阻塞和唤醒进程 假设计算机中正在运行进程 A 和进程 B，在某时刻进程 A 运行到了 epoll_wait 语句。如下图所示，内核会将进程 A 放入 eventpoll 的等待队列中，阻塞进程。 epoll_wait 阻塞进程 当 socket 接收到数据，中断程序一方面修改 rdlist，另一方面唤醒 eventpoll 等待队列中的进程，进程 A 再次进入运行状态（如下图）。也因为 rdlist 的存在，进程 A 可以知道哪些 socket 发生了变化。 epoll 唤醒进程 epoll 的实现细节至此，相信读者对 epoll 的本质已经有一定的了解。但我们还需要知道 eventpoll 的数据结构是什么样子？ 此外，就绪队列应该应使用什么数据结构？eventpoll 应使用什么数据结构来管理通过 epoll_ctl 添加或删除的 socket？ 如下图所示，eventpoll 包含了 lock、mtx、wq（等待队列）与 rdlist 等成员，其中 rdlist 和 rbr 是我们所关心的。 epoll 原理示意图，图片来源：《深入理解Nginx：模块开发与架构解析(第二版)》，陶辉 就绪列表的数据结构 就绪列表引用着就绪的 socket，所以它应能够快速的插入数据。 程序可能随时调用 epoll_ctl 添加监视 socket，也可能随时删除。当删除时，若该 socket 已经存放在就绪列表中，它也应该被移除。所以就绪列表应是一种能够快速插入和删除的数据结构。 双向链表就是这样一种数据结构，epoll 使用双向链表来实现就绪队列（对应上图的 rdllist）。 索引结构 既然 epoll 将“维护监视队列”和“进程阻塞”分离，也意味着需要有个数据结构来保存监视的 socket，至少要方便地添加和移除，还要便于搜索，以避免重复添加。红黑树是一种自平衡二叉查找树，搜索、插入和删除时间复杂度都是O(log(N))，效率较好，epoll 使用了红黑树作为索引结构（对应上图的 rbr）。 注：因为操作系统要兼顾多种功能，以及由更多需要保存的数据，rdlist 并非直接引用 socket，而是通过 epitem 间接引用，红黑树的节点也是 epitem 对象。同样，文件系统也并非直接引用着 socket。为方便理解，本文中省略了一些间接结构。 小结epoll 在 select 和 poll 的基础上引入了 eventpoll 作为中间层，使用了先进的数据结构，是一种高效的多路复用技术。这里也以表格形式简单对比一下 select、poll 与 epoll，结束此文。希望读者能有所收获。 参考https://zhuanlan.zhihu.com/p/63179839 https://zhuanlan.zhihu.com/p/64138532 https://zhuanlan.zhihu.com/p/64746509 https://mp.weixin.qq.com/s/MzrhaWMwrFxKT7YZvd68jw 原文作者介绍罗培羽，正在创作好玩游戏的程序员。 作为游戏行业从业人员，曾参与《卡布西游》、《卡布仙踪》、《卡布魔镜》、《坦克射击》与《海陆大战》等多个项目研发工作；作为独立游戏开发者，主导《仙剑5前传之心愿》与《蚀梦》等项目研发，拥有丰富的实战经验。 自 2009 年发布第一部视频教程《教你用VB制作RPG游戏》以来，先后出版专业书籍《手把手教你用C#制作RPG游戏》与《Unity3D网络游戏实战》。 目前关注手机游戏与 AI 技术等领域，并以第三方视角记录普通开发者的心路历程。 个人知乎专栏： https://zhuanlan.zhihu.com/pyluo]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>内存</tag>
        <tag>数据结构</tag>
        <tag>链表</tag>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何设计一个百万级用户的抽奖系统？]]></title>
    <url>%2F2019%2F05%2F31%2F%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E7%99%BE%E4%B8%87%E7%BA%A7%E7%94%A8%E6%88%B7%E7%9A%84%E6%8A%BD%E5%A5%96%E7%B3%BB%E7%BB%9F%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[抽奖系统的背景引入本文给大家分享一个之前经历过的抽奖系统的流量削峰架构的设计方案。 抽奖、抢红包、秒杀，这类系统其实都有一些共同的特点，那就是在某个时间点会瞬间涌入大量的人来点击系统，给系统造成瞬间高于平时百倍、千倍甚至几十万倍的流量压力。 比如抽奖，有一种场景：某个网站或者APP规定好了在某个时间点，所有人都可以参与抽奖，那么可能百万级的用户会蹲守在那个时间点，到时间大家一起参与这个抽奖。 抢红包，可能是某个电视节目上，突然说扫码可以抢红包，那么电视机前可能千万级的用户会瞬间一起打开手机扫码抢红包。 秒杀更是如此，所谓秒杀，意思是让大家都在电脑前等着，在某个时间突然就可以抢购某个限量的商品 比如某个手机平时卖5999，现在限量100台价格才2999，50%的折扣，可能百万级的用户就会蹲守在电脑前在比如凌晨12点一起点击按钮抢购这款手机。 类似的场景其实现在是很多的，那么本文就用一个抽奖系统举例，说说应对这种瞬时超高并发的流量，应该如何设计流量削峰的架构来应对，才能保证系统不会突然跨掉？ 结合具体业务需求分析抽奖系统假设现在有一个抽奖的业务场景，用户在某个时间可以参与抽奖，比如一共有1万个奖，奖品就是某个礼物。 然后参与抽奖的用户可能有几十万，一瞬间可能几十万请求涌入过来，接着瞬间其中1万人中奖了，剩余的人都是没中奖的。然后中奖的1万人的请求会联动调用礼品服务，完成这1万中奖人的礼品发放。 简单来说，需求场景就是如此，然而这里就有很多的地方值得优化了。 一个未经过优化的系统架构先来看一个未经过任何优化的系统架构，简单来说就是有一个负载均衡的设备会把瞬间涌入的超高并发的流量转发到后台的抽奖服务上。 这个抽奖服务就是用普通的Tomcat来部署的，里面实现了具体的抽奖逻辑，假设刚开始最常规的抽奖逻辑是基于MySQL来实现的，接着就是基于Tomcat部署的礼品服务，抽奖服务如果发现中奖了需要调用礼品服务去发放礼品。 如下图所示： 负载均衡层的限流防止用户重复抽奖首先第一次在负载均衡层可以做的事情，就是防止重复抽奖。 我们可以在负载均衡设备中做一些配置，判断如果同一个用户在1分钟之内多次发送请求来进行抽奖，就认为是恶意重复抽奖，或者是他们自己写的脚本在刷奖，这种流量一律认为是无效流量，在负载均衡设备那个层次就给直接屏蔽掉。 举个例子，比如有几十万用户瞬间同时抽奖，最多其实也就几十万请求而已，但是如果有人重复抽奖或者是写脚本刷奖，那可能瞬间涌入的是几百万的请求，就不是几十万的请求了，所以这里就可以把无效流量给拦截掉。 如下图所示： 全部开奖后暴力拦截流量其实秒杀、抢红包、抽奖，这类系统有一个共同的特点，那就是假设有50万请求涌入进来，可能前5万请求就直接把事儿干完了，甚至是前500请求就把事儿干完了，后续的几十万流量是无效的，不需要让他们进入后台系统执行业务逻辑了。 什么意思呢？ 举个例子，秒杀商品，假设有50万人抢一个特价手机，人家就准备了100台手机，那么50万请求瞬间涌入，其实前500个请求就把手机抢完了，后续的几十万请求没必要让他转发到Tomcat服务中去执行秒杀业务逻辑了，不是吗？ 抽奖、红包都是一样的 ，可能50万请求涌入，但是前1万个请求就把奖品都抽完了，或者把红包都抢完了，后续的流量其实已经不需要放到Tomcat抽奖服务上去了，直接暴力拦截返回抽奖结束就可以了。 这样的话，其实在负载均衡这一层（可以考虑用Nginx之类的来实现）就可以拦截掉99%的无效流量。 所以必须让抽奖服务跟负载均衡之间有一个状态共享的机制。 就是说抽奖服务一旦全部开奖完毕，直接更新一个共享状态。然后负载均衡感知到了之后，后续请求全部拦截掉返回一个抽奖结束的标识就可以了。 这么做可能就会做到50万人一起请求，结果就可能2万请求到了后台的Tomcat抽奖服务中，48万请求直接拦截掉了。 我们可以基于Redis来实现这种共享抽奖状态，它非常轻量级，很适合两个层次的系统的共享访问。 当然其实用ZooKeeper也是可以的，在负载均衡层可以基于zk客户端监听某个znode节点状态。一旦抽奖结束，抽奖服务更新zk状态，负载均衡层会感知到。 下图展示了上述所说的过程： Tomcat线程数量的优化其次就是对于线上生产环境的Tomcat，有一个至关重要的参数是需要根据自己的情况调节好的，那就是他的工作线程数量。 众所周知，对于进入Tomcat的每个请求，其实都会交给一个独立的工作线程来进行处理，那么Tomcat有多少线程，就决定了并发请求处理的能力。 但是这个线程数量是需要经过压测来进行判断的，因为每个线程都会处理一个请求，这个请求又需要访问数据库之类的外部系统，所以不是每个系统的参数都可以一样的，需要自己对系统进行压测。 但是给一个经验值的话，Tomcat的线程数量不宜过多。因为线程过多，普通虚拟机的CPU是扛不住的，反而会导致机器CPU负载过高，最终崩溃。 同时，Tomcat的线程数量也不宜太少，因为如果就100个线程，那么会导致无法充分利用Tomcat的线程资源和机器的CPU资源。 所以一般来说，Tomcat线程数量在200~500之间都是可以的，但是具体多少需要自己压测一下，不断的调节参数，看具体的CPU负载以及线程执行请求的一个效率。 在CPU负载尚可，以及请求执行性能正常的情况下，尽可能提高一些线程数量。 但是如果到一个临界值，发现机器负载过高，而且线程处理请求的速度开始下降，说明这台机扛不住这么多线程并发执行处理请求了，此时就不能继续上调线程数量了。 基于Redis实现抽奖业务逻辑现在问题又来了，虽然在负载均衡那个层面，已经把比如50万流量中的48万都拦截掉了，但是可能还是会有2万流量进入抽奖服务 此时抽奖服务自然是可以多机器来部署的，比如假设一台Tomcat可以抗500请求，那么2万并发就是40台机器。 如果你是基于云平台来部署系统的，搞活动临时租用一批机器就可以了，活动结束了机器立马可以释放掉，现在云平台都很方便。 但是有个问题，你的数据库MySQL能抗住2万的并发请求吗？ 如果你基于MySQL来实现核心的抽奖业务逻辑，40个Tomcat部署的抽奖服务频繁对MySQL进行增删改查，这一个MySQL实例也是很难抗住的。 所以此时还得把MySQL给替换成Redis，通常这种场景下，建议是基于Redis来实现核心的业务逻辑。 Redis单机抗2万并发那是很轻松的一件事情，所以在这里又需要做进一步的优化。如下图： 发放礼品环节进行限流削峰接着问题又来了，假设抽奖服务在2万请求中有1万请求抽中了奖品，那么势必会造成抽奖服务对礼品服务调用1万次。 礼品服务假设也是优化后的Tomcat，可以抗500并发，难道礼品服务也要去部署20台机器吗？ 其实这是没必要的，因为抽奖之后完全可以让礼品服务在后台慢慢的把中奖的礼品给发放出去，不需要一下子就立马对1万个请求完成礼品的发放逻辑。 所以这里可以在抽奖服务和礼品服务之间，引入消息中间件，进行限流削峰。 也就是说，抽奖服务把中奖信息发送到MQ，然后礼品服务假设就部署两个Tomcat，慢慢的从MQ中消费中奖消息，然后慢慢完成1完礼品的发放就可以了。 假设两个礼品服务实例每秒可以完成100个礼品的发放，那么1万个礼品也就是延迟100秒发放完毕罢了。 也就是你抽奖之后，可能过了一两分钟，会看到自己的礼品发放的一些物流配送的进度之类的。 而且礼品服务可能需要在MySQL数据库中做很多增删改查的操作，比如插入中奖纪录，然后进行礼品发货等等。 此时因为礼品服务就2个Tomcat实例，所以对MySQL的并发读写不会太高，那么数据库层面也是可以抗住的。 整个过程，如下图所示： 系统架构设计总结其实对于商品秒杀、抽奖活动、抢红包类的系统而言，架构设计的思路很多都是类似的，核心思路都是对于这种瞬时超高流量的系统，尽可能在负载均衡层就把99%的无效流量拦截掉 然后在1%的流量进入核心业务服务后，此时每秒并发还是可能会上万，那么可以基于Redis实现核心业务逻辑 ，抗住上万并发。 最后对于类似秒杀商品发货、抽奖商品发货、红包资金转账之类的非常耗时的操作，完全可以基于MQ来限流削峰，后台有一个服务慢慢执行即可。 参考原文出处：微信公众号： 狸猫技术窝 https://mp.weixin.qq.com/s/LNqMbTWcbaa0Bz_NI6OkpA]]></content>
      <categories>
        <category>系统架构</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>线程</tag>
        <tag>Redis</tag>
        <tag>负载均衡</tag>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka参数调优实战]]></title>
    <url>%2F2019%2F05%2F31%2FKafka%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[背景引入：很多同学看不懂kafka参数今天给大家聊一个很有意思的话题，大家知道很多公司都会基于Kafka作为MQ来开发一些复杂的大型系统。 而在使用Kafka的客户端编写代码与服务器交互的时候，是需要对客户端设置很多的参数的。 所以我就见过很多年轻的同学，可能刚刚加入团队，对Kafka这个技术其实并不是很了解。 此时就会导致他们看团队里的一些资深同事写的一些代码，会看不懂是怎么回事，不了解背后的含义，这里面尤其是一些Kafka参数的设置。 所以这篇文章，我们还是采用老规矩画图的形式，来聊聊Kafka生产端一些常见参数的设置，让大家下次看到一些Kafka客户端设置的参数时，不会再感到发怵。 一段Kafka生产端的示例代码12345678910111213Properties props = new Properties();props.put("bootstrap.servers", "localhost:9092"); props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");props.put("buffer.memory", 67108864); props.put("batch.size", 131072); props.put("linger.ms", 100); props.put("max.request.size", 10485760); props.put("acks", "1"); props.put("retries", 10); props.put("retry.backoff.ms", 500);KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(props); 内存缓冲的大小首先我们看看“buffer.memory”这个参数是什么意思？ Kafka的客户端发送数据到服务器，一般都是要经过缓冲的，也就是说，你通过KafkaProducer发送出去的消息都是先进入到客户端本地的内存缓冲里，然后把很多消息收集成一个一个的Batch，再发送到Broker上去的。 所以这个“buffer.memory”的本质就是用来约束KafkaProducer能够使用的内存缓冲的大小的，他的默认值是32MB。 那么既然了解了这个含义，大家想一下，在生产项目里，这个参数应该怎么来设置呢？ 你可以先想一下，如果这个内存缓冲设置的过小的话，可能会导致一个什么问题？ 首先要明确一点，那就是在内存缓冲里大量的消息会缓冲在里面，形成一个一个的Batch，每个Batch里包含多条消息。 然后KafkaProducer有一个Sender线程会把多个Batch打包成一个Request发送到Kafka服务器上去。 那么如果要是内存设置的太小，可能导致一个问题：消息快速的写入内存缓冲里面，但是Sender线程来不及把Request发送到Kafka服务器。 这样是不是会造成内存缓冲很快就被写满？一旦被写满，就会阻塞用户线程，不让继续往Kafka写消息了。 所以对于“buffer.memory”这个参数应该结合自己的实际情况来进行压测，你需要测算一下在生产环境，你的用户线程会以每秒多少消息的频率来写入内存缓冲。 比如说每秒300条消息，那么你就需要压测一下，假设内存缓冲就32MB，每秒写300条消息到内存缓冲，是否会经常把内存缓冲写满？经过这样的压测，你可以调试出来一个合理的内存大小。 多少数据打包为一个Batch合适？接着你需要思考第二个问题，就是你的“batch.size”应该如何设置？这个东西是决定了你的每个Batch要存放多少数据就可以发送出去了。 比如说你要是给一个Batch设置成是16KB的大小，那么里面凑够16KB的数据就可以发送了。 这个参数的默认值是16KB，一般可以尝试把这个参数调节大一些，然后利用自己的生产环境发消息的负载来测试一下。 比如说发送消息的频率就是每秒300条，那么如果比如“batch.size”调节到了32KB，或者64KB，是否可以提升发送消息的整体吞吐量。 因为理论上来说，提升batch的大小，可以允许更多的数据缓冲在里面，那么一次Request发送出去的数据量就更多了，这样吞吐量可能会有所提升。 但是这个东西也不能无限的大，过于大了之后，要是数据老是缓冲在Batch里迟迟不发送出去，那么岂不是你发送消息的延迟就会很高。 比如说，一条消息进入了Batch，但是要等待5秒钟Batch才凑满了64KB，才能发送出去。那这条消息的延迟就是5秒钟。 所以需要在这里按照生产环境的发消息的速率，调节不同的Batch大小自己测试一下最终出去的吞吐量以及消息的 延迟，设置一个最合理的参数。 要是一个Batch迟迟无法凑满怎么办？要是一个Batch迟迟无法凑满，此时就需要引入另外一个参数了，“linger.ms” 他的含义就是说一个Batch被创建之后，最多过多久，不管这个Batch有没有写满，都必须发送出去了。 给大家举个例子，比如说batch.size是16kb，但是现在某个低峰时间段，发送消息很慢。 这就导致可能Batch被创建之后，陆陆续续有消息进来，但是迟迟无法凑够16KB，难道此时就一直等着吗？ 当然不是，假设你现在设置“linger.ms”是50ms，那么只要这个Batch从创建开始到现在已经过了50ms了，哪怕他还没满16KB，也要发送他出去了。 所以“linger.ms”决定了你的消息一旦写入一个Batch，最多等待这么多时间，他一定会跟着Batch一起发送出去。 避免一个Batch迟迟凑不满，导致消息一直积压在内存里发送不出去的情况。这是一个很关键的参数。 这个参数一般要非常慎重的来设置，要配合batch.size一起来设置。 举个例子，首先假设你的Batch是32KB，那么你得估算一下，正常情况下，一般多久会凑够一个Batch，比如正常来说可能20ms就会凑够一个Batch。 那么你的linger.ms就可以设置为25ms，也就是说，正常来说，大部分的Batch在20ms内都会凑满，但是你的linger.ms可以保证，哪怕遇到低峰时期，20ms凑不满一个Batch，还是会在25ms之后强制Batch发送出去。 如果要是你把linger.ms设置的太小了，比如说默认就是0ms，或者你设置个5ms，那可能导致你的Batch虽然设置了32KB，但是经常是还没凑够32KB的数据，5ms之后就直接强制Batch发送出去，这样也不太好其实，会导致你的Batch形同虚设，一直凑不满数据。 最大请求大小“max.request.size”这个参数决定了每次发送给Kafka服务器请求的最大大小，同时也会限制你一条消息的最大大小也不能超过这个参数设置的值，这个其实可以根据你自己的消息的大小来灵活的调整。 给大家举个例子，你们公司发送的消息都是那种大的报文消息，每条消息都是很多的数据，一条消息可能都要20KB。 此时你的batch.size是不是就需要调节大一些？比如设置个512KB？然后你的buffer.memory是不是要给的大一些？比如设置个128MB？ 只有这样，才能让你在大消息的场景下，还能使用Batch打包多条消息的机制。但是此时“max.request.size”是不是也得同步增加？ 因为可能你的一个请求是很大的，默认他是1MB，你是不是可以适当调大一些，比如调节到5MB？ 重试机制“retries”和“retries.backoff.ms”决定了重试机制，也就是如果一个请求失败了可以重试几次，每次重试的间隔是多少毫秒。 这个大家适当设置几次重试的机会，给一定的重试间隔即可，比如给100ms的重试间隔。 持久化机制“acks”参数决定了发送出去的消息要采用什么样的持久化策略，这个涉及到了很多其他的概念，可以参考另一篇文章： 《简历写了会Kafka，面试官90%会让你讲讲acks参数对消息持久化的影响》。 参考原文出处：微信公众号： 石杉的架构笔记 https://mp.weixin.qq.com/s/YLrGg-jx5ddmHECmdccppw]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息中间件消费到的消息处理失败怎么办？]]></title>
    <url>%2F2019%2F05%2F31%2F%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E6%B6%88%E8%B4%B9%E5%88%B0%E7%9A%84%E6%B6%88%E6%81%AF%E5%A4%84%E7%90%86%E5%A4%B1%E8%B4%A5%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[消息中间件在生产系统中的使用这是一个非常典型的生产环境的问题，很多公司都会在生产系统里使用MQ，即消息队列，或者消息中间件。 也就是说，一个系统跟另外一个系统之间进行通信的时候，假如系统A希望发送一个消息给系统B，让他去处理。 但是系统A不关注系统B到底怎么处理或者有没有处理好，所以系统A把消息发送给MQ，然后就不管这条消息的“死活”了，接着系统B从MQ里消费出来处理即可。 至于怎么处理，是否处理完毕，什么时候处理，都是系统B的事儿，与系统A无关。 上述过程，可以通过下图看的很清晰： 这样的一种通信方式，就是所谓的“异步”通信方式 对于系统A来说，只要把消息发给MQ，然后系统B就会异步的去进行处理了，系统A不需要“同步”的等待系统B处理完。 这样的好处是什么呢？ 两个字：解耦 系统A要跟系统B通信，但是他不需要关注系统B如何处理的一些细节。我们来举几个例子说明： 比如，A不需要关注B什么时候处理完，这样假如系统B处理一个消息要耗费10分钟也不关系统A的事儿。 否则，系统A直接调用系统B的接口，系统B一下子处理了10分钟怎么办？难不成系统A也阻塞等待10分钟？ 再比如，系统A不需要关注系统B处理成功与否，即使系统B处理失败了，也是系统B自己去考虑这个场景和重新尝试处理。 否则如果系统调用系统B的接口，万一处理失败了报错了，系统A受到一个调用异常该怎么处理？ 还有，系统A不需要关注系统B是否存活。万一要是系统B挂掉了，系统A通过MQ来通信也不需要管系统B的“死活”，系统B自己恢复了之后就可以从MQ消费消息再次处理即可。 否则系统A直接调用系统B的接口，万一系统B挂了，难道系统A还要把消息暂存到数据库？等待系统B恢复了再给他发过去吗？ 这就是通过MQ进行异步通信，让两个系统解耦之后的好处，可以大幅度提升整个大系统的容错性，增加系统的弹性，而不是处处耦合，一个系统出错连带导致其他系统全部出错。 解耦之后，即使出错也只是大系统中的一个系统B出错而已，不影响别人。 经典生产案例：早教盒子APP的发货接下来用一个经典的生产案例给大家说说MQ在生产的使用。 现在很多早教类的APP，都会提供早教盒子，什么意思呢？ 早教APP提供的核心服务就是三块： APP里的早教视频课程 线上微信群的助教答疑指导 线下送你早教盒子，里面有很多上课道具 这样一个妈妈陪伴孩子上早教的过程可能是这样的： 首先在APP里看早教视频课程，孩子看着很感兴趣。 接着妈妈从早教盒子里取出来道具，陪孩子把视频里的游戏和任务都做一遍，让孩子加深印象 最后每天妈妈会打卡，有助教会来给妈妈进行答疑。 所以说，假设现在我们要在一个早教APP里购买一个早教课程，他的流程大致如下： 选择购买早教课程 直接支付 创建订单 给用户增加课程权限 通知仓库准备发早教盒子 通知物流公司去仓库取早教盒子进行配送。 我们来分析一下每个环节。首先你要是购买一个早教课程，那么点击“购买”的按钮之后，一般直接会跳入一个支付界面。 这个时候，你就可以直接选择支付了。此时后台系统一定会通过支付系统跟第三方支付系统进行通信，比如说支付宝、微信之类的，然后等待支付完成。 一旦支付完成，就会在自己内部系统干两个事： 第一，给这个用户id创建一个订单； 第二，给这个用户id增加看某个早教视频课程的权限。 此时用户其实在“我的订单”界面就可以看到自己的订单了，而且在“我的课程”界面，就可以开始看早教课程的视频了。 如果对上面过程不太理解的，再看看下面的图，应该就清楚了： 但是现在问题主要在后面两个步骤，现在你的订单系统作为核心入口，他要通知仓库系统去扣减一个早教盒子的库存。 同时，还得准备好早教盒子的发货（比如说提前打包装箱，准备一些给快递公司使用的发货单之类的，需要帖子箱子上）。 然后通知第三方物流公司的系统，可以去自己的仓库取早教盒子发货了。 这两个步骤需要涉及到对仓库系统以及第三方物流公司系统的调用，那么是采用订单系统直接同步调用那两个系统的方式吗？ 恐怕不妥，因为这里最大的问题就是性能问题和可用性问题。 举个例子，假如现在仓库系统部署在其他地方，因为网络问题导致性能很差，访问速度很慢，那么是不是可能会导致用户支付之后，等待了几分钟都看不到整个流程的完成？ 或者要是说第三方物流公司的系统现在要是故障了，暂时无法访问，那么会不会导致用户支付了之后，一直没有给用户发货早教盒子？ 所以说，在这里就应该引入MQ，订单系统在完成订单的创建以及课程的分配之后，就可以发送一个消息到MQ，然后有一个专门的仓储系统负责消费这个消息，接着尝试去调用独立仓库系统通知发货，以及通知第三方物流系统去配送。 整个过程，如下图所示： 这么做有什么好处呢？ 好处是显而易见的，假如现在独立仓库系统和第三方物流系统的访问性能突然变得很差，大不了就是仓储系统在后面慢慢的跟人家通信等着人家处理完毕好了，对订单系统是没影响的。 对于订单系统而言，创建订单和分配课程都是速度很快的，然后发送个消息到MQ速度也很快。 这样一来，用户看到的就是一两秒的时间支付就成功了，然后可以查到订单，看到自己的课程，然后订单的物流显示的是“待配送”的状态。 那么如果独立仓库系统或者第三方物流系统故障了，导致仓储系统消费到一条订单消息之后，尝试进行发货失败，也就是对这条消费到的消息处理失败。这种情况，怎么处理？ 这就是本文最核心的地方了！！！ 死信队列的使用：处理失败的消息一般生产环境中，如果你有丰富的架构设计经验，都会在使用MQ的时候设计两个队列：一个是核心业务队列，一个是死信队列。 核心业务队列，就是比如上面专门用来让订单系统发送订单消息的，然后另外一个死信队列就是用来处理异常情况的。 之所以我们这篇文章抛出一个面试题，结果先长篇大论说一个生产实践案例和业务场景，就是因为面试被问到这个问题时，必须要结合你自己的业务实践经验来说。 你需要先给面试官说有血有肉的业务系统场景，然后再结合这个场景回答他的问题，因为面试官想听的就是你真实的实践经验。 比如说要是第三方物流系统故障了，此时无法请求，那么仓储系统每次消费到一条订单消息，尝试通知发货和配送，都会遇到对方的接口报错。 此时仓储系统就可以把这条消息拒绝访问，或者标志位处理失败！注意，这个步骤很重要。 一旦标志这条消息处理失败了之后，MQ就会把这条消息转入提前设置好的一个死信队列中。 然后你会看到的就是，在第三方物流系统故障期间，所有订单消息全部处理失败，全部会转入死信队列。 然后你的仓储系统得专门有一个后台线程，监控第三方物流系统是否正常，能否请求的，不停的监视。 一旦发现对方恢复正常，这个后台线程就从死信队列消费出来处理失败的订单，重新执行发货和配送的通知逻辑。 死信队列的使用，其实就是MQ在生产实践中非常重要的一环，也就是架构设计必须要考虑的。 整个过程，如下图所示： 总结最后再给各位朋友强调一下，如果面试被问到生产实践类的问题，一定记住：结合有血有肉的业务系统和场景来阐述你的实践经验，以及在业务场景下，应该如何设计技术方案。 这样你的回答，才能匹配上面试官内心深处最希望听到的满分答案！ 参考原文出处：微信公众号： 狸猫技术窝 https://mp.weixin.qq.com/s/mvLza4CT4S5S8mzPuMSydg]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2F2019%2F05%2F20%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[正则表达式在几乎所有语言中都可以使用，无论是前端的JavaScript、还是后端的Java、c#。他们都提供相应的接口/函数支持正则表达式。 很神奇的是：无论你大学选择哪一门计算机语言，都没有关于正则表达式的课程给你修，在你学会正则之前，你只能看着那些正则大师们，写了一串外星文似的字符串，替代了你用一大篇幅的if else代码来做一些数据校验。 既然喜欢，那就动手学呗，可当你百度出一一堆相关资料时，你发现无一不例外的枯燥至极，难以学习。 本文旨在用最通俗的语言讲述最枯燥的基本知识！ 正则基础知识点元字符万物皆有缘，正则也是如此，元字符是构造正则表达式的一种基本元素。 我们先来记几个常用的元字符： 元字符说明.匹配除换行符以外的任意字符w匹配字母或数字或下划线或汉字s匹配任意的空白符d匹配数字匹配单词的开始或结束^匹配字符串的开始$匹配字符串的结束 有了元字符之后，我们就可以利用这些元字符来写一些简单的正则表达式了， 比如： 匹配有abc开头的字符串：abc或者^abc 匹配8位数字的QQ号码：^dddddddd$ 匹配1开头11位数字的手机号码：^1dddddddddd$ 重复限定符有了元字符就可以写不少的正则表达式了，但细心的你们可能会发现：别人写的正则简洁明了，而不理君写的正则一堆乱七八糟而且重复的元字符组成的。正则没提供办法处理这些重复的元字符吗？ 答案是有的！ 为了处理这些重复问题，正则表达式中一些重复限定符，把重复部分用合适的限定符替代，下面我们来看一些限定符： 语法说明*重复零次或更多次+重复一次或更多次?重复零次或一次{n}重复n次{n,}重复n次或更多次{n,m}重复n到m次 有了这些限定符之后，我们就可以对之前的正则表达式进行改造了，比如： 匹配8位数字的QQ号码：^d{8}$ 匹配1开头11位数字的手机号码：^1d{10}$ 匹配银行卡号是14~18位的数字：^d{14,18}$ 匹配以a开头的，0个或多个b结尾的字符串^ab*$ 分组从上面的例子（4）中看到，限定符是作用在与他左边最近的一个字符，那么问题来了，如果我想要ab同时被限定那怎么办呢？ 正则表达式中用小括号()来做分组，也就是括号中的内容作为一个整体。 因此当我们要匹配多个ab时，我们可以这样。 如匹配字符串中包含0到多个ab开头：^(ab)* 转义我们看到正则表达式用小括号来做分组，那么问题来了： 如果要匹配的字符串中本身就包含小括号，那是不是冲突？应该怎么办？ 针对这种情况，正则提供了转义的方式，也就是要把这些元字符、限定符或者关键字转义成普通的字符，做法很简答，就是在要转义的字符前面加个斜杠，也就是即可。 如要匹配以(ab)开头：^((ab))* 条件或回到我们刚才的手机号匹配，我们都知道：国内号码都来自三大网，它们都有属于自己的号段，比如联通有130/131/132/155/156/185/186/145/176等号段，假如让我们匹配一个联通的号码，那按照我们目前所学到的正则，应该无从下手的，因为这里包含了一些并列的条件，也就是“或”，那么在正则中是如何表示“或”的呢？ 正则用符号 | 来表示或，也叫做分支条件，当满足正则里的分支条件的任何一种条件时，都会当成是匹配成功。 那么我们就可以用“或”条件来处理这个问题：^(130|131|132|155|156|185|186|145|176)d{8}$ 区间看到上面的例子，是不是看到有什么规律？是不是还有一种想要简化的冲动？ 实际是有的 正则提供一个元字符中括号 [] 来表示区间条件。 限定0到9 可以写成[0-9] 限定A-Z 写成[A-Z] 限定某些数字 [165] 那上面的正则我们还改成这样： ^((13[0-2])|(15[56])|(18[5-6])|145|176)d{8}$ 好了，正则表达式的基本用法就讲到这里了，其实它还有非常多的知识点以及元字符，我们在此只列举了部分元字符和语法来讲，旨在给那些不懂正则或者想学正则但有看不下去文档的人做一个快速入门级的教程，看完本教程，即使你不能写出高大上的正则，至少也能写一些简单的正则或者看得懂别人写的正则了。 正则进阶知识点零宽断言无论是零宽还是断言，听起来都古古怪怪的， 那先解释一下这两个词。 断言：俗话的断言就是“我断定什么什么”，而正则中的断言，就是说正则可以指明在指定的内容的前面或后面会出现满足指定规则的内容，意思正则也可以像人类那样断定什么什么，比如”ss1aa2bb3”,正则可以用断言找出aa2前面有bb3，也可以找出aa2后面有ss1. 零宽：就是没有宽度，在正则中，断言只是匹配位置，不占字符，也就是说，匹配结果里是不会返回断言本身。 意思是讲明白了，那他有什么用呢？ 我们来举个栗子：假设我们要用爬虫抓取csdn里的文章阅读量。通过查看源代码可以看到文章阅读量这个内容是这样的结构 “阅读数：641“ 其中也就‘641’这个是变量，也就是说不同文章不同的值，当我们拿到这个字符串时，需要获得这里边的‘641’有很多种办法，但如果正则应该怎么匹配呢？ 下面先来讲几种类型的断言： 正向先行断言（正前瞻）语法：（?=pattern） 作用：匹配pattern表达式的前面内容，不返回本身。 这样子说，还是一脸懵逼，好吧，回归刚才那个栗子，要取到阅读量，在正则表达式中就意味着要能匹配到‘’前面的数字内容。 按照上所说的正向先行断言可以匹配表达式前面的内容，那意思就是:(?=) 就可以匹配到前面的内容了。 匹配什么内容呢？如果要所有内容那就是： 12345678910String reg=".+(?=&lt;/span&gt;)";String test = "&lt;span class="read-count"&gt;阅读数：641&lt;/span&gt;"; Pattern pattern = Pattern.compile(reg); Matcher mc= pattern.matcher(test); while(mc.find())&#123; System.out.println("匹配结果：")； System.out.println(mc.group());&#125; //匹配结果：//&lt;span class="read-count"&gt;阅读数：641 可是老哥我们要的只是前面的数字呀，那也简单咯，匹配数字 d,那可以改成： 123456789String reg="\d+(?=&lt;/span&gt;)";String test = "&lt;span class="read-count"&gt;阅读数：641&lt;/span&gt;";Pattern pattern = Pattern.compile(reg);Matcher mc= pattern.matcher(test);while(mc.find())&#123; System.out.println(mc.group());&#125;//匹配结果：//641 大功告成！ 正向后行断言（正后顾）语法：（?&lt;=pattern） 作用：匹配pattern表达式的后面的内容，不返回本身。 有先行就有后行，先行是匹配前面的内容，那后行就是匹配后面的内容啦。 上面的栗子，我们也可以用后行断言来处理。 12345678910//(?&lt;=&lt;span class="read-count"&gt;阅读数：)d+String reg="(?&lt;=&lt;span class="read-count"&gt;阅读数：)\d+";String test = "&lt;span class="read-count"&gt;阅读数：641&lt;/span&gt;"; Pattern pattern = Pattern.compile(reg); Matcher mc= pattern.matcher(test); while(mc.find())&#123; System.out.println(mc.group()); &#125;//匹配结果：//641 就这么简单。 负向先行断言（负前瞻）语法：(?!pattern) 作用：匹配非pattern表达式的前面内容，不返回本身。 有正向也有负向，负向在这里其实就是非的意思。 举个栗子：比如有一句 “我爱祖国，我是祖国的花朵” 现在要找到不是’的花朵’前面的祖国 用正则就可以这样写：祖国(?!的花朵)。 负向后行断言（负后顾）语法：(?&lt;!pattern) 作用：匹配非pattern表达式的后面内容，不返回本身。 捕获和非捕获单纯说到捕获，他的意思是匹配表达式，但捕获通常和分组联系在一起，也就是“捕获组”。 捕获组：匹配子表达式的内容，把匹配结果保存到内存中中数字编号或显示命名的组里，以深度优先进行编号，之后可以通过序号或名称来使用这些匹配结果。 而根据命名方式的不同，又可以分为两种组。 数字编号捕获组语法：(exp) 解释：从表达式左侧开始，每出现一个左括号和它对应的右括号之间的内容为一个分组，在分组中，第0组为整个表达式，第一组开始为分组。 比如固定电话的：020-85653333 他的正则表达式为：(0d{2})-(d{8}) 按照左括号的顺序，这个表达式有如下分组： 序号编号分组内容00(0d{2})-(d{8})020-8565333311(0d{2})02022(d{8})85653333 我们用Java来验证一下： 1234567891011String test = "020-85653333"; String reg="(0\d&#123;2&#125;)-(\d&#123;8&#125;)"; Pattern pattern = Pattern.compile(reg); Matcher mc= pattern.matcher(test); if(mc.find())&#123; System.out.println("分组的个数有："+mc.groupCount()); for(int i=0;i&lt;=mc.groupCount();i++)&#123; System.out.println("第"+i+"个分组为："+mc.group(i)); &#125; &#125; 输出结果： 1234分组的个数有：2第0个分组为：020-85653333第1个分组为：020第2个分组为：85653333 可见，分组个数是2，但是因为第0个为整个表达式本身，因此也一起输出了。 命名编号捕获组语法：(?exp) 解释：分组的命名由表达式中的name指定 比如区号也可以这样写:(?d{2})-(?d{8}) 按照左括号的顺序，这个表达式有如下分组：序号名称分组内容00(0d{2})-(d{8})020-856533331quhao(0d{2})0202haoma(d{8})85653333 用代码来验证一下： 12345678910String test = "020-85653333";String reg="(?&lt;quhao&gt;0\d&#123;2&#125;)-(?&lt;haoma&gt;\d&#123;8&#125;)";Pattern pattern = Pattern.compile(reg);Matcher mc= pattern.matcher(test);if(mc.find())&#123; System.out.println("分组的个数有："+mc.groupCount()); System.out.println(mc.group("quhao")); System.out.println(mc.group("haoma"));&#125; 输出结果： 123分组的个数有：2分组名称为:quhao,匹配内容为：020分组名称为:haoma,匹配内容为：85653333 非捕获组语法：(?:exp) 解释：和捕获组刚好相反，它用来标识那些不需要捕获的分组，说的通俗一点，就是你可以根据需要去保存你的分组。 比如上面的正则表达式，程序不需要用到第一个分组，那就可以这样写：(?:d{2})-(d{8}) 序号编号分组内容00(0d{2})-(d{8})020-8565333311(d{8})85653333 验证一下： 1234567891011String test = "020-85653333";String reg="(?:0\d&#123;2&#125;)-(\d&#123;8&#125;)";Pattern pattern = Pattern.compile(reg);Matcher mc= pattern.matcher(test);if(mc.find())&#123; System.out.println("分组的个数有："+mc.groupCount()); for(inti=0;i&lt;=mc.groupCount();i++)&#123; System.out.println("第"+i+"个分组为："+mc.group(i)); &#125;&#125; 输出结果： 1分组的个数有：1第0个分组为：020-85653333第1个分组为：85653333 反向引用上面讲到捕获，我们知道：捕获会返回一个捕获组，这个分组是保存在内存中，不仅可以在正则表达式外部通过程序进行引用，也可以在正则表达式内部进行引用，这种引用方式就是反向引用。 根据捕获组的命名规则，反向引用可分为： 数字编号组反向引用：k或 umber 命名编号组反向引用：k或者’name’ 好了 讲完了，懂吗？不懂！！！ 可能连前面讲的捕获有什么用都还不懂吧？ 其实只是看完捕获不懂不会用是很正常的！ 因为捕获组通常是和反向引用一起使用的。 上面说到捕获组是匹配子表达式的内容按序号或者命名保存起来以便使用。 注意两个字眼：“内容” 和 “使用”。 这里所说的“内容”，是匹配结果，而不是子表达式本身，强调这个有什么用？嗯，先记住。 那这里所说的“使用”是怎样使用呢？ 因为它的作用主要是用来查找一些重复的内容或者做替换指定字符。 还是举栗子吧。 比如要查找一串字母”aabbbbgbddesddfiid”里成对的字母 如果按照我们之前学到的正则，什么区间啊限定啊断言啊可能是办不到的， 现在我们先用程序思维理一下思路： 1）匹配到一个字母 2）匹配第下一个字母，检查是否和上一个字母是否一样 3）如果一样，则匹配成功，否则失败 这里的思路2中匹配下一个字母时，需要用到上一个字母，那怎么记住上一个字母呢？？？ 这下子捕获就有用处啦，我们可以利用捕获把上一个匹配成功的内容用来作为本次匹配的条件 好了，有思路就要实践 首先匹配一个字母：w 我们需要做成分组才能捕获，因此写成这样：(w) 那这个表达式就有一个捕获组：（w） 然后我们要用这个捕获组作为条件，那就可以：(w) 这样就大功告成了 可能有人不明白了，是什么意思呢？ 还记得捕获组有两种命名方式吗，一种是是根据捕获分组顺序命名，一种是自定义命名来作为捕获组的命名 在默认情况下都是以数字来命名，而且数字命名的顺序是从1开始的 因此要引用第一个捕获组，根据反向引用的数字命名规则 就需要 k或者 当然，通常都是是后者。 我们来测试一下： 1234567String test = "aabbbbgbddesddfiid";Pattern pattern = Pattern.compile("(\w)\1");Matcher mc= pattern.matcher(test);while(mc.find())&#123; System.out.println(mc.group());&#125; 输出结果： 123456aabbbbddddii 嗯，这就是我们想要的了。 在举个替换的例子，假如想要把字符串中abc换成a。 123String test = "abcbbabcbcgbddesddfiid";String reg="(a)(b)c";System.out.println(test.replaceAll(reg, "$1")); 输出结果： 1abbabcgbddesddfiid 贪婪和非贪婪贪婪我们都知道，贪婪就是不满足，尽可能多的要。 在正则中，贪婪也是差不多的意思: 贪婪匹配：当正则表达式中包含能接受重复的限定符时，通常的行为是（在使整个表达式能得到匹配的前提下）匹配尽可能多的字符，这匹配方式叫做贪婪匹配。 特性：一次性读入整个字符串进行匹配，每当不匹配就舍弃最右边一个字符，继续匹配，依次匹配和舍弃（这种匹配-舍弃的方式也叫做回溯），直到匹配成功或者把整个字符串舍弃完为止，因此它是一种最大化的数据返回，能多不会少。 前面我们讲过重复限定符，其实这些限定符就是贪婪量词，比如表达式：d{3,6}。 用来匹配3到6位数字，在这种情况下，它是一种贪婪模式的匹配，也就是假如字符串里有6个个数字可以匹配，那它就是全部匹配到。 如下面的代码。 123456789String reg="\d&#123;3,6&#125;";String test="61762828 176 2991 871";System.out.println("文本："+test);System.out.println("贪婪模式："+reg);Pattern p1 =Pattern.compile(reg);Matcher m1 = p1.matcher(test);while(m1.find())&#123; System.out.println("匹配结果："+m1.group(0));&#125; 输出结果： 123456文本：61762828 176 2991 44 871贪婪模式：d&#123;3,6&#125;匹配结果：617628匹配结果：176匹配结果：2991匹配结果：871 由结果可见：本来字符串中的“61762828”这一段，其实只需要出现3个（617）就已经匹配成功了的，但是他并不满足，而是匹配到了最大能匹配的字符，也就是6个。 一个量词就如此贪婪了， 那有人会问，如果多个贪婪量词凑在一起，那他们是如何支配自己的匹配权的呢？ 是这样的，多个贪婪在一起时，如果字符串能满足他们各自最大程度的匹配时，就互不干扰，但如果不能满足时，会根据深度优先原则，也就是从左到右的每一个贪婪量词，优先最大数量的满足，剩余再分配下一个量词匹配。 123456789String reg="(\d&#123;1,2&#125;)(\d&#123;3,4&#125;)";String test="61762828 176 2991 87321";System.out.println("文本："+test);System.out.println("贪婪模式："+reg);Pattern p1 =Pattern.compile(reg);Matcher m1 = p1.matcher(test);while(m1.find())&#123; System.out.println("匹配结果："+m1.group(0));&#125; 输出结果： 12345文本：61762828 176 2991 87321贪婪模式：(d&#123;1,2&#125;)(d&#123;3,4&#125;)匹配结果：617628匹配结果：2991匹配结果：87321 “617628” 是前面的d{1,2}匹配出了61，后面的匹配出了7628 “2991” 是前面的d{1,2}匹配出了29 ，后面的匹配出了91 “87321”是前面的d{1,2}匹配出了87，后面的匹配出了321 懒惰（非贪婪）懒惰匹配：当正则表达式中包含能接受重复的限定符时，通常的行为是（在使整个表达式能得到匹配的前提下）匹配尽可能少的字符，这匹配方式叫做懒惰匹配。 特性：从左到右，从字符串的最左边开始匹配，每次试图不读入字符匹配，匹配成功，则完成匹配，否则读入一个字符再匹配，依此循环（读入字符、匹配）直到匹配成功或者把字符串的字符匹配完为止。 懒惰量词是在贪婪量词后面加个“？” 代码说明*?重复任意次，但尽可能少重复+?重复1次或更多次，但尽可能少重复??重复0次或1次，但尽可能少重复{n,m}?重复n到m次，但尽可能少重复{n,}?重复n次以上，但尽可能少重复。 123456789String reg="(\d&#123;1,2&#125;?)(\d&#123;3,4&#125;)";String test="61762828 176 2991 87321";System.out.println("文本："+test);System.out.println("贪婪模式："+reg);Pattern p1 =Pattern.compile(reg);Matcher m1 = p1.matcher(test);while(m1.find())&#123; System.out.println("匹配结果："+m1.group(0));&#125; 输出结果： 12345文本：61762828 176 2991 87321贪婪模式：(d&#123;1,2&#125;?)(d&#123;3,4&#125;)匹配结果：61762匹配结果：2991匹配结果：87321 “61762” 是左边的懒惰匹配出6，右边的贪婪匹配出1762 “2991” 是左边的懒惰匹配出2，右边的贪婪匹配出991 “87321” 左边的懒惰匹配出8，右边的贪婪匹配出7321 反义前面说到元字符的都是要匹配什么什么，当然如果你想反着来，不想匹配某些字符，正则也提供了一些常用的反义元字符。 元字符解释W匹配任意不是字母，数字，下划线，汉字的字符S匹配任意不是空白符的字符D匹配任意非数字的字符B匹配不是单词开头或结束的位置[x]匹配除了x以外的任意字符[aeiou]匹配除了aeiou这几个字母以外的任意字符 正则进阶知识就讲到这里，正则是一门博大精深的语言，其实学会它的一些语法和知识点还算不太难，但想要做到真正学以致用能写出非常6的正则，还有很远的距离，只有真正对它感兴趣的，并且经常研究和使用它，才会渐渐的理解它的博大精深之处，我就带你们走到这，剩下的，靠自己啦。 参考https://juejin.im/post/5b96a8e2e51d450e6a2de115 https://mp.weixin.qq.com/s/ndfSdcgErGEAob_mk0KfdA]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[消息队列的本质区别]]></title>
    <url>%2F2019%2F05%2F20%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E6%9C%AC%E8%B4%A8%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[平时经常会看到很多人写文章分析Kafka、RabbitMQ、RocketMQ等各种MQ之间的性能比较，功能比较，但是实际上从MQ消息队列的门派上来说，这些MQ其实是分属不同的门派的。 那么这不同的门派之间，到底有什么区别呢？ 有Broker的暴力路由这个流派最典型的就是Kafka了，Kafka实际上为了提升性能，简化了MQ功能模型，仅仅提供了一些最基础的MQ相关的功能，但是大幅度优化和提升了吞吐量。 首先，这个流派一定是有一个Broker角色的，也就是说，Kafka需要部署一套服务器集群，每台机器上都有一个Kafka Broker进程，这个进程就负责接收请求，存储数据，发送数据。 Kafka的生产消费模型做的相对是比较暴力简单的，就是简单的数据流模型。 简单来说，他有一个概念，叫做“Topic”，你可以往这个“Topic”里写数据，然后让别人从这里来消费。 这个Topic可以划分为多个Partition，每个Partition放一台机器上，存储一部分数据。 在写消息到Topic的时候，会自动把你这个消息给分发到某一个Partition上去。 然后消费消息的时候，有一个Consumer Group的概念，你部署在多台机器上的Consumer可以组成一个Group，一个Partition只能给一个Consumer消费，一个Cosumer可以消费多个Partition，这是最最核心的一点。 通过这个模型，保证一个Topic里的每条消息，只会交给Consumer Group里的一个Consumer来消费，形成了一个Queue（队列）的效果。 假如你想要有一个Queue的效果，也就是希望不停的往Queue里写数据，然后多个消费者消费，每条消息就只能给一个消费者，那么通过Kafka来实现，其实就是生产者写多个Partition，每个Partition只能给Consumer Group中的一个Consumer来消费。如下图所示： 如果要实现Publish/Subscribe的模型呢？就是说生产者发送的每条消息，都要让所有消费都消费到，怎么实现？ 那就让每个消费者都是一个独立的消费组，这样每条消息都会发送给所有的消费组，每个消费组里那唯一的一个消费者一定会消费到所有的消息。 但是除此之外，Kafka就没有任何其他的消费功能了，就是如此简单，所以属于一种比较暴力直接的流派。 它就是简单的消费模型，实现最基础的Queue和Pub/Sub两种消费模型，但是内核中大幅度优化和提升了性能以及吞吐量。 所以Kafka天生适合的场景，就是大数据领域的实时数据计算的场景。 因为在大数据的场景下，通常是弱业务的场景，没有太多复杂的业务系统交互，而主要是大量的数据流入Kafka，然后进行实时计算。 所以就是需要简单的消费模型，但是必须在内核中对吞吐量和性能进行大幅度的优化。 因此Kafka技术通常是在大数据的实时数据计算领域中使用的，比如说每秒处理几十万条消息，甚至每秒处理上百万条消息。 有Broker的复杂路由第二个流派，就是RabbitMQ为代表的流派，他强调的不是说如何提升性能和吞吐量，关注的是说要提供非常强大、复杂而且完善的消息路由功能。 所以对于RabbitMQ而言，他就不是那么简单的Topic-Partition的消费模型了。 在RabbitMQ中引入了一个非常核心的概念，叫做Exchange，这个Exchange就是负责根据复杂的业务规则把消息路由到内部的不同的Queue里去。 举个例子，如果要实现最简单的队列功能，就是让exchange往一个queue里写数据，然后多个消费者来消费这个queue里的数据，每条消息只能给一个消费者，那么可以是类似下面的方式。 如果想要实现Pub/Sub的模型，就是一条消息要被所有的消费者给消费到，那么就可以让每个消费者都有一个自己的Queue，然后绑定到一个Exchange上去。 接着，这个Exchange就设定把消息路由给所有的Queue即可，如下面这样。 此时Exchange可以把每条消息都路由给所有的Queue，每个Consumer都可以从自己的Queue里拿到所有的消息。 RabbitMQ这种流派，其实最核心的是，基于Exchange这个概念，他可以做很多复杂的事情。 比如：如果你想要某个Consumer只能消费到某一类数据，那么Exchange可以把消息里比如带“XXX”前缀的消息路由给某个Queue。或者你可以限定某个Consumer就只能消费某一部分数据。总之在这里你可以做很多的限制，设置复杂的路由规则。 但是也正是因为引入了这种复杂的消费模型，支持复杂的路由功能，导致RabbitMQ在内核以及架构设计上没法像Kafka做的那么的轻量级、高性能、可扩展、高吞吐，所以RabbitMQ在吞吐量上要比Kafka低一个数量级。 所以这种流派的MQ，往往适合用在Java业务系统中，不同的业务系统需要进行复杂的消息路由。 比如说业务系统A发送了10条消息，其中3条消息是给业务系统B的，7条消息是给业务系统C的，要实现这种复杂的路由模型，就必须依靠RabbitMQ来实现。 当然，对于这种业务系统之间的消息流转而言，可能不需要那么高的吞吐量，可能每秒业务系统之间也就转发几十条或者几百条消息，那么就完全适合采用RabbitMQ来实现。 （3）流派3：无Broker的通信流派 ZeroMQ代表的是第三种MQ。说白了，他是不需要在服务器上部署的，就是一个客户端的库而已。 也就是说，他主要是封装了底层的Socket网络通讯，然后一个系统要发送一条消息给另外一个消息消费 。 通过ZeroMQ，本质就是底层ZeroMQ发送一条消息到另外一个系统上去。 所以ZeroMQ是去中心化的，不需要跟Kafka、RabbitMQ一样在服务器上部署的。 他主要是用来进行业务系统之间的网络通信的，有点类似于比如你是一个分布式系统架构，那么此时分布式架构中的各个子系统互相之间要通信，你是基于Dubbo RPC？还是Spring Cloud HTTP？ 可能上述两种你都不想要，就是要基于原始的Socket进行网络通信，简单的收发消息而已。 此时就可以使用ZeroMQ作为分布式系统之间的消息通信，如下面那样 总结其实现在基本上MQ主要就是这三个流派，很多小众的MQ一般很少有人会用。 而且用MQ的场景主要就是两大类： 业务系统之间异步通信 大数据领域的实时数据计算 所以一般业务系统之间通信就是会采用RabbitMQ/RocketMQ，需要复杂的消息路由功能的支撑。 大数据的实时计算场景会采用Kafka，需要简单的消费模型，但是超高的吞吐量。 至于ZeroMQ，一般来说，少数分布式系统中子系统之间的分布式通信时会采用，作为轻量级的异步化的通信组件。 参考原文出处：微信公众号： 狸猫技术窝 https://mp.weixin.qq.com/s/UFT0Oc3xLONZHa_n4Gsaeg]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>数据结构</tag>
        <tag>进程</tag>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM的垃圾回收机制]]></title>
    <url>%2F2019%2F05%2F18%2FJVM%E7%9A%84%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[技术背景按照套路是要先装装X，谈谈JVM垃圾回收的前世今生的。说起垃圾回收（GC），大部分人都把这项技术当做Java语言的伴生产物。 事实上，GC的历史比Java久远，早在1960年Lisp这门语言中就使用了内存动态分配和垃圾回收技术。 哪些内存需要回收？猿们都知道JVM的内存结构包括五大区域：程序计数器、虚拟机栈、本地方法栈、堆区、方法区。 其中程序计数器、虚拟机栈、本地方法栈3个区域随线程而生、随线程而灭，因此这几个区域的内存分配和回收都具备确定性，就不需要过多考虑回收的问题，因为方法结束或者线程结束时，内存自然就跟随着回收了。 而Java堆区和方法区则不一样，这部分内存的分配和回收是动态的，正是垃圾收集器所需关注的部分。 垃圾收集器在对堆区和方法区进行回收前，首先要确定这些区域的对象哪些可以被回收，哪些暂时还不能回收，这就要用到判断对象是否存活的算法！（面试官肯定没少问你吧） 引用计数算法算法分析引用计数是垃圾收集器中的早期策略。在这种方法中，堆中每个对象实例都有一个引用计数。 当一个对象被创建时，就将该对象实例分配给一个变量，该变量计数设置为1。 当任何其它变量被赋值为这个对象的引用时，计数加1（a = b,则b引用的对象实例的计数器+1），但当一个对象实例的某个引用超过了生命周期或者被设置为一个新值时，对象实例的引用计数器减1。 任何引用计数器为0的对象实例可以被当作垃圾收集。当一个对象实例被垃圾收集时，它引用的任何对象实例的引用计数器减1。 优缺点优点：引用计数收集器可以很快的执行，交织在程序运行中。对程序需要不被长时间打断的实时环境比较有利。 缺点：无法检测出循环引用。如父对象有一个对子对象的引用，子对象反过来引用父对象。这样，他们的引用计数永远不可能为0。 感觉很无趣 ？来段代码压压惊 123456789101112public class ReferenceFindTest &#123; public static void main(String[] args) &#123; MyObject object1 = new MyObject(); MyObject object2 = new MyObject(); object1.object = object2; object2.object = object1; object1 = null; object2 = null; &#125;&#125; 这段代码是用来验证引用计数算法不能检测出循环引用。最后面两句将object1和object2赋值为null 也就是说object1和object2指向的对象已经不可能再被访问，但是由于它们互相引用对方，导致它们的引用计数器都不为0，那么垃圾收集器就永远不会回收它们。 可达性分析算法可达性分析算法是从离散数学中的图论引入的，程序把所有的引用关系看作一张图，从一个节点GC ROOT开始，寻找对应的引用节点 找到这个节点以后，继续寻找这个节点的引用节点，当所有的引用节点寻找完毕之后，剩余的节点则被认为是没有被引用到的节点，即无用的节点，无用的节点将会被判定为是可回收的对象。 在Java语言中，可作为GC Roots的对象包括下面几种： 虚拟机栈中引用的对象（栈帧中的本地变量表）； 方法区中类静态属性引用的对象； 方法区中常量引用的对象； 本地方法栈中JNI（Native方法）引用的对象。 Java中的引用你了解多少无论是通过引用计数算法判断对象的引用数量，还是通过可达性分析算法判断对象的引用链是否可达，判定对象是否存活都与“引用”有关。 在Java语言中，将引用又分为强引用、软引用、弱引用、虚引用4种，这四种引用强度依次逐渐减弱。 强引用 在程序代码中普遍存在的，类似 Object obj = new Object() 这类引用，只要强引用还存在，垃圾收集器永远不会回收掉被引用的对象。 软引用 用来描述一些还有用但并非必须的对象。 对于软引用关联着的对象，在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收。如果这次回收后还没有足够的内存，才会抛出内存溢出异常。 弱引用 也是用来描述非必需对象的，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生之前。 当垃圾收集器工作时，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。 虚引用 也叫幽灵引用或幻影引用（名字真会取，很魔幻的样子），是最弱的一种引用关系。 一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。 它的作用是能在这个对象被收集器回收时收到一个系统通知。 不要被概念吓到，也别担心，还没跑题，再深入，可就不好说了。小编罗列这四个概念的目的是为了说明，无论引用计数算法还是可达性分析算法都是基于强引用而言的。 对象死亡（被回收）前的最后一次挣扎即使在可达性分析算法中不可达的对象，也并非是“非死不可”，这时候它们暂时处于“缓刑”阶段，要真正宣告一个对象死亡，至少要经历两次标记过程。 第一次标记：如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链，那它将会被第一次标记； 第二次标记：第一次标记后接着会进行一次筛选，筛选的条件是此对象是否有必要执行finalize()方法。在finalize()方法中没有重新与引用链建立关联关系的，将被进行第二次标记。 第二次标记成功的对象将真的会被回收，如果对象在finalize()方法中重新与引用链建立了关联关系，那么将会逃离本次回收，继续存活。 猿们还跟的上吧，嘿嘿。 方法区如何判断是否需要回收方法区存储内容是否需要回收的判断可就不一样咯。方法区主要回收的内容有：废弃常量和无用的类。 对于废弃常量也可通过引用的可达性来判断，但是对于无用的类则需要同时满足下面3个条件： 该类所有的实例都已经被回收，也就是Java堆中不存在该类的任何实例； 加载该类的ClassLoader已经被回收； 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 讲了半天，主角终于要粉墨登场了。 常用的垃圾收集算法标记-清除算法标记-清除算法采用从根集合（GC Roots）进行扫描，对存活的对象进行标记 标记完毕后，再扫描整个空间中未被标记的对象，进行回收，如下图所示。 标记-清除算法不需要进行对象的移动，只需对不存活的对象进行处理，在存活对象比较多的情况下极为高效 但由于标记-清除算法直接回收不存活的对象，因此会造成内存碎片 复制算法复制算法的提出是为了克服句柄的开销和解决内存碎片的问题。 它开始时把堆分成 一个对象面和多个空闲面， 程序从对象面为对象分配空间，当对象满了，基于copying算法的垃圾收集就从根集合（GC Roots）中扫描活动对象，并将每个活动对象复制到空闲面(使得活动对象所占的内存之间没有空闲洞) 这样空闲面变成了对象面，原来的对象面变成了空闲面，程序会在新的对象面中分配内存。 标记-整理算法标记-整理算法采用标记-清除算法一样的方式进行对象的标记，但在清除时不同，在回收不存活的对象占用的空间后，会将所有的存活对象往左端空闲空间移动，并更新对应的指针。 标记-整理算法是在标记-清除算法的基础上，又进行了对象的移动，因此成本更高，但是却解决了内存碎片的问题。 具体流程见下图： 分代收集算法分代收集算法是目前大部分JVM的垃圾收集器采用的算法。它的核心思想是根据对象存活的生命周期将内存划分为若干个不同的区域。 一般情况下将堆区划分为老年代（Tenured Generation）和新生代（Young Generation），在堆区之外还有一个代就是永久代（Permanet Generation）。 老年代的特点是每次垃圾收集时只有少量对象需要被回收，而新生代的特点是每次垃圾回收时都有大量的对象需要被回收，那么就可以根据不同代的特点采取最适合的收集算法。 年轻代（Young Generation）的回收算法a) 所有新生成的对象首先都是放在年轻代的。年轻代的目标就是尽可能快速的收集掉那些生命周期短的对象。 b) 新生代内存按照8:1:1的比例分为一个eden区和两个survivor(survivor0,survivor1)区。 大部分对象在Eden区中生成，回收时先将eden区存活对象复制到一个survivor0区，然后清空eden区。 当这个survivor0区也存放满了时，则将eden区和survivor0区存活对象复制到另一个survivor1区，然后清空eden和这个survivor0区，此时survivor0区是空的 然后将survivor0区和survivor1区交换，即保持survivor1区为空， 如此往复。 c) 当survivor1区不足以存放 eden和survivor0的存活对象时，就将存活对象直接存放到老年代。 若是老年代也满了就会触发一次Full GC，也就是新生代、老年代都进行回收。 d) 新生代发生的GC也叫做Minor GC，MinorGC发生频率比较高(不一定等Eden区满了才触发)。 年老代（Old Generation）的回收算法a) 在年轻代中经历了N次垃圾回收后仍然存活的对象，就会被放到年老代中。 因此，可以认为年老代中存放的都是一些生命周期较长的对象。 b) 内存比新生代也大很多(大概比例是1:2)，当老年代内存满时触发Major GC即Full GC，Full GC发生频率比较低，老年代对象存活时间比较长，存活率标记高。 持久代（Permanent Generation）的回收算法用于存放静态文件，如Java类、方法等。持久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如Hibernate 等， 在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。 持久代也称方法区，具体的回收可参见上文2.5节。 猿们加油跟上，离offer不远啦！！！ 常见的垃圾收集器下面一张图是HotSpot虚拟机包含的所有收集器，图是借用过来滴： Serial收集器（复制算法)新生代单线程收集器，标记和清理都是单线程，优点是简单高效。是client级别默认的GC方式，可以通过-XX:+UseSerialGC来强制指定。 Serial Old收集器(标记-整理算法) 老年代单线程收集器，Serial收集器的老年代版本。 ParNew收集器(停止-复制算法) 新生代收集器，可以认为是Serial收集器的多线程版本,在多核CPU环境下有着比Serial更好的表现。 Parallel Scavenge收集器(停止-复制算法) 并行收集器，追求高吞吐量，高效利用CPU。吞吐量一般为99%， 吞吐量= 用户线程时间/(用户线程时间+GC线程时间)。适合后台应用等对交互相应要求不高的场景。是server级别默认采用的GC方式，可用-XX:+UseParallelGC来强制指定，用-XX:ParallelGCThreads=4来指定线程数。 Parallel Old收集器(停止-复制算法) Parallel Scavenge收集器的老年代版本，并行收集器，吞吐量优先。 CMS(Concurrent Mark Sweep)收集器（标记-清理算法） 高并发、低停顿，追求最短GC回收停顿时间，cpu占用比较高，响应时间快，停顿时间短，多核cpu 追求高响应时间的选择。 GC什么时候触发的（面试 5 星问题）由于对象进行了分代处理，因此垃圾回收区域、时间也不一样。GC有两种类型：Scavenge GC和Full GC。 Scavenge GC一般情况下，当新对象生成，并且在Eden申请空间失败时，就会触发Scavenge GC，对Eden区域进行GC，清除非存活对象，并且把尚且存活的对象移动到Survivor区。然后整理Survivor的两个区。 这种方式的GC是对年轻代的Eden区进行，不会影响到年老代。因为大部分对象都是从Eden区开始的，同时Eden区不会分配的很大，所以Eden区的GC会频繁进行。 因而，一般在这里需要使用速度快、效率高的算法，使Eden去能尽快空闲出来。 Full GC对整个堆进行整理，包括Young、Tenured和Perm。 Full GC因为需要对整个堆进行回收，所以比Scavenge GC要慢，因此应该尽可能减少Full GC的次数。 在对JVM调优的过程中，很大一部分工作就是对于Full GC的调节。 有如下原因可能导致Full GC： 老年代（Tenured）被写满； 持久代（Perm）被写满； System.gc()被显示调用； 上一次GC之后Heap的各域分配策略动态变化； 参考原文出处： https://www.cnblogs.com/1024Community/p/honery.html]]></content>
      <categories>
        <category>Java虚拟机</category>
      </categories>
      <tags>
        <tag>垃圾回收</tag>
        <tag>对象</tag>
        <tag>内存</tag>
        <tag>CPU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[彻底理解cookie，session，token]]></title>
    <url>%2F2019%2F05%2F16%2F%E5%BD%BB%E5%BA%95%E7%90%86%E8%A7%A3cookie%EF%BC%8Csession%EF%BC%8Ctoken%2F</url>
    <content type="text"><![CDATA[发展史1、很久很久以前，Web 基本上就是文档的浏览而已， 既然是浏览，作为服务器， 不需要记录谁在某一段时间里都浏览了什么文档，每次请求都是一个新的HTTP协议， 就是请求加响应， 尤其是我不用记住是谁刚刚发了HTTP请求， 每个请求对我来说都是全新的。这段时间很嗨皮 2、但是随着交互式Web应用的兴起，像在线购物网站，需要登录的网站等等，马上就面临一个问题，那就是要管理会话，必须记住哪些人登录系统， 哪些人往自己的购物车中放商品， 也就是说我必须把每个人区分开，这就是一个不小的挑战，因为HTTP请求是无状态的，所以想出的办法就是给大家发一个会话标识(session id), 说白了就是一个随机的字串，每个人收到的都不一样， 每次大家向我发起HTTP请求的时候，把这个字符串给一并捎过来， 这样我就能区分开谁是谁了 3、这样大家很嗨皮了，可是服务器就不嗨皮了，每个人只需要保存自己的session id，而服务器要保存所有人的session id ！ 如果访问服务器多了， 就得由成千上万，甚至几十万个。 这对服务器说是一个巨大的开销 ， 严重的限制了服务器扩展能力， 比如说我用两个机器组成了一个集群， 小F通过机器A登录了系统， 那session id会保存在机器A上， 假设小F的下一次请求被转发到机器B怎么办？ 机器B可没有小F的 session id啊。 有时候会采用一点小伎俩： session sticky ， 就是让小F的请求一直粘连在机器A上， 但是这也不管用， 要是机器A挂掉了， 还得转到机器B去。 那只好做session 的复制了， 把session id 在两个机器之间搬来搬去， 快累死了。 后来有个叫Memcached的支了招： 把session id 集中存储到一个地方， 所有的机器都来访问这个地方的数据， 这样一来，就不用复制了， 但是增加了单点失败的可能性， 要是那个负责session 的机器挂了， 所有人都得重新登录一遍， 估计得被人骂死。 也尝试把这个单点的机器也搞出集群，增加可靠性， 但不管如何， 这小小的session 对我来说是一个沉重的负担 4 于是有人就一直在思考， 我为什么要保存这可恶的session呢， 只让每个客户端去保存该多好？ 可是如果不保存这些session id , 怎么验证客户端发给我的session id 的确是我生成的呢？ 如果不去验证，我们都不知道他们是不是合法登录的用户， 那些不怀好意的家伙们就可以伪造session id , 为所欲为了。 嗯，对了，关键点就是验证 ！ 比如说， 小F已经登录了系统， 我给他发一个令牌(token)， 里边包含了小F的 user id， 下一次小F 再次通过Http 请求访问我的时候， 把这个token 通过Http header 带过来不就可以了。 不过这和session id没有本质区别啊， 任何人都可以可以伪造， 所以我得想点儿办法， 让别人伪造不了。 那就对数据做一个签名吧， 比如说我用HMAC-SHA256 算法，加上一个只有我才知道的密钥， 对数据做一个签名， 把这个签名和数据一起作为token ， 由于密钥别人不知道， 就无法伪造token了。 这个token 我不保存， 当小F把这个token 给我发过来的时候，我再用同样的HMAC-SHA256 算法和同样的密钥，对数据再计算一次签名， 和token 中的签名做个比较， 如果相同， 我就知道小F已经登录过了，并且可以直接取到小F的user id , 如果不相同， 数据部分肯定被人篡改过， 我就告诉发送者： 对不起，没有认证。 Token 中的数据是明文保存的（虽然我会用Base64做下编码， 但那不是加密）， 还是可以被别人看到的， 所以我不能在其中保存像密码这样的敏感信息。 当然， 如果一个人的token 被别人偷走了， 那我也没办法， 我也会认为小偷就是合法用户， 这其实和一个人的session id 被别人偷走是一样的。 这样一来， 我就不保存session id 了， 我只是生成token , 然后验证token ， 我用我的CPU计算时间获取了我的session 存储空间 ！ 解除了session id这个负担， 可以说是无事一身轻， 我的机器集群现在可以轻松地做水平扩展， 用户访问量增大， 直接加机器就行。 这种无状态的感觉实在是太好了！ Cookiecookie 是一个非常具体的东西，指的就是浏览器里面能永久存储的一种数据，仅仅是浏览器实现的一种数据存储功能。 cookie由服务器生成，发送给浏览器，浏览器把cookie以kv形式保存到某个目录下的文本文件内，下一次请求同一网站时会把该cookie发送给服务器。由于cookie是存在客户端上的，所以浏览器加入了一些限制确保cookie不会被恶意使用，同时不会占据太多磁盘空间，所以每个域的cookie数量是有限的。 Sessionsession 从字面上讲，就是会话。这个就类似于你和一个人交谈，你怎么知道当前和你交谈的是张三而不是李四呢？对方肯定有某种特征（长相等）表明他就是张三。 session 也是类似的道理，服务器要知道当前发请求给自己的是谁。为了做这种区分，服务器就要给每个客户端分配不同的“身份标识”，然后客户端每次向服务器发请求的时候，都带上这个“身份标识”，服务器就知道这个请求来自于谁了。至于客户端怎么保存这个“身份标识”，可以有很多种方式，对于浏览器客户端，大家都默认采用 cookie 的方式。 服务器使用session把用户的信息临时保存在了服务器上，用户离开网站后session会被销毁。这种用户信息存储方式相对cookie来说更安全，可是session有一个缺陷：如果web服务器做了负载均衡，那么下一个操作请求到了另一台服务器的时候session会丢失。 Token在Web领域基于Token的身份验证随处可见。在大多数使用Web API的互联网公司中，tokens 是多用户下处理认证的最佳方式。 以下几点特性会让你在程序中使用基于Token的身份验证 1.无状态、可扩展 2.支持移动设备 3.跨程序调用 4.安全 那些使用基于Token的身份验证的大佬们 大部分你见到过的API和Web应用都使用tokens。例如Facebook, Twitter, Google+, GitHub等。 Token的起源在介绍基于Token的身份验证的原理与优势之前，不妨先看看之前的认证都是怎么做的。 基于服务器的验证 我们都是知道HTTP协议是无状态的，这种无状态意味着程序需要验证每一次请求，从而辨别客户端的身份。 在这之前，程序都是通过在服务端存储的登录信息来辨别请求的。这种方式一般都是通过存储Session来完成。 随着Web，应用程序，已经移动端的兴起，这种验证的方式逐渐暴露出了问题。尤其是在可扩展性方面。 基于服务器验证方式暴露的一些问题 1.Seesion：每次认证用户发起请求时，服务器需要去创建一个记录来存储信息。当越来越多的用户发请求时，内存的开销也会不断增加。 2.可扩展性：在服务端的内存中使用Seesion存储登录信息，伴随而来的是可扩展性问题。 3.CORS(跨域资源共享)：当我们需要让数据跨多台移动设备上使用时，跨域资源的共享会是一个让人头疼的问题。在使用Ajax抓取另一个域的资源，就可以会出现禁止请求的情况。 4.CSRF(跨站请求伪造)：用户在访问银行网站时，他们很容易受到跨站请求伪造的攻击，并且能够被利用其访问其他的网站。 在这些问题中，可扩展行是最突出的。因此我们有必要去寻求一种更有行之有效的方法。 基于Token的验证原理基于Token的身份验证是无状态的，我们不将用户信息存在服务器或Session中。 这种概念解决了在服务端存储信息时的许多问题 NoSession意味着你的程序可以根据需要去增减机器，而不用去担心用户是否登录。 基于Token的身份验证的过程如下: 1.用户通过用户名和密码发送请求。 2.程序验证。 3.程序返回一个签名的token 给客户端。 4.客户端储存token,并且每次用于每次发送请求。 5.服务端验证token并返回数据。 每一次请求都需要token。token应该在HTTP的头部发送从而保证了Http请求无状态。我们同样通过设置服务器属性Access-Control-Allow-Origin: ，让服务器能接受到来自所有域的请求。需要主要的是，在ACAO头部标明(designating)时，不得带有像HTTP认证，客户端SSL证书和cookies的证书。 实现思路： 1.用户登录校验，校验成功后就返回Token给客户端。 2.客户端收到数据后保存在客户端 3.客户端每次访问API是携带Token到服务器端。 4.服务器端采用filter过滤器校验。校验成功则返回请求数据，校验失败则返回错误码 当我们在程序中认证了信息并取得token之后，我们便能通过这个Token做许多的事情。 我们甚至能基于创建一个基于权限的token传给第三方应用程序，这些第三方程序能够获取到我们的数据（当然只有在我们允许的特定的token） Tokens的优势无状态、可扩展在客户端存储的Tokens是无状态的，并且能够被扩展。基于这种无状态和不存储Session信息，负载负载均衡器能够将用户信息从一个服务传到其他服务器上。 如果我们将已验证的用户的信息保存在Session中，则每次请求都需要用户向已验证的服务器发送验证信息(称为Session亲和性)。用户量大时，可能会造成 一些拥堵。 但是不要着急。使用tokens之后这些问题都迎刃而解，因为tokens自己hold住了用户的验证信息。 安全性请求中发送token而不再是发送cookie能够防止CSRF(跨站请求伪造)。即使在客户端使用cookie存储token，cookie也仅仅是一个存储机制而不是用于认证。不将信息存储在Session中，让我们少了对session操作。 token是有时效的，一段时间之后用户需要重新验证。我们也不一定需要等到token自动失效，token有撤回的操作，通过token revocataion可以使一个特定的token或是一组有相同认证的token无效。 可扩展性Tokens能够创建与其它程序共享权限的程序。例如，能将一个随便的社交帐号和自己的大号(Fackbook或是Twitter)联系起来。当通过服务登录Twitter(我们将这个过程Buffer)时，我们可以将这些Buffer附到Twitter的数据流上(we are allowing Buffer to post to our Twitter stream)。 使用tokens时，可以提供可选的权限给第三方应用程序。当用户想让另一个应用程序访问它们的数据，我们可以通过建立自己的API，得出特殊权限的tokens。 多平台跨域我们提前先来谈论一下CORS(跨域资源共享)，对应用程序和服务进行扩展的时候，需要介入各种各种的设备和应用程序。 Having our API just serve data, we can also make the design choice to serve assets from a CDN. This eliminates the issues that CORS brings up after we set a quick header configuration for our application. 只要用户有一个通过了验证的token，数据和资源就能够在任何域上被请求到。 1Access-Control-Allow-Origin: * 基于标准创建token的时候，你可以设定一些选项。我们在后续的文章中会进行更加详尽的描述，但是标准的用法会在JSON Web Tokens体现。 最近的程序和文档是供给JSON Web Tokens的。它支持众多的语言。这意味在未来的使用中你可以真正的转换你的认证机制。 参考原文出处： https://www.cnblogs.com/moyand/p/9047978.html #####]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[缓存更新的套路]]></title>
    <url>%2F2019%2F04%2F29%2F%E7%BC%93%E5%AD%98%E6%9B%B4%E6%96%B0%E7%9A%84%E5%A5%97%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[看到好些人在写更新缓存数据代码时，先删除缓存，然后再更新数据库，而后续的操作会把数据再装载的缓存中。然而，这个是逻辑是错误的。试想，两个并发操作，一个是更新操作，另一个是查询操作，更新操作删除缓存后，查询操作没有命中缓存，先把老数据读出来后放到缓存中，然后更新操作更新了数据库。于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，而且还一直这样脏下去了。 我不知道为什么这么多人用的都是这个逻辑，当我在微博上发了这个贴以后，我发现好些人给了好多非常复杂和诡异的方案，所以，我想写这篇文章说一下几个缓存更新的Design Pattern（让我们多一些套路吧）。 这里，我们先不讨论更新缓存和更新数据这两个事是一个事务的事，或是会有失败的可能，我们先假设更新数据库和更新缓存都可以成功的情况（我们先把成功的代码逻辑先写对）。 更新缓存的的Design Pattern有四种：Cache aside, Read through, Write through, Write behind caching，我们下面一一来看一下这四种Pattern。 Cache Aside Pattern这是最常用最常用的pattern了。其具体逻辑如下： 失效：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。 命中：应用程序从cache中取数据，取到后返回。 更新：先把数据存到数据库中，成功后，再让缓存失效。 注意，我们的更新是先更新数据库，成功后，让缓存失效。那么，这种方式是否可以没有文章前面提到过的那个问题呢？我们可以脑补一下。 一个是查询操作，一个是更新操作的并发，首先，没有了删除cache数据的操作了，而是先更新了数据库中的数据，此时，缓存依然有效，所以，并发的查询操作拿的是没有更新的数据，但是，更新操作马上让缓存的失效了，后续的查询操作再把数据从数据库中拉出来。而不会像文章开头的那个逻辑产生的问题，后续的查询操作一直都在取老的数据。 这是标准的design pattern，包括Facebook的论文《Scaling Memcache at Facebook》也使用了这个策略。为什么不是写完数据库后更新缓存？你可以看一下Quora上的这个问答《Why does Facebook use delete to remove the key-value pair in Memcached instead of updating the Memcached during write request to the backend?》，主要是怕两个并发的写操作导致脏数据。 那么，是不是Cache Aside这个就不会有并发问题了？不是的，比如，一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，所以，会造成脏数据。 但，这个case理论上会出现，不过，实际上出现的概率可能非常低，因为这个条件需要发生在读缓存时缓存失效，而且并发着有一个写操作。而实际上数据库的写操作会比读操作慢得多，而且还要锁表，而读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存，所有的这些条件都具备的概率基本并不大。 所以，这也就是Quora上的那个答案里说的，要么通过2PC或是Paxos协议保证一致性，要么就是拼命的降低并发时脏数据的概率，而Facebook使用了这个降低概率的玩法，因为2PC太慢，而Paxos太复杂。当然，最好还是为缓存设置上过期时间。 Read/Write Through Pattern我们可以看到，在上面的Cache Aside套路中，我们的应用代码需要维护两个数据存储，一个是缓存（Cache），一个是数据库（Repository）。所以，应用程序比较啰嗦。而Read/Write Through套路是把更新数据库（Repository）的操作由缓存自己代理了，所以，对于应用层来说，就简单很多了。可以理解为，应用认为后端就是一个单一的存储，而存储自己维护自己的Cache。 Read ThroughRead Through 套路就是在查询操作中更新缓存，也就是说，当缓存失效的时候（过期或LRU换出），Cache Aside是由调用方负责把数据加载入缓存，而Read Through则用缓存服务自己来加载，从而对应用方是透明的。 Write ThroughWrite Through 套路和Read Through相仿，不过是在更新数据时发生。当有数据更新的时候，如果没有命中缓存，直接更新数据库，然后返回。如果命中了缓存，则更新缓存，然后再由Cache自己更新数据库（这是一个同步操作） 下图自来Wikipedia的Cache词条)。其中的Memory你可以理解为就是我们例子里的数据库。 Write Behind Caching PatternWrite Behind 又叫 Write Back。一些了解Linux操作系统内核的同学对write back应该非常熟悉，这不就是Linux文件系统的Page Cache的算法吗？是的，你看基础这玩意全都是相通的。所以，基础很重要，我已经不是一次说过基础很重要这事了。 Write Back套路，一句说就是，在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。这个设计的好处就是让数据的I/O操作飞快无比（因为直接操作内存嘛 ），因为异步，write backg还可以合并对同一个数据的多次操作，所以性能的提高是相当可观的。 但是，其带来的问题是，数据不是强一致性的，而且可能会丢失（我们知道Unix/Linux非正常关机会导致数据丢失，就是因为这个事）。在软件设计上，我们基本上不可能做出一个没有缺陷的设计，就像算法设计中的时间换空间，空间换时间一个道理，有时候，强一致性和高性能，高可用和高性性是有冲突的。软件设计从来都是取舍Trade-Off。 另外，Write Back实现逻辑比较复杂，因为他需要track有哪数据是被更新了的，需要刷到持久层上。操作系统的write back会在仅当这个cache需要失效的时候，才会被真正持久起来，比如，内存不够了，或是进程退出了等情况，这又叫lazy write。 在wikipedia上有一张write back的流程图，基本逻辑如下： 再多唠叨一些1）上面讲的这些Design Pattern，其实并不是软件架构里的mysql数据库和memcache/redis的更新策略，这些东西都是计算机体系结构里的设计，比如CPU的缓存，硬盘文件系统中的缓存，硬盘上的缓存，数据库中的缓存。基本上来说，这些缓存更新的设计模式都是非常老古董的，而且历经长时间考验的策略，所以这也就是，工程学上所谓的Best Practice，遵从就好了。 2）有时候，我们觉得能做宏观的系统架构的人一定是很有经验的，其实，宏观系统架构中的很多设计都来源于这些微观的东西。比如，云计算中的很多虚拟化技术的原理，和传统的虚拟内存不是很像么？Unix下的那些I/O模型，也放大到了架构里的同步异步的模型，还有Unix发明的管道不就是数据流式计算架构吗？TCP的好些设计也用在不同系统间的通讯中，仔细看看这些微观层面，你会发现有很多设计都非常精妙……所以，请允许我在这里放句观点鲜明的话——如果你要做好架构，首先你得把计算机体系结构以及很多老古董的基础技术吃透了。 3）在软件开发或设计中，我非常建议在之前先去参考一下已有的设计和思路，看看相应的guideline，best practice或design pattern，吃透了已有的这些东西，再决定是否要重新发明轮子。千万不要似是而非地，想当然的做软件设计。 4）上面，我们没有考虑缓存（Cache）和持久层（Repository）的整体事务的问题。比如，更新Cache成功，更新数据库失败了怎么吗？或是反过来。关于这个事，如果你需要强一致性，你需要使用“两阶段提交协议”——prepare, commit/rollback，比如Java 7 的XAResource，还有MySQL 5.7的 XA Transaction，有些cache也支持XA，比如EhCache。当然，XA这样的强一致性的玩法会导致性能下降，关于分布式的事务的相关话题，你可以看看《分布式系统的事务处理》一文。 总结Cache Aside Pattern —— 应用来操作和维护缓存和数据源的一致性 Read/Write Through Pattern —— 应用认为后端就是一个单一的存储，而存储自己维护自己的Cache Write Behind Caching Pattern（Write Back） —— 更新数据的时候，只更新缓存，不更新数据库，缓存会异步地批量更新数据库 参考原文出处：https://coolshell.cn/articles/17416.html?spm=5176.100239.0.0.wgNzgk]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>内存</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云Redis开发规范]]></title>
    <url>%2F2019%2F04%2F21%2F%E9%98%BF%E9%87%8C%E4%BA%91Redis%E5%BC%80%E5%8F%91%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[键值设计key名设计 (1)【建议】: 可读性和可管理性 以业务名(或数据库名)为前缀(防止key冲突)，用冒号分隔，比如业务名:表名:id 1ugc:video:1 (2)【建议】：简洁性 保证语义的前提下，控制key的长度，当key较多时，内存占用也不容忽视，例如： 1user:&#123;uid&#125;:friends:messages:&#123;mid&#125;简化为u:&#123;uid&#125;:fr:m:&#123;mid&#125;。 (3)【强制】：不要包含特殊字符 反例：包含空格、换行、单双引号以及其他转义字符 详细解析 value设计 (1)【强制】：拒绝bigkey(防止网卡流量、慢查询) string类型控制在10KB以内，hash、list、set、zset元素个数不要超过5000。 反例：一个包含200万个元素的list。 非字符串的bigkey，不要使用del删除，使用hscan、sscan、zscan方式渐进式删除，同时要注意防止bigkey过期时间自动删除问题(例如一个200万的zset设置1小时过期，会触发del操作，造成阻塞，而且该操作不会不出现在慢查询中(latency可查)) 详细解析 (2)【推荐】：选择适合的数据类型。 例如：实体类型(要合理控制和使用数据结构内存编码优化配置,例如ziplist，但也要注意节省内存和性能之间的平衡) 反例： 123set user:1:name tomset user:1:age 19set user:1:favor football 正例: 1hmset user:1 name tom age 19 favor football 【推荐】：控制key的生命周期，redis不是垃圾桶。建议使用expire设置过期时间(条件允许可以打散过期时间，防止集中过期)，不过期的数据重点关注idletime。 命令使用【推荐】 O(N)命令关注N的数量例如hgetall、lrange、smembers、zrange、sinter等并非不能使用，但是需要明确N的值。有遍历的需求可以使用hscan、sscan、zscan代替。 【推荐】：禁用命令禁止线上使用keys、flushall、flushdb等，通过redis的rename机制禁掉命令，或者使用scan的方式渐进式处理。 【推荐】合理使用selectredis的多数据库较弱，使用数字进行区分，很多客户端支持较差，同时多业务用多数据库实际还是单线程处理，会有干扰。 【推荐】使用批量操作提高效率12原生命令：例如mget、mset。非原生命令：可以使用pipeline提高效率。 但要注意控制一次批量操作的元素个数(例如500以内，实际也和元素字节数有关)。 注意两者不同： 1231. 原生是原子操作，pipeline是非原子操作。2. pipeline可以打包不同的命令，原生做不到3. pipeline需要客户端和服务端同时支持。 【建议】Redis事务功能较弱，不建议过多使用Redis的事务功能较弱(不支持回滚)，而且集群版本(自研和官方)要求一次事务操作的key必须在一个slot上(可以使用hashtag功能解决) 【建议】Redis集群版本在使用Lua上有特殊要求： 1.所有key都应该由 KEYS 数组来传递，redis.call/pcall 里面调用的redis命令，key的位置，必须是KEYS array, 否则直接返回error，”-ERR bad lua script for redis cluster, all the keys that the script uses should be passed using the KEYS array” 2.所有key，必须在1个slot上，否则直接返回error, “-ERR eval/evalsha command keys must in same slot” 【建议】必要情况下使用monitor命令时，要注意不要长时间使用。客户端使用【推荐】避免多个应用使用一个Redis实例 正例：不相干的业务拆分，公共数据做服务化。 【推荐】使用带有连接池的数据库，可以有效控制连接，同时提高效率，标准使用方式： 12345678910111213执行命令如下：Jedis jedis = null;try &#123; jedis = jedisPool.getResource(); //具体的命令 jedis.executeCommand()&#125; catch (Exception e) &#123; logger.error("op key &#123;&#125; error: " + e.getMessage(), key, e);&#125; finally &#123; //注意这里不是关闭连接，在JedisPool模式下，Jedis会被归还给资源池。 if (jedis != null) jedis.close();&#125; 下面是JedisPool优化方法的文章: Jedis常见异常汇总 JedisPool资源池优化 【建议】高并发下建议客户端添加熔断功能(例如netflix hystrix) 【推荐】设置合理的密码，如有必要可以使用SSL加密访问（阿里云Redis支持） 【建议】根据自身业务类型，选好maxmemory-policy(最大内存淘汰策略)，设置好过期时间。 默认策略是volatile-lru，即超过最大内存后，在过期键中使用lru算法进行key的剔除，保证不过期数据不被删除，但是可能会出现OOM问题。 其他策略如下： allkeys-lru：根据LRU算法删除键，不管数据有没有设置超时属性，直到腾出足够空间为止。 allkeys-random：随机删除所有键，直到腾出足够空间为止。 volatile-random:随机删除过期键，直到腾出足够空间为止。 volatile-ttl：根据键值对象的ttl属性，删除最近将要过期数据。如果没有，回退到noeviction策略。 noeviction：不会剔除任何数据，拒绝所有写入操作并返回客户端错误信息”(error) OOM command not allowed when used memory”，此时Redis只响应读操作。 相关工具【推荐】：数据同步redis间数据同步可以使用：redis-port 【推荐】：big key搜索redis大key搜索工具 【推荐】：热点key寻找(内部实现使用monitor，所以建议短时间使用)facebook的redis-faina 1阿里云Redis已经在内核层面解决热点key问题，欢迎使用。 附录：删除bigkey121. 下面操作可以使用pipeline加速。2. redis 4.0已经支持key的异步删除，欢迎使用。 Hash删除: hscan + hdel123456789101112131415161718192021public void delBigHash(String host, int port, String password, String bigHashKey) &#123; Jedis jedis = new Jedis(host, port); if (password != null &amp;&amp; !"".equals(password)) &#123; jedis.auth(password); &#125; ScanParams scanParams = new ScanParams().count(100); String cursor = "0"; do &#123; ScanResult&lt;Entry&lt;String, String&gt;&gt; scanResult = jedis.hscan(bigHashKey, cursor, scanParams); List&lt;Entry&lt;String, String&gt;&gt; entryList = scanResult.getResult(); if (entryList != null &amp;&amp; !entryList.isEmpty()) &#123; for (Entry&lt;String, String&gt; entry : entryList) &#123; jedis.hdel(bigHashKey, entry.getKey()); &#125; &#125; cursor = scanResult.getStringCursor(); &#125; while (!"0".equals(cursor)); //删除bigkey jedis.del(bigHashKey);&#125; List删除: ltrim12345678910111213141516public void delBigList(String host, int port, String password, String bigListKey) &#123; Jedis jedis = new Jedis(host, port); if (password != null &amp;&amp; !"".equals(password)) &#123; jedis.auth(password); &#125; long llen = jedis.llen(bigListKey); int counter = 0; int left = 100; while (counter &lt; llen) &#123; //每次从左侧截掉100个 jedis.ltrim(bigListKey, left, llen); counter += left; &#125; //最终删除key jedis.del(bigListKey);&#125; Set删除: sscan + srem123456789101112131415161718192021public void delBigSet(String host, int port, String password, String bigSetKey) &#123; Jedis jedis = new Jedis(host, port); if (password != null &amp;&amp; !"".equals(password)) &#123; jedis.auth(password); &#125; ScanParams scanParams = new ScanParams().count(100); String cursor = "0"; do &#123; ScanResult&lt;String&gt; scanResult = jedis.sscan(bigSetKey, cursor, scanParams); List&lt;String&gt; memberList = scanResult.getResult(); if (memberList != null &amp;&amp; !memberList.isEmpty()) &#123; for (String member : memberList) &#123; jedis.srem(bigSetKey, member); &#125; &#125; cursor = scanResult.getStringCursor(); &#125; while (!"0".equals(cursor)); //删除bigkey jedis.del(bigSetKey);&#125; SortedSet删除: zscan + zrem123456789101112131415161718192021public void delBigZset(String host, int port, String password, String bigZsetKey) &#123; Jedis jedis = new Jedis(host, port); if (password != null &amp;&amp; !"".equals(password)) &#123; jedis.auth(password); &#125; ScanParams scanParams = new ScanParams().count(100); String cursor = "0"; do &#123; ScanResult&lt;Tuple&gt; scanResult = jedis.zscan(bigZsetKey, cursor, scanParams); List&lt;Tuple&gt; tupleList = scanResult.getResult(); if (tupleList != null &amp;&amp; !tupleList.isEmpty()) &#123; for (Tuple tuple : tupleList) &#123; jedis.zrem(bigZsetKey, tuple.getElement()); &#125; &#125; cursor = scanResult.getStringCursor(); &#125; while (!"0".equals(cursor)); //删除bigkey jedis.del(bigZsetKey);&#125; 参考本文转载自：云栖社区 https://yq.aliyun.com/articles/531067]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>内存</tag>
        <tag>Redis</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Unix与Java的IO模型]]></title>
    <url>%2F2019%2F04%2F21%2FUnix%E4%B8%8EJava%E7%9A%84IO%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[要搞明白IO相关的概念，首先就得弄清楚同步与异步，阻塞与非阻塞到底是什么意思。 同步与异步想要搞明白IO模型，就先得搞明白“同步”与“异步”的关系。 所谓的“同步”，比如说调用者去调用一个接口，这个接口比如要执行一些磁盘文件读写操作，或者是网络通信操作。 假设是“同步”的模式，调用者必须要等待这个接口的磁盘读写或者网络通信的操作执行完毕了，调用者才能返回，这就是“同步”，如下图所示： 所谓的“异步”，就是说这个调用者调用接口之后，直接就返回了，他去干别的事儿了，也不管那个接口的磁盘读写或者是网络通信是否成功。 然后这个接口后续如果干完了自己的任务，比如写完了文件或者是什么的，会反过来通知调用者，之前你的那个调用成功了。可以通过一些内部通信机制来通知，也可以通过回调函数来通知，如下图。 用生活中的例子理解同步与异步如果给大家举个生活中的例子，那么就可以用买烟这个事儿来举个例子 比如说现在你要去一个柜台买很多条香烟，但是现在柜台没那么多货，他需要打电话给库房来查一下有没有足够的货。 这个时候，库房的工作人员正好去吃饭了，那现在你有两种选择： 第一种选择，你可以在柜台等着，一直等待库房工作人员回来，柜台专员打通电话给他查到了库存是否充足，你再走。 这个就是“同步”，你找柜台工作人员买香烟，他要打电话给库房工作人员问库存，如果你选择“同步”模式，那么你就在柜台一直等着，直到成功查询到库存为止。 第二种选择，你可以先回家干点儿别的，比如说洗衣服做饭之类的，然后过了一会儿，柜台工作人员打通电话给库房工作人员，查到香烟库存了，就会打个电话给你，告诉你这个事儿。 这就是“异步”，你跟柜台工作人员说了这个事儿，就直接走了，干别的去了，柜台工作人员后面完成他的任务之后，就会反过来打电话回调通知你。 阻塞与非阻塞实际上阻塞与非阻塞的概念，通常是针对底层的IO操作来说的。 比如现在我们的程序想要通过网络读取数据，如果是阻塞IO模式，一旦发起请求到操作系统内核去从网络中读取数据，就会阻塞在那里，必须要等待网络中的数据到达了之后，才能从网络读取数据到内核，再从内核返回给程序，如下图。 而非阻塞，指的就是程序发送请求给内核要从网络读取数据，但是此时网络中的数据还没到，此时不会阻塞住，内核会返回一个异常消息给程序。 程序就可以干点儿别的，然后过一会儿再来发起一次请求给内核，让内核尝试从网络读取数据。 因为如果网络中的数据还没到位，是不会阻塞住程序的，需要程序自己不断的轮询内核去尝试读取数据，所以这种IO就是非阻塞的。如下图： 不要把“同步/异步”概念和“阻塞/非阻塞”概念混淆起来，实际上他们是两组不同的概念。 “同步/异步”更多的是针对比如接口调用，服务调用，API类库调用，类似这样的场景。 而“阻塞/非阻塞”概念针对的是底层IO操作的场景，比如磁盘IO，网络IO。但是在Java IO模型里，两种概念之间是有一定的关联关系的 。 Unix支持的5种IO模型Unix操作系统支持的IO模型主要就是5种： 阻塞IO：就是上面图里的那种阻塞IO模式，程序发起请求之后会阻塞，一直到系统内核发现网络中有数据到达了，拷贝数据给程序进程了，才能返回 非阻塞IO：就是上面图里的那种非阻塞IO模式，程序发起请求读取数据，系统内核发现网络数据还没到，就返回一个异常信息，程序不会阻塞在IO操作上，但是过一会儿还得再来发起请求给内核，直到内核发现网络数据到达了，此时就会拷贝数据给程序进程 IO多路复用：这个下面来讲 信号驱动式IO：一般很少用到，这里不说明 异步IO：下面来讲 JDK 1.4之前的同步阻塞IO在JDK 1.4之前，主要就是同步阻塞IO模型，在Java里叫做BIO。 在Java代码里调用IO相关接口，发起IO操作之后，Java程序就会同步等待，这个同步指的是Java程序调用IO API接口的层面而言。 而IO API在底层的IO操作是基于阻塞IO来的，向操作系统内核发起IO请求，系统内核会等待数据就位之后，才会执行IO操作，执行完毕了才会返回。 JDK 1.4之后的同步非阻塞NIO在JDK 1.4之后提供了NIO，他的概念是同步非阻塞，也就是说如果你调用NIO接口去执行IO操作，其实还是同步等待的，但是在底层的IO操作上 ，会对系统内核发起非阻塞IO请求，以非阻塞的形式来执行IO。 也就是说，如果底层数据没到位，那么内核返回异常信息，不会阻塞住，但是NIO接口内部会采用非阻塞方式过一会儿再次调用内核发起IO请求，直到成功为止。 但是之所以说是同步非阻塞，这里的“同步”指的就是因为在你的Java代码调用NIO接口层面是同步的，你还是要同步等待底层IO操作真正完成了才可以返回，只不过在执行底层IO的时候采用了非阻塞的方式来执行罢了。 NIO网络通信与IO多路复用模型实际上，如果基于NIO进行网络通信，采取的就是多路复用的IO模型，这个多路复用IO模型针对的是网络通信中的IO场景来说的。 简单来说，就是在基于Socket进行网络通信的时候，如果有多个客户端跟你的服务端建立了Socket连接，那你就需要维护多个Socket连接。 而所谓的多路复用IO模型，就是说你的Java代码直接通过一个select函数调用，直接会进入一个同步等待的状态。 这也是为什么说NIO一定是“同步”的，因为你必须在这里同步等待某个Socket连接有请求到来。 接着你就要同步等着select函数去对底层的多个 Socket 连接进行轮询，不断的查看各个 Socket 连接谁有请求到达，就可以让select函数返回，交给我们的Java程序来处理。 select函数在底层会通过非阻塞的方式轮询各个Socket，任何一个Socket如果没有数据到达，那么非阻塞的特性会立即返回一个信息。 然后select函数可以轮询下一个Socket，不会阻塞在某个Socket上，所以底层是基于这种非阻塞的模式来“监视”各个Socket谁有数据到达的。 这就是所谓的“同步非阻塞”，但是因为操作系统把上述工作都封装在一个select函数调用里了，可以对多路Socket连接同时进行监视，所以就把这种模型称之为“IO多路复用”模型。 通过这种IO多路复用的模型，就可以用一个线程，调用一个select函数，然后监视大量的客户端连接了，如下图。 AIO以及异步IO模型最后就是JDK 1.7之后，又支持了AIO，也叫做NIO 2.0，他就支持异步IO模型了。 我们先说一下异步IO模型是什么意思。 简单来说，就是你的Java程序可以基于AIO API发起一个请求，比如说接收网络数据，AIO API底层会基于异步IO模型来调用操作系统内核。 此时不需要去管这个IO是否成功了，AIO接口会直接返回，你的Java程序也会直接返回。 然后，你的Java程序就可以去干别的事儿了。大家联想一下上面说的那个异步的例子，就可以理解这里为什么叫做异步了。 因为BIO、NIO都是同步的，你发起IO请求，都必须同步等待IO操作完成。 但是这里你发起一个IO请求，直接AIO接口就返回了，你就可以干别的事儿了，纯异步的方式。 不过你需要提供一个回调函数给AIO接口，一旦底层系统内核完成了具体的IO请求，比如网络读写之类的，就会回调你提供的回调函数。 比如说你要是通过网络读取数据，那么此时AIO接口就会把操作系统异步读取到的数据交给你的回调函数。 整个过程如下图： 参考本文转载自：狸猫技术窝 https://mp.weixin.qq.com/s/mEahtWqeFqzzaETHKAWtzw]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 单表查询的效率级别]]></title>
    <url>%2F2019%2F03%2F24%2FMySQL-%E5%8D%95%E8%A1%A8%E6%9F%A5%E8%AF%A2%E7%9A%84%E6%95%88%E7%8E%87%E7%BA%A7%E5%88%AB%2F</url>
    <content type="text"><![CDATA[对于我们这些 MySQL的使用者来说，平时用的最多的就是查询功能。DBA时不时丢过来一些慢查询语句让优化，如果连查询是怎么执行的都不清楚还优化个毛线，所以是时候掌握真正的技术了。 MySQL有一个称为 查询优化器的模块，一条查询语句进行语法解析之后就会被交给查询优化器来进行优化，优化的结果就是生成一个所谓的 执行计划，这个执行计划表明了应该使用哪些索引进行查询，表之间的连接顺序是啥样的，最后会按照执行计划中的步骤调用存储引擎提供的方法来真正的执行查询，并将查询结果返回给用户。不过查询优化这个主题有点儿大，在学会跑之前还得先学会走，所以本章先来瞅瞅 MySQL怎么执行单表查询（就是 FROM子句后边只有一个表，最简单的那种查询～）。 为了故事的发展，先得有个表： 123456789101112131415CREATE TABLE single_table ( id INT NOT NULL AUTO_INCREMENT, key1 VARCHAR(100), key2 INT, key3 VARCHAR(100), key_part1 VARCHAR(100), key_part2 VARCHAR(100), key_part3 VARCHAR(100), common_field VARCHAR(100), PRIMARY KEY (id), KEY idx_key1 (key1), UNIQUE KEY idx_key2 (key2), KEY idx_key3 (key3), KEY idx_key_part(key_part1, key_part2, key_part3)) Engine=InnoDB CHARSET=utf8; 我们为这个 single_table表建立了1个聚簇索引和4个二级索引，分别是： 为 id列建立的聚簇索引。 为 key1列建立的 idx_key1二级索引。 为 key2列建立的 idx_key2二级索引，而且该索引是唯一二级索引。 为 key3列建立的 idx_key3二级索引。 为 key_part1、 key_part2、 key_part3列建立的 idx_key_part二级索引，这也是一个联合索引。 然后我们需要为这个表插入10000行记录，除 id列外其余的列都插入随机值就好了，具体的插入语句我就不写了，自己写个程序插入吧（id列是自增主键列，不需要我们手动插入）。 访问方法（access method）的概念我们平时所写的那些查询语句本质上只是一种声明式的语法，只是告诉 MySQL我们要获取的数据符合哪些规则，至于 MySQL背地里是怎么把查询结果搞出来的那是 MySQL自己的事儿。对于单个表的查询来说，设计MySQL的大叔把查询的执行方式大致分为下边两种： 使用全表扫描进行查询这种执行方式很好理解，就是把表的每一行记录都扫一遍嘛，把符合搜索条件的记录加入到结果集就完了。不管是啥查询都可以使用这种方式执行，当然，这种也是最笨的执行方式。 使用索引进行查询因为直接使用全表扫描的方式执行查询要遍历好多记录，所以代价可能太大了。如果查询语句中的搜索条件可以使用到某个索引，那直接使用索引来执行查询可能会加快查询执行的时间。使用索引来执行查询的方式五花八门，又可以细分为许多种类： 针对主键或唯一二级索引的等值查询 针对普通二级索引的等值查询 针对索引列的范围查询 直接扫描整个索引 设计 MySQL的大叔把 MySQL执行查询语句的方式称之为 访问方法或者 访问类型。同一个查询语句可能可以使用多种不同的访问方法来执行，虽然最后的查询结果都是一样的，但是执行的时间可能差老鼻子远了，就像是从钟楼到大雁塔，你可以坐火箭去，也可以坐飞机去，当然也可以坐乌龟去。下边细细道来各种 访问方法的具体内容。 const有的时候我们可以通过主键列来定位一条记录，比方说这个查询： 1SELECT * FROM single_table WHERE id = 1438; MySQL会直接利用主键值在聚簇索引中定位对应的用户记录，就像这样： 原谅我把聚簇索引对应的复杂的 B+树结构搞了一个极度精简版，为了突出重点，我们忽略掉了 页的结构，直接把所有的叶子节点的记录都放在一起展示，而且记录中只展示我们关心的索引列，对于 single_table表的聚簇索引来说，展示的就是 id列。我们想突出的重点就是： B+树叶子节点中的记录是按照索引列排序的，对于的聚簇索引来说，它对应的 B+树叶子节点中的记录就是按照 id列排序的。 B+树本来就是一个矮矮的大胖子，所以这样根据主键值定位一条记录的速度贼快。类似的，我们根据唯一二级索引列来定位一条记录的速度也是贼快的，比如下边这个查询： 1SELECT * FROM single_table WHERE key2 = 3841; 这个查询的执行过程的示意图就是这样： 可以看到这个查询的执行分两步，第一步先从 idx_key2对应的 B+树索引中根据 key2列与常数的等值比较条件定位到一条二级索引记录，然后再根据该记录的 id值到聚簇索引中获取到完整的用户记录。 设计 MySQL的大叔认为通过主键或者唯一二级索引列与常数的等值比较来定位一条记录是像坐火箭一样快的，所以他们把这种通过主键或者唯一二级索引列来定位一条记录的访问方法定义为： const，意思是常数级别的，代价是可以忽略不计的。不过这种 const访问方法只能在主键列或者唯一二级索引列和一个常数进行等值比较时才有效，如果主键或者唯一二级索引是由多个列构成的话，索引中的每一个列都需要与常数进行等值比较，这个 const访问方法才有效（这是因为只有该索引中全部列都采用等值比较才可以定位唯一的一条记录）。 对于唯一二级索引来说，查询该列为 NULL值的情况比较特殊，比如这样： 1SELECT * FROM single_table WHERE key2 IS NULL; 因为唯一二级索引列并不限制 NULL值的数量，所以上述语句可能访问到多条记录，也就是说上边这个语句不可以使用 const访问方法来执行。 ref有时候我们对某个普通的二级索引列与常数进行等值比较，比如这样： 1SELECT * FROM single_table WHERE key1 = &apos;abc&apos;; 对于这个查询，我们当然可以选择全表扫描来逐一对比搜索条件是否满足要求，我们也可以先使用二级索引找到对应记录的 id值，然后再回表到聚簇索引中查找完整的用户记录。由于普通二级索引并不限制索引列值的唯一性，所以可能找到多条对应的记录，也就是说使用二级索引来执行查询的代价取决于等值匹配到的二级索引记录条数。如果匹配的记录较少，则回表的代价还是比较低的，所以 MySQL可能选择使用索引而不是全表扫描的方式来执行查询。设计 MySQL的大叔就把这种搜索条件为二级索引列与常数等值比较，采用二级索引来执行查询的访问方法称为： ref。我们看一下采用 ref访问方法执行查询的图示： 从图示中可以看出，对于普通的二级索引来说，通过索引列进行等值比较后可能匹配到多条连续的记录，而不是像主键或者唯一二级索引那样最多只能匹配1条记录，所以这种 ref访问方法比 const差了那么一丢丢，但是在二级索引等值比较时匹配的记录数较少时的效率还是很高的（如果匹配的二级索引记录太多那么回表的成本就太大了），跟坐高铁差不多。不过需要注意下边两种情况： 1、二级索引列值为 NULL的情况，不论是普通的二级索引，还是唯一二级索引，它们的索引列对包含 NULL值的数量并不限制，所以我们采用 key IS NULL这种形式的搜索条件最多只能使用 ref的访问方法，而不是 const的访问方法。 2、对于某个包含多个索引列的二级索引来说，只要是最左边的连续索引列是与常数的等值比较就可能采用 ref的访问方法，比方说下边这几个查询： 1SELECT * FROM single_table WHERE key_part1 = &apos;god like&apos;;SELECT * FROM single_table WHERE key_part1 = &apos;god like&apos; AND key_part2 = &apos;legendary&apos;;SELECT * FROM single_table WHERE key_part1 = &apos;god like&apos; AND key_part2 = &apos;legendary&apos; AND key_part3 = &apos;penta kill&apos;; 但是如果最左边的连续索引列并不全部是等值比较的话，它的访问方法就不能称为 ref了，比方说这样： 1SELECT * FROM single_table WHERE key_part1 = &apos;god like&apos; AND key_part2 &gt; &apos;legendary&apos;; ref or null有时候我们不仅想找出某个二级索引列的值等于某个常数的记录，还想把该列的值为 NULL的记录也找出来，就像下边这个查询： 1SELECT * FROM single_demo WHERE key1 = &apos;abc&apos; OR key1 IS NULL; 当使用二级索引而不是全表扫描的方式执行该查询时，这种类型的查询使用的访问方法就称为 ref_or_null，这个 ref_or_null访问方法的执行过程如下： 可以看到，上边的查询相当于先分别从 idx_key1索引对应的 B+树中找出 key1 IS NULL和 key1=&#39;abc&#39;的两个连续的记录范围，然后根据这些二级索引记录中的 id值再回表查找完整的用户记录。 range我们之前介绍的几种访问方法都是在对索引列与某一个常数进行等值比较的时候才可能使用到（ ref_or_null比较奇特，还计算了值为 NULL的情况），但是有时候我们面对的搜索条件更复杂，比如下边这个查询： 1SELECT * FROM single_table WHERE key2 IN (1438, 6328) OR (key2 &gt;= 38 AND key2 &lt;= 39); 我们当然还可以使用全表扫描的方式来执行这个查询，不过也可以使用 二级索引+回表的方式执行，如果采用 二级索引+回表的方式来执行的话，那么此时的搜索条件就不只是要求索引列与常数的等值匹配了，而是索引列需要匹配某个或某些范围的值，在本查询中 key2列的值只要匹配下列3个范围中的任何一个就算是匹配成功了： key2的值是 1438 key2的值是 6328 key2的值在 38和 79之间。 设计 MySQL的大叔把这种利用索引进行范围匹配的访问方法称之为： range。 小贴士：此处所说的使用索引进行范围匹配中的 索引 可以是聚簇索引，也可以是二级索引。 如果把这几个所谓的 key2列的值需要满足的 范围在数轴上体现出来的话，那应该是这个样子： 也就是从数学的角度看，每一个所谓的范围都是数轴上的一个 区间，3个范围也就对应着3个区间： 范围1： key2=1438 范围2： key2=6328 范围3： key2∈[38,39]，注意这里是闭区间。 我们可以把那种索引列等值匹配的情况称之为 单点区间，上边所说的 范围1和 范围2都可以被称为单点区间，像 范围3这种的我们可以称为连续范围区间。 index看下边这个查询： 1SELECT key_part1, key_part2, key_part3 FROM single_table WHERE key_part2 = &apos;abc&apos;; 由于 key_part2并不是联合索引 idx_key_part最左索引列，所以我们无法使用 ref或者 range访问方法来执行这个语句。但是这个查询符合下边这两个条件： 它的查询列表只有3个列： key_part1, key_part2, key_part3，而索引 idx_key_part又包含这三个列。 搜索条件中只有 key_part2列。这个列也包含在索引 idx_key_part中。 也就是说我们可以直接通过遍历 idx_key_part索引的叶子节点的记录来比较 key_part2=&#39;abc&#39;这个条件是否成立，把匹配成功的二级索引记录的 key_part1, key_part2, key_part3列的值直接加到结果集中就行了。由于二级索引记录比聚簇索记录小的多（聚簇索引记录要存储所有用户定义的列以及所谓的隐藏列，而二级索引记录只需要存放索引列和主键），而且这个过程也不用进行回表操作，所以直接遍历二级索引比直接遍历聚簇索引的成本要小很多，设计 MySQL的大叔就把这种采用遍历二级索引记录的执行方式称之为： index 使用index指的是使用覆盖索引扫描索引记录，不用回表。 all最直接的查询执行方式就是我们已经提了无数遍的全表扫描，对于 InnoDB表来说也就是直接扫描聚簇索引，设计 MySQL的大叔把这种使用全表扫描执行查询的方式称之为： all 参考本文转载自：我们都是小青蛙 https://mp.weixin.qq.com/s/GjWUxSN30gALY-QXoIgj-w]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 8中处理集合的优雅姿势——Stream]]></title>
    <url>%2F2019%2F03%2F24%2FJava-8%E4%B8%AD%E5%A4%84%E7%90%86%E9%9B%86%E5%90%88%E7%9A%84%E4%BC%98%E9%9B%85%E5%A7%BF%E5%8A%BF%E2%80%94%E2%80%94Stream%2F</url>
    <content type="text"><![CDATA[在Java中，集合和数组是我们经常会用到的数据结构，需要经常对他们做增、删、改、查、聚合、统计、过滤等操作。相比之下，关系型数据库中也同样有这些操作，但是在Java 8之前，集合和数组的处理并不是很便捷。 不过，这一问题在Java 8中得到了改善，Java 8 API添加了一个新的抽象称为流Stream，可以让你以一种声明的方式处理数据。本文就来介绍下如何使用Stream。 Stream介绍Stream 使用一种类似用 SQL 语句从数据库查询数据的直观方式来提供一种对 Java 集合运算和表达的高阶抽象。 Stream API可以极大提高Java程序员的生产力，让程序员写出高效率、干净、简洁的代码。 这种风格将要处理的元素集合看作一种流，流在管道中传输，并且可以在管道的节点上进行处理，比如筛选，排序，聚合等。 Stream有以下特性及优点： 无存储。Stream不是一种数据结构，它只是某种数据源的一个视图，数据源可以是一个数组，Java容器或I/O channel等。 为函数式编程而生。对Stream的任何修改都不会修改背后的数据源，比如对Stream执行过滤操作并不会删除被过滤的元素，而是会产生一个不包含被过滤元素的新Stream。 惰式执行。Stream上的操作并不会立即执行，只有等到用户真正需要结果的时候才会执行。 可消费性。Stream只能被“消费”一次，一旦遍历过就会失效，就像容器的迭代器那样，想要再次遍历必须重新生成。 我们举一个例子，来看一下到底Stream可以做什么事情： 上面的例子中，获取一些带颜色塑料球作为数据源，首先过滤掉红色的、把它们融化成随机的三角形。再过滤器并删除小的三角形。最后计算出剩余图形的周长。 如上图，对于流的处理，主要有三种关键性操作：分别是流的创建、中间操作（intermediate operation）以及最终操作(terminal operation)。 Stream的创建在Java 8中，可以有多种方法来创建流。 通过已有的集合来创建流在Java 8中，除了增加了很多Stream相关的类以外，还对集合类自身做了增强，在其中增加了stream方法，可以将一个集合类转换成流。 12List&lt;String&gt; strings = Arrays.asList("Hollis", "HollisChuang", "hollis", "Hello", "HelloWorld", "Hollis");Stream&lt;String&gt; stream = strings.stream(); 以上，通过一个已有的List创建一个流。除此以外，还有一个parallelStream方法，可以为集合创建一个并行流。 这种通过集合创建出一个Stream的方式也是比较常用的一种方式。 通过Stream创建流可以使用Stream类提供的方法，直接返回一个由指定元素组成的流。 1Stream&lt;String&gt; stream = Stream.of("Hollis", "HollisChuang", "hollis", "Hello", "HelloWorld", "Hollis"); 如以上代码，直接通过of方法，创建并返回一个Stream。 Stream中间操作Stream有很多中间操作，多个中间操作可以连接起来形成一个流水线，每一个中间操作就像流水线上的一个工人，每人工人都可以对流进行加工，加工后得到的结果还是一个流。 以下是常用的中间操作列表: filterfilter 方法用于通过设置的条件过滤出元素。以下代码片段使用 filter 方法过滤掉空字符串： 123List&lt;String&gt; strings = Arrays.asList("Hollis", "", "HollisChuang", "H", "hollis");strings.stream().filter(string -&gt; !string.isEmpty()).forEach(System.out::println);//Hollis, , HollisChuang, H, hollis mapmap 方法用于映射每个元素到对应的结果，以下代码片段使用 map 输出了元素对应的平方数： 123List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);numbers.stream().map( i -&gt; i*i).forEach(System.out::println);//9,4,4,9,49,9,25 limit/skiplimit 返回 Stream 的前面 n 个元素；skip 则是扔掉前 n 个元素。以下代码片段使用 limit 方法保理4个元素： 123List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);numbers.stream().limit(4).forEach(System.out::println);//3,2,2,3 sortedsorted 方法用于对流进行排序。以下代码片段使用 sorted 方法进行排序： 123List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);numbers.stream().sorted().forEach(System.out::println);//2,2,3,3,3,5,7 distinctdistinct主要用来去重，以下代码片段使用 distinct 对元素进行去重： 123List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);numbers.stream().distinct().forEach(System.out::println);//3,2,7,5 接下来我们通过一个例子和一张图，来演示下，当一个Stream先后通过filter、map、sort、limit以及distinct处理后会发生什么。 代码如下： 123List&lt;String&gt; strings = Arrays.asList("Hollis", "HollisChuang", "hollis", "Hello", "HelloWorld", "Hollis");Stream s = strings.stream().filter(string -&gt; string.length()&lt;= 6).map(String::length).sorted().limit(3) .distinct(); 过程及每一步得到的结果如下图 Stream最终操作Stream的中间操作得到的结果还是一个Stream，那么如何把一个Stream转换成我们需要的类型呢？比如计算出流中元素的个数、将流装换成集合等。这就需要最终操作（terminal operation） 最终操作会消耗流，产生一个最终结果。也就是说，在最终操作之后，不能再次使用流，也不能在使用任何中间操作，否则将抛出异常： 1java.lang.IllegalStateException: stream has already been operated upon or closed 俗话说，“你永远不会两次踏入同一条河”也正是这个意思。 常用的最终操作如下图：￼ forEachStream 提供了方法 ‘forEach’ 来迭代流中的每个数据。以下代码片段使用 forEach 输出了10个随机数： 12Random random = new Random();random.ints().limit(10).forEach(System.out::println); countcount用来统计流中的元素个数。 123List&lt;String&gt; strings = Arrays.asList(&quot;Hollis&quot;, &quot;HollisChuang&quot;, &quot;hollis&quot;,&quot;Hollis666&quot;, &quot;Hello&quot;, &quot;HelloWorld&quot;, &quot;Hollis&quot;);System.out.println(strings.stream().count());//7 collectcollect就是一个归约操作，可以接受各种做法作为参数，将流中的元素累积成一个汇总结果： 1234List&lt;String&gt; strings = Arrays.asList(&quot;Hollis&quot;, &quot;HollisChuang&quot;, &quot;hollis&quot;,&quot;Hollis666&quot;, &quot;Hello&quot;, &quot;HelloWorld&quot;, &quot;Hollis&quot;);strings = strings.stream().filter(string -&gt; string.startsWith(&quot;Hollis&quot;)).collect(Collectors.toList());System.out.println(strings);//Hollis, HollisChuang, Hollis666, Hollis 接下来，我们还是使用一张图，来演示下，前文的例子中，当一个Stream先后通过filter、map、sort、limit以及distinct处理后会，在分别使用不同的最终操作可以得到怎样的结果。 下图，展示了文中介绍的所有操作的位置、输入、输出以及使用一个案例展示了其结果。 总结本文介绍了Java 8中的Stream 的用途，优点等。还接受了Stream的几种用法，分别是Stream创建、中间操作和最终操作。 Stream的创建有两种方式，分别是通过集合类的stream方法、通过Stream的of方法。 Stream的中间操作可以用来处理Stream，中间操作的输入和输出都是Stream，中间操作可以是过滤、转换、排序等。 Stream的最终操作可以将Stream转成其他形式，如计算出流中元素的个数、将流装换成集合、以及元素的遍历等。 参考本文转载自：Hollis公众号 https://mp.weixin.qq.com/s/adKZrOe6nFEmuADHijsAtA]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker容器中无法使用jmap等命令的问题]]></title>
    <url>%2F2019%2F02%2F13%2FDocker%E5%AE%B9%E5%99%A8%E4%B8%AD%E6%97%A0%E6%B3%95%E4%BD%BF%E7%94%A8jmap%E7%AD%89%E5%91%BD%E4%BB%A4%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Java工程部署在 CentOS 服务器上。项目偶尔会出现无响应的情况，这时理所当然要上去用 JDK 相关命令看看堆栈和GC等信息了。近期在开发环境的docker排查问题时，发现GC异常频繁，且为full GC，导致系统资源消耗严重，然而jmap命令不可用，导致无法查看Java虚拟机内存占用情况，无法进一步分析。 jps、top、jstat命令均可用。 然而，使用jmap命令 1jmap -heap 1 打印堆栈信息时，却报错了 123456789101112131415161718192021222324Attaching to process ID 1, please wait...Error attaching to process: sun.jvm.hotspot.debugger.DebuggerException: Can't attach to the process: ptrace(PTRACE_ATTACH, ..) failed for 1: Operation not permittedsun.jvm.hotspot.debugger.DebuggerException: sun.jvm.hotspot.debugger.DebuggerException: Can't attach to the process: ptrace(PTRACE_ATTACH, ..) failed for 1: Operation not permitted at sun.jvm.hotspot.debugger.linux.LinuxDebuggerLocal$LinuxDebuggerLocalWorkerThread.execute(LinuxDebuggerLocal.java:163) at sun.jvm.hotspot.debugger.linux.LinuxDebuggerLocal.attach(LinuxDebuggerLocal.java:278) at sun.jvm.hotspot.HotSpotAgent.attachDebugger(HotSpotAgent.java:671) at sun.jvm.hotspot.HotSpotAgent.setupDebuggerLinux(HotSpotAgent.java:611) at sun.jvm.hotspot.HotSpotAgent.setupDebugger(HotSpotAgent.java:337) at sun.jvm.hotspot.HotSpotAgent.go(HotSpotAgent.java:304) at sun.jvm.hotspot.HotSpotAgent.attach(HotSpotAgent.java:140) at sun.jvm.hotspot.tools.Tool.start(Tool.java:185) at sun.jvm.hotspot.tools.Tool.execute(Tool.java:118) at sun.jvm.hotspot.tools.JInfo.main(JInfo.java:138) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at sun.tools.jinfo.JInfo.runTool(JInfo.java:108) at sun.tools.jinfo.JInfo.main(JInfo.java:76)Caused by: sun.jvm.hotspot.debugger.DebuggerException: Can't attach to the process: ptrace(PTRACE_ATTACH, ..) failed for 1: Operation not permitted at sun.jvm.hotspot.debugger.linux.LinuxDebuggerLocal.attach0(Native Method) at sun.jvm.hotspot.debugger.linux.LinuxDebuggerLocal.access$100(LinuxDebuggerLocal.java:62) at sun.jvm.hotspot.debugger.linux.LinuxDebuggerLocal$1AttachTask.doit(LinuxDebuggerLocal.java:269) at sun.jvm.hotspot.debugger.linux.LinuxDebuggerLocal$LinuxDebuggerLocalWorkerThread.run(LinuxDebuggerLocal.java:138) 以上的关键信息就是：Can&#39;t attach to the process: ptrace(PTRACE_ATTACH, ..) failed for 1: Operation not permitted，操作不允许。 解决方案这其实不是什么 Bug，而是 Docker 自 1.10 版本开始加入的安全特性。 类似于 jmap 这些 JDK 工具依赖于 Linux 的 PTRACE_ATTACH，而是 Docker 自 1.10 在默认的 seccomp 配置文件中禁用了 ptrace。 主要有三种解决办法： –security-opt seccomp=unconfined简单暴力（不推荐），直接关闭 seccomp 配置。用法： 1docker run --security-opt seccomp:unconfined ... –cap-add=SYS_PTRACE使用 –cap-add 明确添加指定功能： 1docker run --cap-add=SYS_PTRACE ... Docker Compose 的支持Docker Compose 自 version 1.1.0 (2015-02-25) 起支持 cap_add。官方文档：cap_add, cap_drop。用法： 前面的 docker-compose.yml 改写后文件内容如下（相同内容部分就不重复贴了）： 12345678910version: '2'services: mysql: ... api: ... cap_add: - SYS_PTRACE]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Linux</tag>
        <tag>垃圾回收</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客迁移成功]]></title>
    <url>%2F2019%2F01%2F06%2F%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E6%88%90%E5%8A%9F%2F</url>
    <content type="text"><![CDATA[成功将博客迁移到新的笔记本上，以此纪念。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud参数优化]]></title>
    <url>%2F2018%2F12%2F23%2FSpring-Cloud%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[相信不少朋友都在自己公司使用Spring Cloud框架来构建微服务架构，毕竟现在这是非常火的一门技术。 如果只是用户量很少的传统IT系统，使用Spring Cloud可能还暴露不出什么问题。 如果是较多用户量，高峰每秒高达上万并发请求的互联网公司的系统，使用Spring Cloud技术就有一些问题需要注意了。 并发量大引起系统问题新系统上线后，一开始用户规模很小，注册用户量小几十万，日活几千用户。 每天都有新的数据进入数据库的表中，就这么日积月累的，没想到数据规模居然慢慢吞吞增长到了单表几百万。 这个时候呢，看起来也没太大的毛病，就是有用户反映，系统有些操作，会感觉卡顿几秒钟，会刷不出来页面。 这是为啥呢？ 核心原因是单表数据量大了一些，达到了几百万。 有个别服务，跑的SQL比较复杂，一大堆的多表关联。 并且还没有设计好索引，或者是设计了索引，但无奈一些小弟写了上百行的大SQL，SQL实在太复杂了，那么一个SQL跑出来好几秒肯定是正常的。 如果大家对微服务框架有点了解的话，应该知道，比如Feign + Ribbon组成的服务调用框架，是有接口调用超时这一说的，有一些参数可以设置接口调用的超时时间。 如果你调用一个接口，好几秒刷不出来，人家就超时异常返回，用户就刷不出来页面了。 一般碰到这种事情，一大坨屎一样的SQL摆在那儿，写SQL的人过一个月自己都看不懂了，80%的工程师看着都不愿意去花时间重写和优化。 一是修改的人力成本太高，二是谁敢负担这责任呢？ 系统跑的好好的，就是慢了点而已，结果你硬是乱改一通，重构，把系统核心业务流程搞挂了怎么办？ 所以，那些兄弟第一反应是：增加超时时间啊！接口慢点可以，但是别超时不响应啊！ 咱们让接口执行个几秒把结果返回，用户不就可以刷出来页面了！不用重构系统了啊！轻松+愉快！ 如何增加呢？很简单，看下面的参数就知道了： 了解Spring Cloud组件作用的程序猿应该知道Spring Cloud里一般会用hystrix的线程池来执行接口调用的请求。 不了解的话，请参看这篇博客Spring Cloud各组件的作用。 所以设置超时一般设置两个地方，feign和ribbon那块的超时，还有hystrix那块的超时。其中后者那块的超时一般必须大于前者。 优化了参数后，看上去效果不错，用户虽然觉得有的页面慢是慢点，但是起码过几秒能刷出来。 这个时候，日活几千的用户量，压根儿没什么并发可言，高峰期每秒最多一二十并发请求罢了。 大家看看下面这张图，感受一下现场氛围： 随着业务的发展，用户量蹭蹭蹭的直线增长。 当高峰期每秒的并发请求居然达到了近万的时候，研发团队紧张的各种扩容服务，一台变两台，两台变四台。 然后数据库主从架构挂上去，读写分离是必须的，否则单个数据库服务器哪能承载那么大的请求！多搞几个从库，扛一下大量的读请求，这样基本就扛住了。 在这个过程中，经常会发现高峰期，系统的某个功能页面，突然就整个hang死了，就是没法再响应任何请求！所有用户刷新这个页面全部都是无法响应！ 这是为什么呢？原因很简单啊！一个服务A的实例里，专门调用服务B的那个线程池里的线程，总共可能就几十个。每个线程调用服务B都会卡住5秒钟。 那如果每秒钟过来几百个请求这个服务实例呢？一下子那个线程池里的线程就全部hang死了，没法再响应任何请求了。 大家来看看下面这张图，再直观的感受一下这个无助的过程！ 这个时候咋办？兄弟们只能祭出程序员最古老的法宝，重启机器！ 遇到页面刷不出来，只能重启机器，相当于短暂的初始化了一下机器内的资源。 然后接着运行一段时间，又卡死，再次重启！真是令人崩溃啊！用户们的体验是极差的，老板的心情是愤怒的！ 其实这个问题本身不大，但如果对Spring Cloud没有高并发场景的真实经验，确实可能会搞出些莫名其妙的问题。 比如，明明应该去优化服务接口性能，结果硬是调大了超时时间。结果导致并发量高了，对那个服务的调用直接hang死，系统的核心页面刷不出来，影响用户体验了，这怪谁呢？ 优化的过程优化服务关键点，优化图中核心服务B的性能。互联网公司，核心业务逻辑，面向C端用户高并发的请求，不要用上百行的大SQL，多表关联，那样单表几百万行数据量的话，会导致一下执行好几秒。 其实最佳的方式，就是对数据库就执行简单的单表查询和更新，然后复杂的业务逻辑全部放在java系统中来执行，比如一些关联，或者是计算之类的工作。 这一步干完了之后，那个核心服务B的响应速度就已经优化成几十毫秒了，是不是很开心？从几秒变成了几十毫秒！ 合理设置超时时间那个超时的时间，也就是上面那段ribbon和hystrix的超时时间设置。 奉劝各位同学，不要因为系统接口的性能过差而懒惰，搞成几秒甚至几十秒的超时，一般超时定义在1秒以内，是比较通用以及合理的。 为什么这么说？ 因为一个接口，理论的最佳响应速度应该在200ms以内，或者慢点的接口就几百毫秒。 如果一个接口响应时间达到1秒+，建议考虑用缓存、索引、NoSQL等各种你能想到的技术手段，优化一下性能。 否则你要是胡乱设置超时时间是几秒，甚至几十秒，万一下游服务偶然出了点问题响应时间长了点呢？那你这个线程池里的线程立马全部卡死！ 这两步解决之后，其实系统表现就正常了，核心服务B响应速度很快，而且超时时间也在1秒以内，不会出现hystrix线程池频繁卡死的情况了。 合理的重试如果你要是超时时间设置成了1秒，如果就是因为偶然发生的网络抖动，导致接口某次调用就是在1.5秒呢？这个是经常发生的，因为网络的问题，接口调用偶然超时。 所以此时配合着超时时间，一般都会设置一个合理的重试，如下所示： 设置这段重试之后，Spring Cloud中的Feign + Ribbon的组合，在进行服务调用的时候，如果发现某台机器超时请求失败，会自动重试这台机器，如果还是不行会换另外一台机器重试。 保障接口幂等性系统架构中，只要涉及到了重试，那么必须上接口的幂等性保障机制。 否则的话，试想一下，你要是对一个接口重试了好几次，结果人家重复插入了多条数据，该怎么办呢？ 其实幂等性保证本身并不复杂，根据业务来，常见的方案： 可以在数据库里建一个唯一索引，插入数据的时候如果唯一索引冲突了就不会插入重复数据 或者是通过redis里放一个唯一id值，然后每次要插入数据，都通过redis判断一下，那个值如果已经存在了，那么就不要插入重复数据了。 类似这样的方案还有一些。总之，要保证一个接口被多次调用的时候，不能插入重复的数据。 总结最终优化后的系统表现大概是长下面这样子的。 参考本文出自微信公众号：石杉的架构笔记]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>分布式</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM与Linux的内存关系分析]]></title>
    <url>%2F2018%2F12%2F23%2FJVM%E4%B8%8ELinux%E7%9A%84%E5%86%85%E5%AD%98%E5%85%B3%E7%B3%BB%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Linux与进程的内存模型JVM以一个进程（Process）的身份运行在Linux系统上，了解Linux与进程的内存关系，是理解JVM与Linux内存的关系的基础。下图给出了硬件、系统、进程三个层面的内存之间的概要关系。 从硬件上看，Linux系统的内存空间由两个部分构成：物理内存和SWAP（位于磁盘）。物理内存是Linux活动时使用的主要内存区域；当物理内 存不够使用时，Linux会把一部分暂时不用的内存数据放到磁盘上的SWAP中去，以便腾出更多的可用内存空间；而当需要使用位于SWAP的数据时，必须 先将其换回到内存中。 从Linux系统上看，除了引导系统的BIN区，整个内存空间主要被分成两个部分：内核内存（Kernel space）、用户内存（User space）。内核内存是Linux自身使用的内存空间，主要提供给程序调度、内存分配、连接硬件资源等程序逻辑使用。用户内存是提供给各个进程主要空间，Linux给 各个进程提供相同的虚拟内存空间；这使得进程之间相互独立，互不干扰。实现的方法是采用虚拟内存技术：给每一个进程一定虚拟内存空间，而只有当虚拟内存实 际被使用时，才分配物理内存。如下图所示，对于32的Linux系统来说，一般将0～3G的虚拟内存空间分配做为用户空间，将3～4G的虚拟内存空间分配 为内核空间；64位系统的划分情况是类似的。 从进程的角度来看，进程能直接访问的用户内存（虚拟内存空间）被划分为5个部分：代码区、数据区、堆区、栈区、未使用区。代码区中存放应用程序的机 器代码，运行过程中代码不能被修改，具有只读和固定大小的特点。数据区中存放了应用程序中的全局数据，静态数据和一些常量字符串等，其大小也是固定的。堆 是运行时程序动态申请的空间，属于程序运行时直接申请、释放的内存资源。栈区用来存放函数的传入参数、临时变量，以及返回地址等数据。未使用区是分配新内 存空间的预备区域。 进程与JVM内存模型JVM本质就是一个进程，因此其内存模型也有进程的一般特点。但是，JVM又不是一个普通的进程，其在内存模型上有许多崭新的特点，主要原因有两 个：1.JVM将许多本来属于操作系统管理范畴的东西，移植到了JVM内部，目的在于减少系统调用的次数；2. Java NIO，目的在于减少用于读写IO的系统调用的开销。 JVM进程与普通进程内存模型比较如下图: 需要说明的是，这个模型的并不是JVM内存使用的精确模型，更侧重于从操作系统的角度而省略了一些JVM的内部细节。下面从用户内存和内核内存两个方面讲解JVM进程的内存特点。 用户内存上图特别强调了JVM进程模型的代码区和数据区指的是JVM自身的，而非Java程序的。普通进程栈区，在JVM一般仅仅用做线程栈。JVM的堆区和普通进程的差别是最大的，下面具体详细说明： 首先是永久代。永久代本质上是Java程序的代码区和数据区。Java程序中类（class），会被加载到整个区域的不同数据结构中去，包括常量 池、域、方法数据、方法体、构造函数、以及类中的专用方法、实例初始化、接口初始化等。这个区域对于操作系统来说，是堆的一个部分；而对于Java程序来 说，这是容纳程序本身及静态资源的空间，使得JVM能够解释执行Java程序。 其次是新生代和老年代。新生代和老年代才是Java程序真正使用的堆空间，主要用于内存对象的存储；但是其管理方式和普通进程有本质的区别。普通进程在运行时给内存对象分配空间时，比如C++执行new操作时，会触发一次分配内存空间的系统调用，由操作系统的线程根据对象的大小分配好空间后返 回；同时，程序释放对象时，比如C++执行delete操作时，也会触发一次系统调用，通知操作系统对象所占用的空间已经可以回收。JVM对内存的使用和一般进程不同。JVM向操作系统申请一整段内存区域（具体大小可以在JVM参数调节）作为Java程序的堆（分为新生代和老年代）； 当Java程序申请内存空间，比如执行new操作，JVM将在这段空间中按所需大小分配给Java程序，并且Java程序不负责通知JVM何时可以释放这 个对象的空间，垃圾对象内存空间的回收由JVM进行。 JVM的内存管理方式的优点是显而易见的，包括：第一，减少系统调用的次数，JVM在给Java程序分配内存空间时不需要操作系统干预，仅仅在 Java堆大小变化时需要向操作系统申请内存或通知回收，而普通程序每次内存空间的分配回收都需要系统调用参与；第二，减少内存泄漏，普通程序没有（或者 没有及时）通知操作系统内存空间的释放是内存泄漏的重要原因之一，而由JVM统一管理，可以避免程序员带来的内存泄漏问题。 最后是未使用区，未使用区是分配新内存空间的预备区域。对于普通进程来说，这个区域被可用于堆和栈空间的申请及释放，每次堆内存分配都会使用这个区 域，因此大小变动频繁；对于JVM进程来说，调整堆大小及线程栈时会使用该区域，而堆大小一般较少调整，因此大小相对稳定。操作系统会动态调整这个区域的 大小，并且这个区域通常并没有被分配实际的物理内存，只是允许进程在这个区域申请堆或栈空间。 内核内存应用程序通常不直接和内核内存打交道，内核内存由操作系统进行管理和使用；不过随着Linux对性能的关注及改进，一些新的特性使得应用程序可以使 用内核内存，或者是映射到内核空间。Java NIO正是在这种背景下诞生的，其充分利用了Linux系统的新特性，提升了Java程序的IO性能。 上图给出了Java NIO使用的内核内存在linux系统中的分布情况。nio buffer主要包括：nio使用各种channel时所使用的ByteBuffer、Java程序主动使用 ByteBuffer.allocateDirector申请分配的Buffer。而在PageCache里面，nio使用的内存主要包 括：FileChannel.map方式打开文件占用mapped、FileChannel.transferTo和 FileChannel.transferFrom所需要的Cache（图中标示 nio file）。 Linux和Java NIO在内核内存上开辟空间给程序使用，主要是减少不要的复制，以减少IO操作系统调用的开销。例如，将磁盘文件的数据发送网卡，使用普通方法和NIO时，数据流动比较下图所示： 将数据在内核内存和用户内存之间拷贝是比较消耗资源和时间的事情，而从上图我们可以看到，通过NIO的方式减少了2次内核内存和用户内存之间的数据拷贝。这是Java NIO高性能的重要机制之一（另一个是异步非阻塞）。 从上面可以看出，内核内存对于Java程序性能也非常重要，因此，在划分系统内存使用时候，一定要给内核留出一定可用空间。 案例分析略 内存分配问题通过上面的分析，省略比较小的区域，可以总结JVM占用的内存：JVM内存 ≈ Java永久代 ＋ Java堆(新生代和老年代) ＋ 线程栈＋ Java NIO 程序在系统上运行的过程中，若SWAP和GC同时发生会导致GC时间很长，JVM严重卡顿，极端的情况下会导致服务崩溃。原因如下：JVM进行GC时，需要对相应堆分区的已用 内存进行遍历；假如GC的时候，有堆的一部分内容被交换到SWAP中，遍历到这部分的时候就需要将其交换回内存，同时由于内存空间不足，就需要把内存中堆 的另外一部分换到SWAP中去；于是在遍历堆分区的过程中，(极端情况下)会把整个堆分区轮流往SWAP写一遍。Linux对SWAP的回收是滞后的，我 们就会看到大量SWAP占用。 上述问题，可以通过减少堆大小，或者增加物理内存解决。 因此，我们得出一个结论：部署Java服务的Linux系统，在内存分配上，需要避免SWAP的使用；具体如何分配需要综合考虑不同场景下JVM对Java永久代 、Java堆(新生代和老年代)、线程栈、Java NIO所使用内存的需求。 内存泄漏问题原因：第一，Java程序没有在必要的时候调用System.gc()；第二，System.gc()被禁用。 参考http://www.open-open.com/lib/view/open1420814127390.html https://tech.meituan.com/ 微信公众号：占小狼的博客]]></content>
      <categories>
        <category>Java虚拟机</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>垃圾回收</tag>
        <tag>内存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息队列基础]]></title>
    <url>%2F2018%2F12%2F22%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[什么是MOMMOM 就是面向消息中间件（Message-oriented middleware），是用于以分布式应用或系统中的异步、松耦合、可靠、可扩展和安全通信的一类软件。MOM 的总体思想是它作为消息发送器和消息接收器之间的消息中介,这种中介提供了一个全新水平的松耦合。 MOM思想就是A和B两个应用程序不直接发送消息。之前A和B直接发送消息有很多效率问题，如A发送之后B没有及时接受，那么A就一直再在那里堵塞并发性不好，A必须等B接受完之后有返结果了A才可以结束。而MOM就是为了解决这样的问题，不让A与B之间交互，在A和B之间加一个消息中间件，A把消息放到消息中间上，就可以走了，去做别的事情，B什么时候来消息中间件取消息A不用知道也不用管。这样就提高了效率提供并发性，等B去走后可以通过状态，通知，回调等方式通知A就可以。市面上实现这种思想的技术有很多，IBM（MQSEVICES）、Microsoft(MSMQ）以及BEA的MessageMQ等。处于百家争鸣阶段都是各自实现各自的，没有统一实现标准。此时SUN为了实现统一标准就出现了JMS统一实现规范。JMS主要有2种消息模型，点到点和发布订阅两种。 什么是消息队列消息队列是在消息的传输过程中保存消息的容器，用于接收消息并以文件的方式存储，一个消息队列可以被一个也可以被多个消费者消费。 消息队列中间件是分布式系统中重要的组件，主要解决应用耦合、异步消息、流量削锋等问题。实现高性能、高可用、可伸缩和最终一致性架构。是大型分布式系统不可缺少的中间件。 目前在生产环境，使用较多的消息队列有ActiveMQ、RabbitMQ、ZeroMQ、Kafka、MetaMQ、RocketMQ等。 消息队列优点 将数据从一个应用程序传到另一个应用程序，或者从软件的一个模块传送到另外一个模块 负责建立网络通信的通道，进行数据的可靠传送 保证数据不重发，不丢失 能够实现跨平台操作，能够为不同操作系统上的软件集成技工数据传送服务 消息队列的应用场景下面详细介绍一下消息队列在实际应用中常用的使用场景。场景分为异步处理、应用解耦、流量削锋和消息通讯四个场景。 异步处理场景说明 用户注册后，需要发送注册邮件和发送注册信息，传统的做法有两种：串行方式、并行方式 串行方式 将注册信息写入数据库成功后，发送注册邮件，然后发送注册短信，而所有任务执行完成后，返回信息给客户端 并行方式 将注册信息写入数据库成功后，同时进行发送注册邮件和发送注册短信的操作。而所有任务执行完成后，返回信息给客户端。同串行方式相比，并行方式可以提高执行效率，减少执行时间。 上面的比较可以发现，假设三个操作均需要50ms的执行时间，排除网络因素，则最终执行完成，串行方式需要150ms，而并行方式需要100ms。 因为cpu在单位时间内处理的请求数量是一致的，假设：CPU每1秒吞吐量是100此，则串行方式1秒内可执行的请求量为1000/150，不到7次；并行方式1秒内可执行的请求量为1000/100，为10次。 由上可以看出，传统串行和并行的方式会受到系统性能的局限，那么如何解决这个问题？我们需要引入消息队列，将不是必须的业务逻辑，异步进行处理，由此改造出来的流程为 根据上述的流程，用户的响应的时间基本相当于将数据写入数据库的时间，发送注册邮件，发送注册短信的消息在写入消息队列后，即可返回执行结果，写入消息队列的时间很快，几乎可以忽略，也有此可以将系统吞吐量提升至20QPS，比串行方式提升近3倍，比并行方式提升2倍。 应用解耦场景说明 用户下单后，订单系统需要通知库存系统。 传统的做法为：订单系统调用库存系统的接口。如下图所示： 传统方式具有如下缺点： 假设库存系统访问失败，则订单减少库存失败，导致订单创建失败 订单系统同库存系统过度耦合 如何解决上述的缺点呢？需要引入消息队列，引入消息队列后的架构如下图所示： 引入消息队列，实现应用解耦 订单系统：用户下单后，订单系统进行数据持久化处理，然后将消息写入消息队列，返回订单创建成功 库存系统：使用拉/推的方式，获取下单信息，库存系统根据订单信息，进行库存操作。 假如在下单时库存系统不能正常使用。也不影响正常下单，因为下单后，订单系统写入消息队列就不再关心其后续操作了。由此实现了订单系统与库存系统的应用解耦。 流量削锋流量削峰 也是消息对列中的常用场景，一般在秒杀或团抢活动中使用广泛。 应用场景 秒杀活动，一般会因为流量过大，导致流量暴增，应用挂掉。为解决这个问题，一般需要在应用前端加入消息队列。 可以控制参与活动的人数； 可以缓解短时间内高流量对应用的巨大压力； 流量削锋处理方式系统图如下： 服务器在接收到用户请求后，首先写入消息队列。这时如果消息队列中消息数量超过最大数量，则直接拒绝用户请求或返回跳转到错误页面； 秒杀业务根据秒杀规则读取消息队列中的请求信息，进行后续处理。 日志处理 日志处理是指将消息队列用在日志处理中，比如Kafka的应用，解决大量日志传输的问题。 日志处理是指将消息队列用在日志处理中，比如Kafka的应用，解决大量日志传输的问题。架构简化如下： 日志采集客户端：负责日志数据采集，定时写受写入Kafka队列； Kafka消息队列：负责日志数据的接收，存储和转发； 日志处理应用：订阅并消费kafka队列中的日志数据； Kafka：接收用户日志的消息队列。 Logstash：做日志解析，统一成JSON输出给Elasticsearch。 Elasticsearch：实时日志分析服务的核心技术，一个schemaless，实时的数据存储服务，通过index组织数据，兼具强大的搜索和统计功能。 Kibana：基于Elasticsearch的数据可视化组件，超强的数据可视化能力是众多公司选择ELK stack的重要原因。 消息通讯消息通讯是指，消息队列一般都内置了高效的通信机制，因此也可以用在纯的消息通讯。比如实现点对点消息队列、聊天室等。 点对点通讯 在点对点通讯架构设计中，客户端A和客户端B共用一个消息队列，即可实现消息通讯功能。 聊天室通讯 客户端A、客户端B、直至客户端N订阅同一消息队列，进行消息的发布与接收，即可实现聊天通讯方案架构设计。 JMS消息服务讲消息队列就不得不提JMS。JMS(Java Message Service，Java消息服务) JMS 叫做 Java 消息服务（Java Message Service）,是 Java 平台上有关面向 MOM 的技术规范，旨在通过提供标准的产生、发送、接收和处理消息的 API 简化企业应用的开发，类似于 JDBC 和关系型数据库通信方式的抽象。 API是一个消息服务的标准/规范，允许应用程序组件基于JavaEE平台创建，发送，接收和读取消息。他是分布式通信耦合度更低，消息服务更加可靠以及异步性。 在EJB架构中，有消息bean可以无缝的与JM消息服务集成。在J2EE架构模式中，有消息服务者模式，用于实现消息与应用直接的解耦。 常用概念 Provider：纯 Java 语言编写的 JMS 接口实现（比如 ActiveMQ 就是） Domains：消息传递方式，包括点对点（P2P）、发布/订阅（Pub/Sub）两种 Connection factory：客户端使用连接工厂来创建与 JMS provider 的连接 Destination：消息被寻址、发送以及接收的对象 消息模型在JMS标准中，有两种消息模型P2P（Point to Point），Publish/Subscribe（Pub/Sub） P2P 模式 P2P（点对点）模式包含三个角色：消息队列（Queue），发送者（Sender），接收者（Receiver）。每个消息都被发送到一个特定的队列，接收者从队列中获取消息。队列保留着消息，知道他们被消费或者超时。 P2P 消息域使用 queue 作为 Destination，消息可以被同步或异步的发送和接收，每个消息只会给一个 Consumer 传送一次。Consumer 可以使用 MessageConsumer.receive() 同步地接收消息，也可以通过使用MessageConsumer.setMessageListener() 注册一个 MessageListener 实现异步接收。 多个 Consumer 可以注册到同一个 queue 上，但一个消息只能被一个 Consumer 所接收，然后由该 Consumer 来确认消息。并且在这种情况下，Provider 对所有注册的 Consumer 以轮询的方式发送消息。 P2P的特点 每个消息只有一个消费者（Consumer）（即一旦被消费，消息就不再在消息队列中，其他的消费者就不能得到这条消息了。） 发送者和接收者质检在时间上没有依赖性，也就是说当发送者发送了消息之后，不管接收者有没有正在运行，他不会影响到消息发送到队列。 消费者必须确认对消息的接收 ​ 收到消息后消费者必须确认消息已被接收，否则JMS服务提供者会认为该消息没有被接收，那么这条消息仍然可以被其他人接收。程序可以自动进行确认，不需要人工干预。 非持久的消息最多只发送一次 ​ 非持久的消息最多只发送一次，表示消息有可能未被发送，造成未被发送的原因可能有： ​ 1、 JMS服务提供者出现宕机等情况，造成非持久信息的丢失 ​ 2、 队列中的消息过期，未被接收 持久的消息严格发送一次 ​ 我们可以将比较重要的消息设置为持久化的消息，持久化后的消息不会因为JMS服务提供者的故障或者其他原因造成消息丢失。 如果希望发送的每个消息都会被成功处理的话，那么需要p2p 模式 Pub/Sub模式 包含三个角色：主题（Topic），发布者（Publisher），订阅者（Subscriber）。多个发布者将消息发送到Topic，系统将这些消息传递给多个订阅者。 Pub/Sub（发布/订阅，Publish/Subscribe）消息域使用 topic 作为 Destination，发布者向 topic 发送消息，订阅者注册接收来自 topic 的消息。发送到 topic 的任何消息都将自动传递给所有订阅者。接收方式（同步和异步）与 P2P 域相同。 除非显式指定，否则 topic 不会为订阅者保留消息。当然，这可以通过持久化（Durable）订阅来实现消息的保存。这种情况下，当订阅者与 Provider 断开时，Provider 会为它存储消息。当持久化订阅者重新连接时，将会受到所有的断连期间未消费的消息。 Pub/Sub的特点 每个消息都可以有多个（0，1，……）订阅者，每条消息可以有多个消费者，如果报纸和杂志一样，谁订阅了谁都可以获得。 发布者和订阅者之间有时间上的依赖性。订阅者只能消费他们订阅之后出版的消息，针对某个主题（Topic）的订阅者，它必须创建一个订阅者之后，才能消费发布者的消息。这就要求订阅者必须先订阅，生产者再发布。即订阅者必须先运行，再等待生产者的运行，这和点对点类型有所差异。 为了消费消息，订阅者必须保持运行的状态。即订阅者必须保持活动状态等待发布者发布的消息，如果订阅者在发布者发布消息之后才运行，则不能获得先前发布者发布的消息。 为了缓和这样严格的时间相关性，JMS允许订阅者创建一个可持久化的订阅。这样，即使订阅者没有被激活（运行），它也能接收到发布者的消息。如果希望发送的消息可以不被做任何处理、或者只被一个消息者处理、或者可以被多个消费者处理的话，那么可以采用Pub/Sub模型。 消息消费在JMS中，消息的产生和消费都是异步的。对于消费来说，JMS的消息者可以通过两种方式来小消费消息。 同步订阅者或接收者通过receive方法来接受消息，receive在接收到消息之前（或超时之前）将一直阻塞。 异步订阅者或接收者亦可以注册未一个消息监听器。当消息到达之后，系统自动调用监听器的onMessage的方法。 JDNI：Java命名和目录接口，是一种标准的Java命名系统接口。可以在网络上查找和访问服务。通过指定一个资源名称，该名称对应于数据库或者命名服务中的一个记录，同时返回资源连接建立所必需的信息。 JNDI在JMS中起到查找二号访问发送目标或消息来源的作用。 JMS编程JMS通用步骤 获取连接工厂 使用连接工厂创建连接 启动连接 从连接创建会话 获取 Destination 创建 Producer，或 创建 Producer 创建 message 创建 Consumer，或发送或接收message发送或接收 message 创建 Consumer 注册消息监听器（可选） 发送或接收 message 关闭资源（connection, session, producer, consumer 等) JMS编程模型1.ConnectionFactory 创建Connection对象的工厂，针对两周不同的JMS消息模型，分别有QueueConnectionFactory和TopicConnectionFactory两种。可以通过JNDI来查找ConnectionFactory对象。 2.Destination Destination的意思是消息生产者的消息发送目标或着说消息消费者的消息来源。对于消息生产者来说。他的Destination是某个队列(queue)或者某个主题（Topic）；对于消息消费者来说，他的Destination也是某个队列或主题（即消息来源）。 所以，Destination实际上就是两种类型的对象：Queue，Topic可以通过JNDI来查找Destination 3.Connection Connection表示在客户端和JMS系统之间建立的链接（对TCP/IP Socket的包装）。Connection可以产生一个或多个Session。跟ConnectionFactory一样，Connection也有两种类型：QueueConnection和TopicConnection。 4.Session Session是操作消息的接口。可以通过session创建生产者、消费者、消息等。Session提供了事务的功能。当需要使用session发送/接收多个消息时，可以将这些发送/接收动作放到一个事务中。同样，也分QueueSession和TopicSession。 5.消息的生产者 消息生产者由Session创建，并用于将消息发送到Destination。同样，消息生产者分两种类型：QueueSender和TopicPublisher。可以调用消息生产者的方法（send或publish方法）发送消息。 6.消息消费者 消息消费者由Session创建，用于接收被发送到Destination的消息。两种类型：QueueReceiver和TopicSubscriber。可分别通过session的createReceiver(Queue)或createSubscriber(Topic)来创建。当然，也可以session的creatDurableSubscriber方法来创建持久化的订阅者。 MessageListener 消息监听器。如果注册了消息监听器，一旦消息到达，将自动调用监听器的onMessage方法。EJB中的MDB（Message-Driven Bean）就是一种MessageListener。 深入学习JMS对掌握JAVA架构、EJB架构有很好的帮助，消息中间件也是大型分布式系统必须的组件。本次分享主要做全局性介绍，具体的深入需要大家学习，实践，总结，领会。 JMS编程实战这里拿ActiveMQ 举例 123456789101112131415161718192021222324252627282930313233343536373839404142public class JMSDemo &#123; ConnectionFactory connectionFactory; Connection connection; Session session; Destination destination; MessageProducer producer; MessageConsumer consumer; Message message; boolean useTransaction = false; try &#123; Context ctx = new InitialContext(); connectionFactory = (ConnectionFactory) ctx.lookup("ConnectionFactoryName"); //使用ActiveMQ时：connectionFactory = new ActiveMQConnectionFactory(user, password, getOptimizeBrokerUrl(broker)); connection = connectionFactory.createConnection(); connection.start(); session = connection.createSession(useTransaction, Session.AUTO_ACKNOWLEDGE); destination = session.createQueue("TEST.QUEUE"); //生产者发送消息 producer = session.createProducer(destination); message = session.createTextMessage("this is a test"); //消费者同步接收 consumer = session.createConsumer(destination); message = (TextMessage) consumer.receive(1000); System.out.println("Received message: " + message); //消费者异步接收 consumer.setMessageListener(new MessageListener() &#123; @Override public void onMessage(Message message) &#123; if (message != null) &#123; doMessageEvent(message); &#125; &#125; &#125;); &#125; catch (JMSException e) &#123; ... &#125; finally &#123; producer.close(); session.close(); connection.close(); &#125;&#125; 参考本文转载自： https://juejin.im/post/5c0522b4e51d4579402b8da2]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql索引优化]]></title>
    <url>%2F2018%2F12%2F19%2FMysql%E7%B4%A2%E5%BC%95%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[关于MySQL中索引的数据结构及分类可以参考这篇博客。 建表、索引语句示例 12345678910111213141516171819202122232425262728建表：creat table student( stu_id int unsigned not null auto_increment, name varchar(32) not null default &apos;&apos;, phone char(11) not null default &apos;&apos;, stu_code varchar(32) not null default &apos;&apos;, stu_desc text, primary key (&apos;stu_id&apos;), //主键索引 unique index &apos;stu_code&apos; (&apos;stu_code&apos;), //唯一索引 index &apos;name_phone&apos; (&apos;name&apos;,&apos;phone&apos;), //普通索引，复合索引 fulltext index &apos;stu_desc&apos; (&apos;stu_desc&apos;), //全文索引) engine=myisam charset=utf8;说明：MySQL5.6版本后的InnoDB存储引擎开始支持全文索引，5.7版本后通过使用ngram插件开始支持中文。更新：alert table student add primary key (&apos;stu_id&apos;), //主键索引 add unique index &apos;stu_code&apos; (&apos;stu_code&apos;), //唯一索引 add index &apos;name_phone&apos; (&apos;name&apos;,&apos;phone&apos;), //普通索引，复合索引 add fulltext index &apos;stu_desc&apos; (&apos;stu_desc&apos;); //全文索引删除：alert table sutdent drop primary key, drop index &apos;stu_code&apos;, drop index &apos;name_phone&apos;, drop index &apos;stu_desc&apos;; 索引的使用原则列独立保证索引包含的字段独立在查询语句中，不能是在表达式中 左前缀like:匹配模式左边不能以通配符开始，才能使用索引注意：前缀索引在排序 order by 和分组 group by 操作的时候无法使用。 复合索引由左到右生效建立联合索引，要同时考虑列查询的频率和列的区分度。 建立的索引为index(a,b,c) 语句 索引是否发挥作用 where a=3 是，只使用了a where a=3 and b=5 是，使用了a,b where a=3 and b=5 and c=4 是，使用了a,b,c where b=3 or where c=4 否 where a=3 and c=4 是，仅使用了a where a=3 and b&gt;10 and c=7 是，使用了a,b where a=3 and b like ‘%xx%’ and c=7 使用了a,b or的两边都有存在可用的索引，该语句才能用索引。 不要滥用索引，多余的索引会降低读写性能即使满足了上述原则，mysql还是可能会弃用索引，因为有些查询即使使用索引，也会出现大量的随机io，相对于从数据记录中的顺序io开销更大。 mysql 中能够使用索引的典型应用 测试库下载地址：https://downloads.mysql.com/d… 匹配全值（match the full value）对索引中所有列都指定具体值，即是对索引中的所有列都有等值匹配的条件。例如，租赁表 rental 中通过指定出租日期 rental_date + 库存编号 inventory_id + 客户编号 customer_id 的组合条件进行查询，从执行计划的 key he extra 两字段的值看到优化器选择了复合索引 idx_rental_date: 关于explain结果值及其含义可以参考我的另一篇博客MySql数据库中的索引 123456789101112131415MySQL [sakila]&gt; explain select * from rental where rental_date=&apos;2005-05-25 17:22:10&apos; and inventory_id=373 and customer_id=343 \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: rental partitions: NULL type: constpossible_keys: rental_date,idx_fk_inventory_id,idx_fk_customer_id key: rental_date key_len: 10 ref: const,const,const rows: 1 filtered: 100.00 Extra: NULL 1 row in set, 1 warning (0.00 sec) explain 输出结果中字段 type 的值为 const，表示是常量；字段 key 的值为 rental_date, 表示优化器选择索引 rental_date 进行扫描。 匹配值的范围查询（match a range of values）对索引的值能够进行范围查找。例如，检索租赁表 rental 中客户编号 customer_id 在指定范围内的记录： 123456789101112131415MySQL [sakila]&gt; explain select * from rental where customer_id &gt;= 373 and customer_id &lt; 400 \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: rental partitions: NULL type: rangepossible_keys: idx_fk_customer_id key: idx_fk_customer_id key_len: 2 ref: NULL rows: 718 filtered: 100.00 Extra: Using index condition 1 row in set, 1 warning (0.05 sec) 类型 type 为 range 说明优化器选择范围查询，索引 key 为 idx_fk_customer_id 说明优化器选择索引 idx_fk_customer_id 来加速访问，注意到这个列子中 extra 列为 using index codition ,表示 mysql 使用了 ICP（using index condition） 来进一步优化查询。 匹配最左前缀（match a leftmost prefix）仅仅使用索引中的最左边列进行查询，比如在 col1 + col2 + col3 字段上的联合索引能够被包含 col1、(col1 + col2)、（col1 + col2 + col3）的等值查询利用到，可是不能够被 col2、（col2、col3）的等值查询利用到。最左匹配原则可以算是 MySQL 中 B-Tree 索引使用的首要原则。 仅仅对索引进行查询（index only query）当查询的列都在索引的字段中时，查询的效率更高，所以应该尽量避免使用 select *，需要哪些字段，就只查哪些字段。 匹配列前缀（match a column prefix）仅仅使用索引中的第一列，并且只包含索引第一列的开头一部分进行查找。例如，现在需要查询出标题 title 是以 AFRICAN 开头的电影信息，从执行计划能够清楚看到，idx_title_desc_part 索引被利用上了： 12345678910111213141516171819MySQL [sakila]&gt; create index idx_title_desc_part on film_text(title (10), description(20));Query OK, 0 rows affected (0.07 sec)Records: 0 Duplicates: 0 Warnings: 0MySQL [sakila]&gt; explain select title from film_text where title like &apos;AFRICAN%&apos;\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: film_text partitions: NULL type: rangepossible_keys: idx_title_desc_part,idx_title_description key: idx_title_desc_part key_len: 32 ref: NULL rows: 1 filtered: 100.00 Extra: Using where 1 row in set, 1 warning (0.00 sec) extra 值为 using where 表示优化器需要通过索引回表查询数据。 能够实现索引匹配部分精确而其他部分进行范围匹配（match one part exactly and match a range on another part）例如，需要查询出租日期 rental_date 为指定日期且客户编号 customer_id 为指定范围的库存： 123456789101112131415MySQL [sakila]&gt; MySQL [sakila]&gt; explain select inventory_id from rental where rental_date=&apos;2006-02-14 15:16:03&apos; and customer_id &gt;= 300 and customer_id &lt;=400\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: rental partitions: NULL type: refpossible_keys: rental_date,idx_fk_customer_id key: rental_date key_len: 5 ref: const rows: 182 filtered: 16.85 Extra: Using where; Using index 1 row in set, 1 warning (0.00 sec) 如果列名是索引，那么使用 column_name is null 就会使用索引。例如，查询支付表 payment 的租赁编号 rental_id 字段为空的记录就用到了索引： 123456789101112131415MySQL [sakila]&gt; explain select * from payment where rental_id is null \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: payment partitions: NULL type: refpossible_keys: fk_payment_rental key: fk_payment_rental key_len: 5 ref: const rows: 5 filtered: 100.00 Extra: Using index condition 1 row in set, 1 warning (0.00 sec) 存在索引但不能使用索引的典型场景有些时候虽然有索引，但是并不被优化器选择使用，下面举例几个不能使用索引的场景。 以%开头的 like 查询不能利用 B-Tree 索引，执行计划中 key 的值为 null 表示没有使用索引123456789101112131415MySQL [sakila]&gt; explain select * from actor where last_name like &quot;%NI%&quot;\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: actor partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 200 filtered: 11.11 Extra: Using where 1 row in set, 1 warning (0.00 sec) 因为 B-Tree 索引的结构，所以以%开头的插叙很自然就没法利用索引了。一般推荐使用全文索引（Fulltext）来解决类似的全文检索的问题。或者考虑利用 innodb 的表都是聚簇表的特点，采取一种轻量级别的解决方式：一般情况下，索引都会比表小，扫描索引要比扫描表更快，而Innodb 表上二级索引 idx_last_name 实际上存储字段 last_name 还有主键 actot_id,那么理想的访问应该是首先扫描二级索引 idx_last_name 获得满足条件的last_name like &#39;%NI%&#39; 的主键 actor_id 列表，之后根据主键回表去检索记录，这样访问避开了全表扫描演员表 actor 产生的大量 IO 请求。 123456789101112131415161718192021222324252627ySQL [sakila]&gt; explain select * from (select actor_id from actor where last_name like &apos;%NI%&apos;) a , actor b where a.actor_id = b.actor_id \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: actor partitions: NULL type: indexpossible_keys: PRIMARY key: idx_actor_last_name key_len: 137 ref: NULL rows: 200 filtered: 11.11 Extra: Using where; Using index*************************** 2. row *************************** id: 1 select_type: SIMPLE table: b partitions: NULL type: eq_refpossible_keys: PRIMARY key: PRIMARY key_len: 2 ref: sakila.actor.actor_id rows: 1 filtered: 100.00 Extra: NULL 从执行计划中能够看出，extra 字段 using wehre；using index。理论上比全表扫描更快一下。 数据类型出现隐式转换的时候也不会使用索引当列的类型是字符串，那么一定记得在 where 条件中把字符常量值用引号引起来，否则即便这个列上有索引，mysql 也不会用到，因为 MySQL 默认把输入的常量值进行转换以后才进行检索。 例如，演员表 actor 中的姓氏字段 last_name 是字符型的，但是 sql 语句中的条件值 1 是一个数值型值，因此即便存在索引 idx_last_name, mysql 也不能正确的用上索引，而是继续进行全表扫描： 12345678910111213141516171819202122232425262728293031MySQL [sakila]&gt; explain select * from actor where last_name = 1 \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: actor partitions: NULL type: ALLpossible_keys: idx_actor_last_name key: NULL key_len: NULL ref: NULL rows: 200 filtered: 10.00 Extra: Using where 1 row in set, 3 warnings (0.00 sec)MySQL [sakila]&gt; explain select * from actor where last_name = &apos;1&apos;\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: actor partitions: NULL type: refpossible_keys: idx_actor_last_name key: idx_actor_last_name key_len: 137 ref: const rows: 1 filtered: 100.00 Extra: NULL 1 row in set, 1 warning (0.00 sec) 复合索引的情况下，假如查询条件不包含索引列最左边部分，即不满足最左原则 leftmost，是不会使用复合索引的。如果 MySQL 估计使用索引比全表扫描更慢，则不使用索引。用 or 分割开的条件，如果 or 前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都不会被用到。查看索引使用情况如果索引正在工作， Handler_read_key 的值将很高，这个值代表了一个行被索引值读的次数，很低的值表名增加索引得到的性能改善不高，因为索引并不经常使用。Handler_read_rnd_next 的值高则意味着查询运行低效，并且应该建立索引补救。这个值的含义是在数据文件中读下一行的请求数。如果正在进行大量的表扫描，Handler_read_rnd_next 的值较高，则通常说明表索引不正确或写入的查询没有利用索引，具体如下。 123456789101112MySQL [sakila]&gt; show status like &apos;Handler_read%&apos;;+-----------------------+-------+| Variable_name | Value |+-----------------------+-------+| Handler_read_first | 1 || Handler_read_key | 5 || Handler_read_last | 0 || Handler_read_next | 200 || Handler_read_prev | 0 || Handler_read_rnd | 0 || Handler_read_rnd_next | 0 |+-----------------------+-------+ 使用索引的小技巧字符串字段权衡区分度与长度的技巧截取不同长度，测试区分度 1234567891011# 这里假设截取6个字符长度计算区别度，直到区别度达到0.1，就可以把这个字段的这个长度作为索引了mysql&gt; select count(distinct left([varchar]],6))/count(*) from table;#注意：设置前缀索引时指定的长度表示字节数，而对于非二进制类型(CHAR, VARCHAR, TEXT)字段而言的字段长度表示字符数，所# 以，在设置前缀索引前需要把计算好的字符数转化为字节数，常用字符集与字节的关系如下：# latin 单字节：1B# GBK 双字节：2B# UTF8 三字节：3B# UTF8mb4 四字节：4B # myisam 表的索引大小默认为 1000字节，innodb 表的索引大小默认为 767 字节，可以在配置文件中修改 innodb_large_prefix # 项的值增大 innodb 索引的大小，最大 3072 字节。 区别度能达到0.1，就可以。 左前缀不易区分的字段索引建立方法这样的字段，左边有大量重复字符，比如url字段汇总的http:// 倒过来存储并建立索引 新增伪hash字段 把字符串转化为整型 索引覆盖概念：如果查询的列恰好是索引的一部分，那么查询只需要在索引文件上进行，不需要回行到磁盘，这种查询，速度极快，江湖人称——索引覆盖 延迟关联在根据条件查询数据时，如果查询条件不能用的索引，可以先查出数据行的id，再根据id去取数据行。eg. 1234//普通查询 没有用到索引select * from post where content like &quot;%新闻%&quot;;//延迟关联优化后 内层查询走content索引，取出id,在用join查所有行select a.* from post as a inner join (select id from post where content like &quot;%新闻%&quot;) as b on a.id=b.id; 索引排序 排序的字段上加入索引，可以提高速度。 重复索引和冗余索引重复索引：在同一列或者相同顺序的几个列建立了多个索引，成为重复索引，没有任何意义，删掉冗余索引：两个或多个索引所覆盖的列有重叠，比如对于列m,n ，加索引index m(m),indexmn(m,n),称为冗余索引。 索引碎片与维护在数据表长期的更改过程中，索引文件和数据文件都会产生空洞，形成碎片。修复表的过程十分耗费资源，可以用比较长的周期修复表。 1234//清理方法alert table xxx engine innodb; //或optimize table xxx; innodb引擎的索引注意事项Innodb 表要尽量自己指定主键，如果有几个列都是唯一的，要选择最常作为访问条件的列作为主键，另外，Innodb 表的普通索引都会保存主键的键值，所以主键要尽可能选择较短的数据类型，可以有效的减少索引的磁盘占用，提高索引的缓存效果。 参考：https://segmentfault.com/a/1190000009717352 https://mp.weixin.qq.com/s/KDIpY22tfmsrOIAuZNw4ZQ]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>索引</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud 自己的服务网关 Gateway]]></title>
    <url>%2F2018%2F12%2F16%2FSpring-Cloud-%E8%87%AA%E5%B7%B1%E7%9A%84%E6%9C%8D%E5%8A%A1%E7%BD%91%E5%85%B3-Gateway%2F</url>
    <content type="text"><![CDATA[Spring 官方最终还是按捺不住推出了自己的网关组件：Spring Cloud Gateway ，相比之前我们使用的 Zuul（1.x） 它有哪些优势呢？Zuul（1.x） 基于 Servlet，使用阻塞 API，它不支持任何长连接，如 WebSockets，Spring Cloud Gateway 使用非阻塞 API，支持 WebSockets，支持限流等新特性 Spring Cloud GatewaySpring Cloud Gateway 是 Spring Cloud 的一个全新项目，该项目是基于 Spring 5.0，Spring Boot 2.0 和 Project Reactor 等技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的 API 路由管理方式。 Spring Cloud Gateway 作为 Spring Cloud 生态系统中的网关，目标是替代 Netflix Zuul，其不仅提供统一的路由方式，并且基于 Filter 链的方式提供了网关基本的功能，例如：安全，监控/指标，和限流。 相关概念: Route（路由）：这是网关的基本构建块。它由一个 ID，一个目标 URI，一组断言和一组过滤器定义。如果断言为真，则路由匹配。 Predicate（断言）：这是一个 Java 8 的 Predicate。输入类型是一个 ServerWebExchange。我们可以使用它来匹配来自 HTTP 请求的任何内容，例如 headers 或参数。 Filter（过滤器）：这是org.springframework.cloud.gateway.filter.GatewayFilter的实例，我们可以使用它修改请求和响应。 工作流程： 客户端向 Spring Cloud Gateway 发出请求。如果 Gateway Handler Mapping 中找到与请求相匹配的路由，将其发送到 Gateway Web Handler。Handler 再通过指定的过滤器链来将请求发送到我们实际的服务执行业务逻辑，然后返回。过滤器之间用虚线分开是因为过滤器可能会在发送代理请求之前（“pre”）或之后（“post”）执行业务逻辑。 Spring Cloud Gateway 的特征： 基于 Spring Framework 5，Project Reactor 和 Spring Boot 2.0 动态路由 Predicates 和 Filters 作用于特定路由 集成 Hystrix 断路器 集成 Spring Cloud DiscoveryClient 易于编写的 Predicates 和 Filters 限流 路径重写 配置Spring Cloud Gateway 网关路由有两种配置方式： 在配置文件 yml 中配置 通过@Bean自定义 RouteLocator，在启动主类 Application 中配置 这两种方式是等价的，建议使用 yml 方式进配置。 使用 Spring Cloud Finchley 版本，Finchley 版本依赖于 Spring Boot 2.0.6.RELEASE。 123456789101112131415161718&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.6.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;&lt;/parent&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Finchley.SR2&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 经测试 Finchley.RELEASE 有 bug 多次请求会报空指针异常，SR2 是 Spring Cloud 的最新版本。 添加项目需要使用的依赖包 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;&lt;/dependency&gt; Spring Cloud Gateway 是使用 netty+webflux 实现因此不需要再引入 web 模块。 我们先来测试一个最简单的请求转发。 12345678910server: port: 8080spring: cloud: gateway: routes: - id: neo_route uri: http://www.ityouknow.com predicates: - Path=/spring-cloud 各字段含义如下： id：我们自定义的路由 ID，保持唯一 uri：目标服务地址 predicates：路由条件，Predicate 接受一个输入参数，返回一个布尔值结果。该接口包含多种默认方法来将 Predicate 组合成其他复杂的逻辑（比如：与，或，非）。 filters：过滤规则，本示例暂时没用。 转发功能同样可以通过代码来实现，我们可以在启动类 GateWayApplication 中添加方法 customRouteLocator() 来定制转发规则。 12345678910111213141516@SpringBootApplicationpublic class GateWayApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(GateWayApplication.class, args); &#125; @Bean public RouteLocator customRouteLocator(RouteLocatorBuilder builder) &#123; return builder.routes() .route("path_route", r -&gt; r.path("/about") .uri("http://ityouknow.com")) .build(); &#125;&#125; 上面配置了一个 id 为 path_route 的路由，当访问地址http://localhost:8080/about时会自动转发到地址：http://www.ityouknow.com/about和上面的转发效果一样，只是这里转发的是以项目地址/about格式的请求地址。 上面两个示例中 uri 都是指向了我的个人网站，在实际项目使用中可以将 uri 指向对外提供服务的项目地址，统一对外输出接口。 以上便是 Spring Cloud Gateway 最简单的两个请求示例，Spring Cloud Gateway 还有更多实用的功能接下来我们一一介绍。 路由规则Spring Cloud Gateway 的功能很强大，我们仅仅通过 Predicates 的设计就可以看出来，前面我们只是使用了 predicates 进行了简单的条件匹配，其实 Spring Cloud Gataway 帮我们内置了很多 Predicates 功能。 Spring Cloud Gateway 是通过 Spring WebFlux 的 HandlerMapping 做为底层支持来匹配到转发路由，Spring Cloud Gateway 内置了很多 Predicates 工厂，这些 Predicates 工厂通过不同的 HTTP 请求参数来匹配，多个 Predicates 工厂可以组合使用。 PredicatePredicate 来源于 Java 8，是 Java 8 中引入的一个函数，Predicate 接受一个输入参数，返回一个布尔值结果。该接口包含多种默认方法来将 Predicate 组合成其他复杂的逻辑（比如：与，或，非）。可以用于接口请求参数校验、判断新老数据是否有变化需要进行更新操作。and–与、or–或、negate–非 在 Spring Cloud Gateway 中 Spring 利用 Predicate 的特性实现了各种路由匹配规则，有通过 Header、请求参数等不同的条件来进行作为条件匹配到对应的路由。网上有一张图总结了 Spring Cloud 内置的几种 Predicate 的实现。 说白了 Predicate 就是为了实现一组匹配规则，方便让请求过来找到对应的 Route 进行处理，接下来我们介绍Spring Cloud GateWay 内置几种 Predicate 的使用。 通过时间匹配Predicate 支持设置一个时间，在请求进行转发的时候，可以通过判断在这个时间之前或者之后进行转发。比如我们现在设置只有在2019年1月1日才会转发到我的网站，在这之前不进行转发，我就可以这样配置： 12345678spring: cloud: gateway: routes: - id: time_route uri: http://ityouknow.com predicates: - After=2018-01-20T06:06:06+08:00[Asia/Shanghai] Spring 是通过 ZonedDateTime 来对时间进行的对比，ZonedDateTime 是 Java 8 中日期时间功能里，用于表示带时区的日期与时间信息的类，ZonedDateTime 支持通过时区来设置时间，中国的时区是：Asia/Shanghai。 After Route Predicate 是指在这个时间之后的请求都转发到目标地址。上面的示例是指，请求时间在 2018年1月20日6点6分6秒之后的所有请求都转发到地址http://ityouknow.com。+08:00是指时间和UTC时间相差八个小时，时间地区为Asia/Shanghai。 添加完路由规则之后，访问地址http://localhost:8080会自动转发到http://ityouknow.com。 Before Route Predicate 刚好相反，在某个时间之前的请求的请求都进行转发。我们把上面路由规则中的 After 改为 Before，如下： 12345678spring: cloud: gateway: routes: - id: after_route uri: http://ityouknow.com predicates: - Before=2018-01-20T06:06:06+08:00[Asia/Shanghai] 就表示在这个时间之前可以进行路由，在这时间之后停止路由，修改完之后重启项目再次访问地址http://localhost:8080，页面会报 404 没有找到地址。 除过在时间之前或者之后外，Gateway 还支持限制路由请求在某一个时间段范围内，可以使用 Between Route Predicate 来实现。 12345678spring: cloud: gateway: routes: - id: after_route uri: http://ityouknow.com predicates: - Between=2018-01-20T06:06:06+08:00[Asia/Shanghai], 2019-01-20T06:06:06+08:00[Asia/Shanghai] 这样设置就意味着在这个时间段内可以匹配到此路由，超过这个时间段范围则不会进行匹配。通过时间匹配路由的功能很酷，可以用在限时抢购的一些场景中。 通过 Cookie 匹配Cookie Route Predicate 可以接收两个参数，一个是 Cookie name ,一个是正则表达式，路由规则会通过获取对应的 Cookie name 值和正则表达式去匹配，如果匹配上就会执行路由，如果没有匹配上则不执行。 12345678spring: cloud: gateway: routes: - id: cookie_route uri: http://ityouknow.com predicates: - Cookie=ityouknow, kee.e 使用 curl 测试，命令行输入: 1curl http://localhost:8080 --cookie "ityouknow=kee.e" 则会返回页面代码，如果去掉--cookie &quot;ityouknow=kee.e&quot;，后台会报 404 错误。 通过 Header 属性匹配Header Route Predicate 和 Cookie Route Predicate 一样，也是接收 2 个参数，一个 header 中属性名称和一个正则表达式，这个属性值和正则表达式匹配则执行。 12345678spring: cloud: gateway: routes: - id: header_route uri: http://ityouknow.com predicates: - Header=X-Request-Id, \d+ 使用 curl 测试，命令行输入: 1curl http://localhost:8080 -H "X-Request-Id:666666" 则返回页面代码证明匹配成功。将参数-H &quot;X-Request-Id:666666&quot;改为-H &quot;X-Request-Id:neo&quot;再次执行时返回404证明没有匹配。 通过 Host 匹配Host Route Predicate 接收一组参数，一组匹配的域名列表，这个模板是一个 ant 分隔的模板，用.号作为分隔符。它通过参数中的主机地址作为匹配规则。 12345678spring: cloud: gateway: routes: - id: host_route uri: http://ityouknow.com predicates: - Host=**.ityouknow.com 使用 curl 测试，命令行输入: 12curl http://localhost:8080 -H "Host: www.ityouknow.com" curl http://localhost:8080 -H "Host: md.ityouknow.com" 经测试以上两种 host 均可匹配到 host_route 路由，去掉 host 参数则会报 404 错误。 通过请求方式匹配可以通过是 POST、GET、PUT、DELETE 等不同的请求方式来进行路由。 12345678spring: cloud: gateway: routes: - id: method_route uri: http://ityouknow.com predicates: - Method=GET 使用 curl 测试，命令行输入: 12# curl 默认是以 GET 的方式去请求curl http://localhost:8080 测试返回页面代码，证明匹配到路由，我们再以 POST 的方式请求测试。 12# curl 默认是以 GET 的方式去请求curl -X POST http://localhost:8080 返回 404 没有找到，证明没有匹配上路由 通过请求路径匹配Path Route Predicate 接收一个匹配路径的参数来判断是否走路由。 12345678spring: cloud: gateway: routes: - id: host_route uri: http://ityouknow.com predicates: - Path=/foo/&#123;segment&#125; 如果请求路径符合要求，则此路由将匹配，例如：/foo/1 或者 /foo/bar。 使用 curl 测试，命令行输入: 123curl http://localhost:8080/foo/1curl http://localhost:8080/foo/xxcurl http://localhost:8080/boo/xx 经过测试第一和第二条命令可以正常获取到页面返回值，最后一个命令报404，证明路由是通过指定路由来匹配。 通过请求参数匹配Query Route Predicate 支持传入两个参数，一个是属性名一个为属性值，属性值可以是正则表达式。 12345678spring: cloud: gateway: routes: - id: query_route uri: http://ityouknow.com predicates: - Query=smile 这样配置，只要请求中包含 smile 属性的参数即可匹配路由。 使用 curl 测试，命令行输入: 1curl localhost:8080?smile=x&amp;id=2 经过测试发现只要请求汇总带有 smile 参数即会匹配路由，不带 smile 参数则不会匹配。 还可以将 Query 的值以键值对的方式进行配置，这样在请求过来时会对属性值和正则进行匹配，匹配上才会走路由。 12345678spring: cloud: gateway: routes: - id: query_route uri: http://ityouknow.com predicates: - Query=keep, pu. 这样只要当请求中包含 keep 属性并且参数值是以 pu 开头的长度为三位的字符串才会进行匹配和路由。 使用 curl 测试，命令行输入: 1curl localhost:8080?keep=pub 测试可以返回页面代码，将 keep 的属性值改为 pubx 再次访问就会报 404,证明路由需要匹配正则表达式才会进行路由。 通过请求 ip 地址进行匹配Predicate 也支持通过设置某个 ip 区间号段的请求才会路由，RemoteAddr Route Predicate 接受 cidr 符号(IPv4 或 IPv6 )字符串的列表(最小大小为1)，例如 192.168.0.1/16 (其中 192.168.0.1 是 IP 地址，16 是子网掩码)。 12345678spring: cloud: gateway: routes: - id: remoteaddr_route uri: http://ityouknow.com predicates: - RemoteAddr=192.168.1.1/24 可以将此地址设置为本机的 ip 地址进行测试。 1curl localhost:8080 果请求的远程地址是 192.168.1.10，则此路由将匹配。 组合使用上面为了演示各个 Predicate 的使用，我们是单个单个进行配置测试，其实可以将各种 Predicate 组合起来一起使用。 例如： 123456789101112131415spring: cloud: gateway: routes: - id: host_foo_path_headers_to_httpbin uri: http://ityouknow.com predicates: - Host=**.foo.org - Path=/headers - Method=GET - Header=X-Request-Id, \d+ - Query=foo, ba. - Query=baz - Cookie=chocolate, ch.p - After=2018-01-20T06:06:06+08:00[Asia/Shanghai] 各种 Predicates 同时存在于同一个路由时，请求必须同时满足所有的条件才被这个路由匹配。 一个请求满足多个路由的谓词条件时，请求只会被首个成功匹配的路由转发 总结通过今天的学习发现 Spring Cloud Gateway 使用非常的灵活，可以根据不同的情况来进行路由分发，在实际项目中可以自由组合使用。同时 Spring Cloud Gateway 还有更多很酷的功能，比如 Filter 、熔断和限流等。 参考本文转载自公众号：纯洁的微笑]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据库开发规范]]></title>
    <url>%2F2018%2F12%2F14%2FMySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BC%80%E5%8F%91%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[所有的数据库对象名称必须使用小写字母并用下划线分割（MySQL大小写敏感，名称要见名知意，最好不超过32字符） 所有的数据库对象名称禁止使用MySQL保留关键字（如 desc、range、match、delayed 等 ） 临时库表必须以tmp为前缀并以日期为后缀（tmp_） 备份库和库必须以bak为前缀并以日期为后缀(bak_) 所有存储相同数据的列名和列类型必须一致。（在多个表中的字段如user_id，它们类型必须一致） mysql5.5之前默认的存储的引擎是myisam，没有特殊要求，所有的表必须使用innodb（innodb好处支持失误，行级锁，高并发下性能更好，对多核，大内存，ssd等硬件支持更好） 数据库和表的字符集尽量统一使用utf8（字符集必须统一，避免由于字符集转换产生的乱码，汉字utf8下占3个字节） 所有表和字段都要添加注释COMMENT，从一开始就进行数据字典的维护 尽量控制单表数据量的大小在500w以内，超过500w可以使用历史数据归档，分库分表来实现（500万行并不是MySQL数据库的限制。过大对于修改表结构，备份，恢复都会有很大问题。MySQL没有对存储有限制，取决于存储设置和文件系统） 谨慎使用mysql分区表（分区表在物理上表现为多个文件，在逻辑上表现为一个表） 谨慎选择分区键，跨分区查询效率可能更低 建议使用物理分表的方式管理大数据 尽量做到冷热数据分离，减小表的宽度（mysql限制最多存储4096列，行数没有限制，但是每一行的字节总数不能超过65535。列限制好处：减少磁盘io，保证热数据的内存缓存命中率，避免读入无用的冷数据） 禁止在表中建立预留字段（无法确认存储的数据类型，对预留字段类型进行修改，会对表进行锁定） 禁止在数据中存储图片，文件二进制数据（使用文件服务器） 禁止在线上做数据库压力测试 禁止从开发环境，测试环境直接连生产环境数据库 限制每张表上的索引数量，建议单表索引不超过5个（索引会增加查询效率，但是会降低插入和更新的速度） 避免建立冗余索引和重复索引（冗余：index（a,b,c) index(a,b) index(a)） 禁止给表中的每一列都建立单独的索引 每个innodb表必须有一个主键，选择自增id（不能使用更新频繁的列作为主键，不适用UUID,MD5,HASH,字符串列作为主键） 区分度最高的列放在联合索引的最左侧 尽量把字段长度小的列放在联合索引的最左侧 尽量避免使用外键（禁止使用物理外键，建议使用逻辑外键） 优先选择符合存储需要的最小数据类型 优先使用无符号的整形来存储 优先选择存储最小的数据类型（varchar(N),N代表的是字符数，而不是字节数，N代表能存储多少个汉字） 避免使用Text或是Blob类型 避免使用ENUM数据类型（修改ENUM值需要使用ALTER语句，ENUM类型的ORDER BY操作效率低，需要额外操作，禁止使用书值作为ENUM的枚举值 尽量把所有的字段定义为NOT NULL（索引NULL需要额外的空间来保存，所以需要暂用更多的内存，进行比较和计算要对NULL值做特别的处理） 使用timestamp或datetime类型来存储时间 同财务相关的金额数据，采用decimal类型（不丢失精度，禁止使用 float 和 double） 避免使用双%号和like，搜索严禁左模糊或者全模糊（如果需要请用搜索引擎来解决。索引文件具有 B-Tree 的最左前缀匹配特性，如果左边的值未确定，那么无法使用此索） 建议使用预编译语句进行数据库操作 禁止跨库查询（为数据迁移和分库分表留出余地，降低耦合度，降低风险） 禁止select * 查询（消耗更多的cpu和io及网络带宽资源，无法使用覆盖索引） 禁止使用不含字段列表的insert语句（不允许insert into t values（‘a’，‘b’，‘c’）不允许） in 操作能避免则避免，若实在避免不了，需要仔细评估 in 后边的集合元素数量，控制在 1000 个之内 禁止使用order by rand（）进行随机排序 禁止where从句中对列进行函数转换和计算（例如：where date（createtime）=‘20160901’ 会无法使用createtime列上索引。改成 where createtime&gt;=’20160901’ and createtime &lt;’20160902’） 尽量使用 union all 代替 union 拆分复杂的大SQL为多个小SQL（ MySQL一个SQL只能使用一个CPU进行计算） 尽量避免使用子查询，可以把子查询优化为join操作（子查询的结果集无法使用索引，子查询会产生临时表操作，如果子查询数据量大会影响效率，消耗过多的CPU及IO资源） 超过100万行的批量写操作，要分批多次进行操作（大批量操作可能会造成严重的主从延迟，binlog日志为row格式会产生大量的日志，避免产生大事务操作） 对于大表使用pt—online-schema-change修改表结构（避免大表修改产生的主从延迟，避免在对表字段进行修改时进行锁表） 对于程序连接数据库账号，遵循权限最小原则 超过三个表禁止 join。（需要 join 的字段，数据类型必须绝对一致；多表关联查询时，保证被关联的字段需要有索引。即使双表 join 也要注意表索引、SQL 性能。） 在varchar字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度即可。 SQL 性能优化的目标：至少要达到 range 级别，要求是 ref 级别，如果可以是 consts最好 使用 IS NULL()来判断是否为 NULL 值。 尽量不要使用物理删除（即直接删除，如果要删除的话提前做好备份），而是使用逻辑删除，使用字段delete_flag做逻辑删除，类型为tinyint，0表示未删除，1表示已删除 如果有 order by 的场景，请注意利用索引的有序性。order by 最后的字段是组合,索引的一部分，并且放在索引组合顺序的最后，避免出现 file_sort 的情况，影响查询性能。 在代码中写分页查询逻辑时，若 count 为 0 应直接返回，避免执行后面的分页语句 参考《阿里巴巴Java开发手册》 《高性能可扩展MySQL数据库设计及架构优化》 https://juejin.im/post/5c15c2b3f265da6170070613]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式事务]]></title>
    <url>%2F2018%2F12%2F09%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[考虑支付重构的时候，自然想到原本属于一个本地事务中的处理，现在要跨应用了要怎么处理。拿充值订单举个栗子吧，假设：原本订单模块和账户模块是放在一起的，现在需要做服务拆分，拆分成订单服务，账户服务。原本收到充值回调后，可以将修改订单状态和增加金币放在一个mysql事务中完成的，但是呢，因为服务拆分了，就面临着需要协调2个服务才能完成这个事务。 所以就带出来，我们今天要分享和讨论的话题是：怎么解决分布式场景下数据一致性问题，暂且用分布式事务 来定义吧。 同样的问题还存在于其他的场景： 送礼： 121. 调用支付服务：先扣送礼用户的金币，然后给主播加相应的荔枝2. 确认第一步成功后，播放特效，发聊天室送礼评论等 充值成功消息： 完成充值订单 发送订单完成的kafka消息 在涉及支付交易等付费接口的时候，数据一致性的问题就显得尤为重要，因为都是钱啊 目前分布式事务是怎么解决的呢？问题肯定不是新问题，也就是目前已经有相应的解决方案了，那就看一下现在是怎么来解决这类问题的吧。 以购买基础商品成功后发送支付订单完成消息为例： 假设支付下单购买基础商品，此刻已经收到支付回调，订单已经处理成功了，这个时候kafka服务故障，消息发送失败；而这个时候处理订单的事务已经提交了，怎么保证订单完成的消息一定能发出去呢？ 解读一下这个流程： 绿色部分，表示流程正常运行的交互过程： 先往JobController中提交一个job（用于故障恢复） 提交成功后，开始处理订单逻辑 处理完订单逻辑之后，开始发送kafka消息 消息也发送成功后，删除第一步提交的job 黄色部分，表示流程出现了异常，数据可能存在不一致现象。这个时候就需要进行流程恢复 JobController任务控制器定时去redis查询延时任务列表（每个任务都有一个时间戳，按时间戳排序过滤） 将任务进行恢复（调用job注册时定义的处理方法） 任务执行成功，表示流程完成；否则下一个定时周期重试 问题： 基于redis存储恢复任务，可能存在数据丢失风险 架构体系中没有统一的分布式事务规范，可否将这层逻辑独立为分布式事务中间件 缺少事务执行策略管理，如：控制最大重试次数等 事务执行状态没有记录，追查需要去翻看日志 行业中有什么解决方案 说解决方案之前，我们先了解一下这些方案的理论依据，有助于帮助我们来理解和实践这些方案 理论依据（讨论的前提）本地事务、分布式事务如果说本地事务是解决单个数据源上的数据操作的一致性问题的话，那么分布式事务则是为了解决跨越多个数据源上数据操作的一致性问题。 强一致性、弱一致性、最终一致性从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。如果能容忍后续的部分或者全部访问不到，则是弱一致性。如果经过一段时间后要求能访问到更新后的数据，则是最终一致性 从服务端角度，如何尽快将更新后的数据分布到整个系统，降低达到最终一致性的时间窗口，是提高系统的可用度和用户体验非常重要的方面。对于分布式数据系统： N — 数据复制的份数 W — 更新数据时需要保证写完成的节点数 R — 读取数据的时候需要读取的节点数 如果W+R&gt;N，写的节点和读的节点重叠，则是强一致性。例如对于典型的一主一备同步复制的关系型数据库，N=2,W=2,R=1，则不管读的是主库还是备库的数据，都是一致的。 如果W+R&lt;=N，则是弱一致性。例如对于一主一备异步复制的关系型数据库，N=2,W=1,R=1，则如果读的是备库，就可能无法读取主库已经更新过的数据，所以是弱一致性。 CAP理论分布式环境下（数据分布）要任何时刻保证数据一致性是不可能的，只能采取妥协的方案来保证数据最终一致性。这个也就是著名的CAP定理。 需要明确的一点是，对于一个分布式系统而言，分区容错性是一个最基本的要求。因为 既然是一个分布式系统，那么分布式系统中的组件必然需要被部署到不同的节点，否则也就无所谓分布式系统了，因此必然出现子网络。而对于分布式系统而言，网 络问题又是一个必定会出现的异常情况，因此分区容错性也就成为了一个分布式系统必然需要面对和解决的问题。因此系统架构师往往需要把精力花在如何根据业务 特点在C（一致性）和A（可用性）之间寻求平衡。 BASE 理论BASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的缩写。BASE理论是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结， 是基于CAP定理逐步演化而来的。BASE理论的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。 BASE理论面向的是大型高可用可扩展的分布式系统，和传统的事物ACID特性是相反的，它完全不同于ACID的强一致性模型，而是通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。但同时，在实际的分布式场景中，不同业务单元和组件对数据一致性的要求是不同的，因此在具体的分布式系统架构设计过程中，ACID特性和BASE理论往往又会结合在一起。 柔性事务不同于ACID的刚性事务，在分布式场景下基于BASE理论，就出现了柔性事务的概念。要想通过柔性事务来达到最终的一致性，就需要依赖于一些特性，这些特性在具体的方案中不一定都要满足，因为不同的方案要求不一样；但是都不满足的话，是不可能做柔性事务的。 可见性(对外可查询) 在分布式事务执行过程中，如果某一个步骤执行出错，就需要明确的知道其他几个操作的处理情况，这就需要其他的服务都能够提供查询接口，保证可以通过查询来判断操作的处理情况。 为了保证操作的可查询，需要对于每一个服务的每一次调用都有一个全局唯一的标识，可以是业务单据号（如订单号）、也可以是系统分配的操作流水号（如支付记录流水号）。除此之外，操作的时间信息也要有完整的记录。 幂等操作 幂等性，其实是一个数学概念。幂等函数，或幂等方法，是指可以使用相同参数重复执行，并能获得相同结果的函数。 f(f(x)) = f(x) 在编程中一个幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。也就是说，同一个方法，使用同样的参数，调用多次产生的业务结果与调用一次产生的业务结果相同。 这一个要求其实也比较好理解，因为要保证数据的最终一致性，很多解决防范都会有很多重试的操作，如果一个方法不保证幂等，那么将无法被重试。 幂等操作的实现方式有多种，如在系统中缓存所有的请求与处理结果、检测到重复操作后，直接返回上一次的处理结果等。 业界方案两阶段提交（2PC）XA是X/Open CAE Specification (Distributed Transaction Processing)模型中定义的TM（Transaction Manager）与RM（Resource Manager）之间进行通信的接口。 在XA规范中，数据库充当RM角色，应用需要充当TM的角色，即生成全局的txId，调用XAResource接口，把多个本地事务协调为全局统一的分布式事务。 二阶段提交是XA的标准实现。它将分布式事务的提交拆分为2个阶段：prepare和commit/rollback。 2PC模型中，在prepare阶段需要等待所有参与子事务的反馈，因此可能造成数据库资源锁定时间过长，不适合并发高以及子事务生命周长较长的业务场景。两阶段提交这种解决方案属于牺牲了一部分可用性来换取的一致性。 sagasaga的提出，最早是为了解决可能会长时间运行的分布式事务（long-running process）的问题。所谓long-running的分布式事务，是指那些企业业务流程，需要跨应用、跨企业来完成某个事务，甚至在事务流程中还需要有手工操作的参与，这类事务的完成时间可能以分计，以小时计，甚至可能以天计。这类事务如果按照事务的ACID的要求去设计，势必造成系统的可用性大大的降低。试想一个由两台服务器一起参与的事务，服务器A发起事务，服务器B参与事务，B的事务需要人工参与，所以处理时间可能很长。如果按照ACID的原则，要保持事务的隔离性、一致性，服务器A中发起的事务中使用到的事务资源将会被锁定，不允许其他应用访问到事务过程中的中间结果，直到整个事务被提交或者回滚。这就造成事务A中的资源被长时间锁定，系统的可用性将不可接受。 而saga，则是一种基于补偿的消息驱动的用于解决long-running process的一种解决方案。目标是为了在确保系统高可用的前提下尽量确保数据的一致性。还是上面的例子，如果用saga来实现，那就是这样的流程：服务器A的事务先执行，如果执行顺利，那么事务A就先行提交；如果提交成功，那么就开始执行事务B，如果事务B也执行顺利，则事务B也提交，整个事务就算完成。但是如果事务B执行失败，那事务B本身需要回滚，这时因为事务A已经提交，所以需要执行一个补偿操作，将已经提交的事务A执行的操作作反操作，恢复到未执行前事务A的状态。这样的基于消息驱动的实现思路，就是saga。我们可以看出，saga是牺牲了数据的强一致性，仅仅实现了最终一致性，但是提高了系统整体的可用性。 补偿事务（TCC）TCC 其实就是采用的补偿机制，其核心思想是：针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作。TCC模型是把锁的粒度完全交给业务处理。它分为三个阶段： Try 阶段主要是对业务系统做检测及资源预留 Confirm 阶段主要是对业务系统做确认提交，Try阶段执行成功并开始执行 Confirm阶段时，默认 Confirm阶段是不会出错的。即：只要Try成功，Confirm一定成功。 Cancel 阶段主要是在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。 下面对TCC模式下，A账户往B账户汇款100元为例子，对业务的改造进行详细的分析： 汇款服务和收款服务分别需要实现，Try-Confirm-Cancel接口，并在业务初始化阶段将其注入到TCC事务管理器中。 1234567891011121314151617181920[汇款服务]Try： 检查A账户有效性，即查看A账户的状态是否为“转帐中”或者“冻结”； 检查A账户余额是否充足； 从A账户中扣减100元，并将状态置为“转账中”； 预留扣减资源，将从A往B账户转账100元这个事件存入消息或者日志中；Confirm： 不做任何操作；Cancel： A账户增加100元； 从日志或者消息中，释放扣减资源。[收款服务]Try： 检查B账户账户是否有效；Confirm： 读取日志或者消息，B账户增加100元； 从日志或者消息中，释放扣减资源；Cancel： 不做任何操作。 由此可以看出，TCC模型对业务的侵入强，改造的难度大。 本地消息表（异步确保）本地消息表这种实现方式应该是业界使用最多的，其核心思想是将分布式事务拆分成本地事务进行处理，这种思路是来源于ebay。我们可以从下面的流程图中看出其中的一些细节： 基本思路就是： 消息生产方，需要额外建一个消息表，并记录消息发送状态。消息表和业务数据要在一个事务里提交，也就是说他们要在一个数据库里面。然后消息会经过MQ发送到消息的消费方。如果消息发送失败，会进行重试发送。 消息消费方，需要处理这个消息，并完成自己的业务逻辑。此时如果本地事务处理成功，表明已经处理成功了，如果处理失败，那么就会重试执行。如果是业务上面的失败，可以给生产方发送一个业务补偿消息，通知生产方进行回滚等操作。 生产方和消费方定时扫描本地消息表，把还没处理完成的消息或者失败的消息再发送一遍。如果有靠谱的自动对账补账逻辑，这种方案还是非常实用的。 事务消息事务消息作为一种异步确保型事务， 将两个事务分支通过MQ进行异步解耦，事务消息的设计流程同样借鉴了两阶段提交理论，整体交互流程如下图所示： 事务发起方首先发送prepare消息到MQ。 在发送prepare消息成功后执行本地事务。 根据本地事务执行结果返回commit或者是rollback。 如果消息是rollback，MQ将删除该prepare消息不进行下发，如果是commit消息，MQ将会把这个消息发送给consumer端。 如果执行本地事务过程中，执行端挂掉，或者超时，MQ将会不停的询问其同组的其它producer来获取状态。 Consumer端的消费成功机制有MQ保证。 有一些第三方的MQ是支持事务消息的，比如RocketMQ，但是市面上一些主流的MQ都是不支持事务消息的，比如 RabbitMQ 和 Kafka 都不支持。 尽最大努力通知最大努力通知方案主要也是借助MQ消息系统来进行事务控制，这一点与可靠消息最终一致方案一样。看来MQ中间件确实在一个分布式系统架构中，扮演者重要的角色。最大努力通知方案是比较简单的分布式事务方案，它本质上就是通过定期校对，实现数据一致性。 最大努力通知方案的实现 业务活动的主动方，在完成业务处理之后，向业务活动的被动方发送消息，允许消息丢失。 主动方可以设置时间阶梯型通知规则，在通知失败后按规则重复通知，直到通知N次后不再通知。 主动方提供校对查询接口给被动方按需校对查询，用于恢复丢失的业务消息。 业务活动的被动方如果正常接收了数据，就正常返回响应，并结束事务。 如果被动方没有正常接收，根据定时策略，向业务活动主动方查询，恢复丢失的业务消息 最大努力通知方案的特点 用到的服务模式：可查询操作、幂等操作。 被动方的处理结果不影响主动方的处理结果； 适用于对业务最终一致性的时间敏感度低的系统； 适合跨企业的系统间的操作，或者企业内部比较独立的系统间的操作，比如银行通知、商户通知等； 方案比较 属性 2PC TCC 本地消息表 事务消息 尽最大努力通知 事务一致性 强 弱 弱 弱 弱 复杂性 中 高 低 低 低 业务侵入性 小 大 中 中 中 使用局限性 大 大 小 中 中 性能 低 中 高 高 高 维护成本 低 高 低 中 中 别人是怎么做的alipay的分布式事务服务DTS https://tech.antfin.com/docs/2/46887 分布式事务服务（Distributed Transaction Service，简称 DTS）是一个分布式事务框架，用来保障在大规模分布式环境下事务的最终一致性。DTS 从架构上分为 xts-client 和 xts-server 两部分，前者是一个嵌入客户端应用的 Jar 包，主要负责事务数据的写入和处理；后者是一个独立的系统，主要负责异常事务的恢复。 核心概念 在 DTS 内部，我们将一个分布式事务的关联方，分为发起方和参与者两类： 发起方： 分布式事务的发起方负责启动分布式事务，触发创建相应的主事务记录。发起方是分布式事务的协调者，负责调用参与者的服务，并记录相应的事务日志，感知整个分布式事务状态来决定整个事务是 COMMIT 还是 ROLLBACK。 参与者：参与者是分布式事务中的一个原子单位，所有参与者都必须在一阶段接口（Prepare）中标注（Annotation）参与者的标识，它定义了 prepare、commit、rollback 3个基本接口，业务系统需要实现这3个接口，并保证其业务数据的幂等性，也必须保证 prepare 中的数据操作能够被提交（COMMIT）或者回滚（ROLLBACK）。从存储结构上，DTS 的事务状态数据可以分为主事务记录（Activity）和分支事务记录（Action）两类： 主事务记录 Activity：主事务记录是整个分布式事务的主体，其最核心的数据结构是事务号（TX_ID）和事务状态（STATE），它是在启动分布式事务的时候持久化写入数据库的，它的状态决定了这笔分布式事务的状态。 分支事务记录 Action：分支事务记录是主事务记录的一个子集，它记录了一个参与者的信息，其中包括参与者的 NAME 名称，DTS 通过这个 NAME 来唯一定位一个参与者。通过这个分支事务信息，我们就可以对参与者进行提交或者回滚操作。 这应该属于我们上面所说的TCC模式。 eBay 本地消息表 https://weibo.com/ttarticle/p/show?id=2309403965965003062676 本地消息表这种实现方式的思路，其实是源于ebay，后来通过支付宝等公司的布道，在业内广泛使用。其基本的设计思想是将远程分布式事务拆分成一系列的本地事务。如果不考虑性能及设计优雅，借助关系型数据库中的表即可实现。 举个经典的跨行转账的例子来描述。 第一步，扣款1W，通过本地事务保证了凭证消息插入到消息表中。 第二步，通知对方银行账户上加1W了。那问题来了，如何通知到对方呢？ 通常采用两种方式： 采用时效性高的MQ，由对方订阅消息并监听，有消息时自动触发事件 采用定时轮询扫描的方式，去检查消息表的数据。 类似使用本地消息表+消息通知的还有去哪儿，蘑菇街 各种第三方支付回调最大努力通知型。如支付宝、微信的支付回调接口方式，不断回调直至成功，或直至调用次数衰减至失败状态。 我们可以怎么来做2PC/3PC需要资源管理器(mysql, redis)支持XA协议，且整个事务的执行期间需要锁住事务资源，会降低性能。故先排除。 TCC的模式，需要事务接口提供try,confirm,cancel三个接口，提高了编程的复杂性。需要依赖于业务方来配合提供这样的接口。推行难度大，暂时排除。 最大努力通知型，应用于异构或者服务平台当中 可以看到ebay的经典模式中，分布式的事务，是通过本地事务+可靠消息，来达到事务的最终一致性的。但是出现了事务消息，就把本地事务的工作给涵盖在事务消息当中了。所以，接下来要基于事务消息来套我们的应用场景，看起是否满足我们对分布式事务产品的要求。 参考https://mp.weixin.qq.com/s/LpebzrHU3YhQ1bkEvyUjGQ https://mp.weixin.qq.com/s/Kxk2Ag-7dbpBZEs1xBJYeQ https://blog.csdn.net/zsh2050/article/details/78034094 https://www.cnblogs.com/savorboard/p/distributed-system-transaction-consistency.html http://www.cnblogs.com/netfocus/p/3149156.html https://juejin.im/post/5baa54e1f265da0ac2566fb2]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>一致性</tag>
        <tag>数据库</tag>
        <tag>分布式</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5种消息队列的差异]]></title>
    <url>%2F2018%2F12%2F08%2F5%E7%A7%8D%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E5%B7%AE%E5%BC%82%2F</url>
    <content type="text"><![CDATA[本文将从17 个方面综合对比Kafka、RabbitMQ、ZeroMQ、RocketMQ、ActiveMQ作为消息队列使用时的差异。 资料文档Kafka：中。有kafka作者自己写的书，网上资料也有一些。 rabbitmq：多。有一些不错的书，网上资料多。 zeromq：少。没有专门写zeromq的书，网上的资料多是一些代码的实现和简单介绍。 rocketmq：少。没有专门写rocketmq的书，网上的资料良莠不齐，官方文档很简洁，但是对技术细节没有过多的描述。 activemq：多。没有专门写activemq的书，网上资料多。 开发语言Kafka：Scala rabbitmq：Erlang zeromq：c rocketmq：java activemq：java 支持的协议Kafka：自己定义的一套…（基于TCP） rabbitmq：AMQP zeromq：TCP、UDP rocketmq：自己定义的一套… activemq：OpenWire、STOMP、REST、XMPP、AMQP 消息存储Kafka：内存、磁盘、数据库。支持大量堆积。 kafka的最小存储单元是分区，一个topic包含多个分区，kafka创建主题时，这些分区会被分配在多个服务器上，通常一个broker一台服务器。 分区首领会均匀地分布在不同的服务器上，分区副本也会均匀的分布在不同的服务器上，确保负载均衡和高可用性，当新的broker加入集群的时候，部分副本会被移动到新的broker上。 根据配置文件中的目录清单，kafka会把新的分区分配给目录清单里分区数最少的目录。 默认情况下，分区器使用轮询算法把消息均衡地分布在同一个主题的不同分区中，对于发送时指定了key的情况，会根据key的hashcode取模后的值存到对应的分区中。 rabbitmq：内存、磁盘。支持少量堆积。 rabbitmq的消息分为持久化的消息和非持久化消息，不管是持久化的消息还是非持久化的消息都可以写入到磁盘。 持久化的消息在到达队列时就写入到磁盘，并且如果可以，持久化的消息也会在内存中保存一份备份，这样可以提高一定的性能，当内存吃紧的时候会从内存中清除。非持久化的消息一般只存在于内存中，在内存吃紧的时候会被换入到磁盘中，以节省内存。 引入镜像队列机制，可将重要队列“复制”到集群中的其他broker上，保证这些队列的消息不会丢失。配置镜像的队列，都包含一个主节点master和多个从节点slave,如果master失效，加入时间最长的slave会被提升为新的master，除发送消息外的所有动作都向master发送，然后由master将命令执行结果广播给各个slave，rabbitmq会让master均匀地分布在不同的服务器上，而同一个队列的slave也会均匀地分布在不同的服务器上，保证负载均衡和高可用性。 zeromq：消息发送端的内存或者磁盘中。不支持持久化。 rocketmq：磁盘。支持大量堆积。 commitLog文件存放实际的消息数据，每个commitLog上限是1G，满了之后会自动新建一个commitLog文件保存数据。ConsumeQueue队列只存放offset、size、tagcode，非常小，分布在多个broker上。ConsumeQueue相当于CommitLog的索引文件，消费者消费时会从consumeQueue中查找消息在commitLog中的offset，再去commitLog中查找元数据。 ConsumeQueue存储格式的特性，保证了写过程的顺序写盘（写CommitLog文件），大量数据IO都在顺序写同一个commitLog，满1G了再写新的。加上rocketmq是累计4K才强制从PageCache中刷到磁盘（缓存），所以高并发写性能突出。 activemq：内存、磁盘、数据库。支持少量堆积。 消息事务Kafka：支持 rabbitmq：支持。 ​ 客户端将信道设置为事务模式，只有当消息被rabbitMq接收，事务才能提交成功，否则在捕获异常后进行回滚。使用事务会使得性能有所下降。 zeromq：不支持 rocketmq：支持 activemq：支持 负载均衡Kafka：支持负载均衡。 1&gt;一个broker通常就是一台服务器节点。对于同一个Topic的不同分区，Kafka会尽力将这些分区分布到不同的Broker服务器上，zookeeper保存了broker、主题和分区的元数据信息。分区首领会处理来自客户端的生产请求，kafka分区首领会被分配到不同的broker服务器上，让不同的broker服务器共同分担任务。 每一个broker都缓存了元数据信息，客户端可以从任意一个broker获取元数据信息并缓存起来，根据元数据信息知道要往哪里发送请求。 2&gt;kafka的消费者组订阅同一个topic，会尽可能地使得每一个消费者分配到相同数量的分区，分摊负载。 3&gt;当消费者加入或者退出消费者组的时候，还会触发再均衡，为每一个消费者重新分配分区，分摊负载。 kafka的负载均衡大部分是自动完成的，分区的创建也是kafka完成的，隐藏了很多细节，避免了繁琐的配置和人为疏忽造成的负载问题。 4&gt;发送端由topic和key来决定消息发往哪个分区，如果key为null，那么会使用轮询算法将消息均衡地发送到同一个topic的不同分区中。如果key不为null，那么会根据key的hashcode取模计算出要发往的分区。 rabbitmq：对负载均衡的支持不好。 1&gt;消息被投递到哪个队列是由交换器和key决定的，交换器、路由键、队列都需要手动创建。 rabbitmq客户端发送消息要和broker建立连接，需要事先知道broker上有哪些交换器，有哪些队列。通常要声明要发送的目标队列，如果没有目标队列，会在broker上创建一个队列，如果有，就什么都不处理，接着往这个队列发送消息。假设大部分繁重任务的队列都创建在同一个broker上，那么这个broker的负载就会过大。（可以在上线前预先创建队列，无需声明要发送的队列，但是发送时不会尝试创建队列，可能出现找不到队列的问题，rabbitmq的备份交换器会把找不到队列的消息保存到一个专门的队列中，以便以后查询使用） 使用镜像队列机制建立rabbitmq集群可以解决这个问题，形成master-slave的架构，master节点会均匀分布在不同的服务器上，让每一台服务器分摊负载。slave节点只是负责转发，在master失效时会选择加入时间最长的slave成为master。 当新节点加入镜像队列的时候，队列中的消息不会同步到新的slave中，除非调用同步命令，但是调用命令后，队列会阻塞，不能在生产环境中调用同步命令。 2&gt;当rabbitmq队列拥有多个消费者的时候，队列收到的消息将以轮询的分发方式发送给消费者。每条消息只会发送给订阅列表里的一个消费者，不会重复。 这种方式非常适合扩展，而且是专门为并发程序设计的。 如果某些消费者的任务比较繁重，那么可以设置basicQos限制信道上消费者能保持的最大未确认消息的数量，在达到上限时，rabbitmq不再向这个消费者发送任何消息。 3&gt;对于rabbitmq而言，客户端与集群建立的TCP连接不是与集群中所有的节点建立连接，而是挑选其中一个节点建立连接。 但是rabbitmq集群可以借助HAProxy、LVS技术，或者在客户端使用算法实现负载均衡，引入负载均衡之后，各个客户端的连接可以分摊到集群的各个节点之中。 客户端均衡算法： 1)轮询法。按顺序返回下一个服务器的连接地址。 2)加权轮询法。给配置高、负载低的机器配置更高的权重，让其处理更多的请求；而配置低、负载高的机器，给其分配较低的权重，降低其系统负载。 3)随机法。随机选取一个服务器的连接地址。 4)加权随机法。按照概率随机选取连接地址。 5)源地址哈希法。通过哈希函数计算得到的一个数值，用该数值对服务器列表的大小进行取模运算。 6)最小连接数法。动态选择当前连接数最少的一台服务器的连接地址。 zeromq：去中心化，不支持负载均衡。本身只是一个多线程网络库。 rocketmq：支持负载均衡。 一个broker通常是一个服务器节点，broker分为master和slave,master和slave存储的数据一样，slave从master同步数据。 1&gt;nameserver与每个集群成员保持心跳，保存着Topic-Broker路由信息，同一个topic的队列会分布在不同的服务器上。 2&gt;发送消息通过轮询队列的方式发送，每个队列接收平均的消息量。发送消息指定topic、tags、keys，无法指定投递到哪个队列（没有意义，集群消费和广播消费跟消息存放在哪个队列没有关系）。 tags选填，类似于 Gmail 为每封邮件设置的标签，方便服务器过滤使用。目前只支 持每个消息设置一个 tag，所以也可以类比为 Notify 的 MessageType 概念。 keys选填，代表这条消息的业务关键词，服务器会根据 keys 创建哈希索引，设置后， 可以在 Console 系统根据 Topic、Keys 来查询消息，由于是哈希索引，请尽可能 保证 key 唯一，例如订单号，商品 Id 等。 3&gt;rocketmq的负载均衡策略规定：Consumer数量应该小于等于Queue数量，如果Consumer超过Queue数量，那么多余的Consumer 将不能消费消息。这一点和kafka是一致的，rocketmq会尽可能地为每一个Consumer分配相同数量的队列，分摊负载。 activemq：支持负载均衡。可以基于zookeeper实现负载均衡。 集群方式Kafka：天然的‘Leader-Slave’无状态集群，每台服务器既是Master也是Slave。 分区首领均匀地分布在不同的kafka服务器上，分区副本也均匀地分布在不同的kafka服务器上，所以每一台kafka服务器既含有分区首领，同时又含有分区副本，每一台kafka服务器是某一台kafka服务器的Slave，同时也是某一台kafka服务器的leader。 kafka的集群依赖于zookeeper，zookeeper支持热扩展，所有的broker、消费者、分区都可以动态加入移除，而无需关闭服务，与不依靠zookeeper集群的mq相比，这是最大的优势。 rabbitmq：支持简单集群，’复制’模式，对高级集群模式支持不好。 rabbitmq的每一个节点，不管是单一节点系统或者是集群中的一部分，要么是内存节点，要么是磁盘节点，集群中至少要有一个是磁盘节点。 在rabbitmq集群中创建队列，集群只会在单个节点创建队列进程和完整的队列信息（元数据、状态、内容），而不是在所有节点上创建。 引入镜像队列，可以避免单点故障，确保服务的可用性，但是需要人为地为某些重要的队列配置镜像。 zeromq：去中心化，不支持集群。 rocketmq：常用 多对’Master-Slave’ 模式，开源版本需手动切换Slave变成Master Name Server是一个几乎无状态节点，可集群部署，节点之间无任何信息同步。 Broker部署相对复杂，Broker分为Master与Slave，一个Master可以对应多个Slave，但是一个Slave只能对应一个Master，Master与Slave的对应关系通过指定相同的BrokerName，不同的BrokerId来定义，BrokerId为0表示Master，非0表示Slave。Master也可以部署多个。每个Broker与Name Server集群中的所有节点建立长连接，定时注册Topic信息到所有Name Server。 Producer与Name Server集群中的其中一个节点（随机选择）建立长连接，定期从Name Server取Topic路由信息，并向提供Topic服务的Master建立长连接，且定时向Master发送心跳。Producer完全无状态，可集群部署。 Consumer与Name Server集群中的其中一个节点（随机选择）建立长连接，定期从Name Server取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳。Consumer既可以从Master订阅消息，也可以从Slave订阅消息，订阅规则由Broker配置决定。 客户端先找到NameServer, 然后通过NameServer再找到 Broker。 一个topic有多个队列，这些队列会均匀地分布在不同的broker服务器上。rocketmq队列的概念和kafka的分区概念是基本一致的，kafka同一个topic的分区尽可能地分布在不同的broker上，分区副本也会分布在不同的broker上。 rocketmq集群的slave会从master拉取数据备份，master分布在不同的broker上。 activemq：支持简单集群模式，比如’主-备’，对高级集群模式支持不好。 管理界面Kafka：一般 rabbitmq：好 zeromq：无 rocketmq：无 activemq：一般 可用性Kafka：非常高（分布式） rabbitmq：高（主从） zeromq：高。 rocketmq：非常高（分布式） activemq：高（主从） 消息重复Kafka：支持at least once、at most once rabbitmq：支持at least once、at most once zeromq：只有重传机制，但是没有持久化，消息丢了重传也没有用。既不是at least once、也不是at most once、更不是exactly only once rocketmq：支持at least once activemq：支持at least once 吞吐量TPSKafka：极大 Kafka按批次发送消息和消费消息。发送端将多个小消息合并，批量发向Broker，消费端每次取出一个批次的消息批量处理。 rabbitmq：比较大 zeromq：极大 rocketmq：大 rocketMQ接收端可以批量消费消息，可以配置每次消费的消息数，但是发送端不是批量发送。 activemq：比较大 订阅形式和消息分发Kafka：基于topic以及按照topic进行正则匹配的发布订阅模式。 【发送】 发送端由topic和key来决定消息发往哪个分区，如果key为null，那么会使用轮询算法将消息均衡地发送到同一个topic的不同分区中。如果key不为null，那么会根据key的hashcode取模计算出要发往的分区。 【接收】 1&gt;consumer向群组协调器broker发送心跳来维持他们和群组的从属关系以及他们对分区的所有权关系，所有权关系一旦被分配就不会改变除非发生再均衡(比如有一个consumer加入或者离开consumer group)，consumer只会从对应的分区读取消息。 2&gt;kafka限制consumer个数要少于分区个数,每个消息只会被同一个 Consumer Group的一个consumer消费（非广播）。 3&gt;kafka的 Consumer Group订阅同一个topic，会尽可能地使得每一个consumer分配到相同数量的分区，不同 Consumer Group订阅同一个主题相互独立，同一个消息会被不同的 Consumer Group处理。 rabbitmq：提供了4种：direct, topic ,Headers和fanout。 【发送】 先要声明一个队列，这个队列会被创建或者已经被创建，队列是基本存储单元。 由exchange和key决定消息存储在哪个队列。 direct&gt;发送到和bindingKey完全匹配的队列。 topic&gt;路由key是含有”.”的字符串，会发送到含有“*”、“#”进行模糊匹配的bingKey对应的队列。 fanout&gt;与key无关，会发送到所有和exchange绑定的队列 headers&gt;与key无关，消息内容的headers属性（一个键值对）和绑定键值对完全匹配时，会发送到此队列。此方式性能低一般不用 【接收】 rabbitmq的队列是基本存储单元，不再被分区或者分片，对于我们已经创建了的队列，消费端要指定从哪一个队列接收消息。 当rabbitmq队列拥有多个消费者的时候，队列收到的消息将以轮询的分发方式发送给消费者。每条消息只会发送给订阅列表里的一个消费者，不会重复。 这种方式非常适合扩展，而且是专门为并发程序设计的。 如果某些消费者的任务比较繁重，那么可以设置basicQos限制信道上消费者能保持的最大未确认消息的数量，在达到上限时，rabbitmq不再向这个消费者发送任何消息。 zeromq：点对点(p2p) rocketmq：基于topic/messageTag以及按照消息类型、属性进行正则匹配的发布订阅模式 【发送】 发送消息通过轮询队列的方式发送，每个队列接收平均的消息量。发送消息指定topic、tags、keys，无法指定投递到哪个队列（没有意义，集群消费和广播消费跟消息存放在哪个队列没有关系）。 tags选填，类似于 Gmail 为每封邮件设置的标签，方便服务器过滤使用。目前只支 持每个消息设置一个 tag，所以也可以类比为 Notify 的 MessageType 概念。 keys选填，代表这条消息的业务关键词，服务器会根据 keys 创建哈希索引，设置后， 可以在 Console 系统根据 Topic、Keys 来查询消息，由于是哈希索引，请尽可能 保证 key 唯一，例如订单号，商品 Id 等。 【接收】 1&gt;广播消费。一条消息被多个Consumer消费，即使Consumer属于同一个ConsumerGroup，消息也会被ConsumerGroup中的每个Consumer都消费一次。 2&gt;集群消费。一个 Consumer Group中的Consumer实例平均分摊消费消息。例如某个Topic有 9 条消息，其中一个Consumer Group有3个实例，那么每个实例只消费其中的 3 条消息。即每一个队列都把消息轮流分发给每个consumer。 activemq：点对点(p2p)、广播（发布-订阅） 点对点模式，每个消息只有1个消费者； 发布/订阅模式，每个消息可以有多个消费者。 【发送】 点对点模式：先要指定一个队列，这个队列会被创建或者已经被创建。 发布/订阅模式：先要指定一个topic，这个topic会被创建或者已经被创建。 【接收】 点对点模式：对于已经创建了的队列，消费端要指定从哪一个队列接收消息。 发布/订阅模式：对于已经创建了的topic，消费端要指定订阅哪一个topic的消息。 顺序消息Kafka：支持。 设置生产者的max.in.flight.requests.per.connection为1，可以保证消息是按照发送顺序写入服务器的，即使发生了重试。 kafka保证同一个分区里的消息是有序的，但是这种有序分两种情况 1&gt;key为null，消息逐个被写入不同主机的分区中，但是对于每个分区依然是有序的 2&gt;key不为null , 消息被写入到同一个分区，这个分区的消息都是有序。 rabbitmq：不支持 zeromq：不支持 rocketmq：支持 activemq：不支持 消息确认Kafka：支持。 1&gt;发送方确认机制 ack=0，不管消息是否成功写入分区 ack=1，消息成功写入首领分区后，返回成功 ack=all，消息成功写入所有分区后，返回成功。 2&gt;接收方确认机制 自动或者手动提交分区偏移量，早期版本的kafka偏移量是提交给Zookeeper的，这样使得zookeeper的压力比较大，更新版本的kafka的偏移量是提交给kafka服务器的，不再依赖于zookeeper群组，集群的性能更加稳定。 rabbitmq：支持。 1&gt;发送方确认机制，消息被投递到所有匹配的队列后，返回成功。如果消息和队列是可持久化的，那么在写入磁盘后，返回成功。支持批量确认和异步确认。 2&gt;接收方确认机制，设置autoAck为false，需要显式确认，设置autoAck为true，自动确认。 当autoAck为false的时候，rabbitmq队列会分成两部分，一部分是等待投递给consumer的消息，一部分是已经投递但是没收到确认的消息。如果一直没有收到确认信号，并且consumer已经断开连接，rabbitmq会安排这个消息重新进入队列，投递给原来的消费者或者下一个消费者。 未确认的消息不会有过期时间，如果一直没有确认，并且没有断开连接，rabbitmq会一直等待，rabbitmq允许一条消息处理的时间可以很久很久。 zeromq：支持。 rocketmq：支持。 activemq：支持。 消息回溯Kafka：支持指定分区offset位置的回溯。 rabbitmq：不支持 zeromq：不支持 rocketmq：支持指定时间点的回溯。 activemq：不支持 消息重试Kafka：不支持，但是可以实现。 kafka支持指定分区offset位置的回溯，可以实现消息重试。 rabbitmq：不支持，但是可以利用消息确认机制实现。 rabbitmq接收方确认机制，设置autoAck为false。 当autoAck为false的时候，rabbitmq队列会分成两部分，一部分是等待投递给consumer的消息，一部分是已经投递但是没收到确认的消息。如果一直没有收到确认信号，并且consumer已经断开连接，rabbitmq会安排这个消息重新进入队列，投递给原来的消费者或者下一个消费者。 zeromq：不支持， rocketmq：支持。 消息消费失败的大部分场景下，立即重试99%都会失败，所以rocketmq的策略是在消费失败时定时重试，每次时间间隔相同。 1&gt;发送端的 send 方法本身支持内部重试，重试逻辑如下： a)至多重试3次； b)如果发送失败，则轮转到下一个broker； c)这个方法的总耗时不超过sendMsgTimeout 设置的值，默认 10s，超过时间不在重试。 2&gt;接收端。 Consumer 消费消息失败后，要提供一种重试机制，令消息再消费一次。Consumer 消费消息失败通常可以分为以下两种情况： \1. 由于消息本身的原因，例如反序列化失败，消息数据本身无法处理（例如话费充值，当前消息的手机号被 注销，无法充值）等。定时重试机制，比如过 10s 秒后再重试。 \2. 由于依赖的下游应用服务不可用，例如 db 连接不可用，外系统网络不可达等。 即使跳过当前失败的消息，消费其他消息同样也会报错。这种情况可以 sleep 30s，再消费下一条消息，减轻 Broker 重试消息的压力。 activemq：不支持 并发度Kafka：高 一个线程一个消费者，kafka限制消费者的个数要小于等于分区数，如果要提高并行度，可以在消费者中再开启多线程，或者增加consumer实例数量。 rabbitmq：极高 本身是用Erlang语言写的，并发性能高。 可在消费者中开启多线程，最常用的做法是一个channel对应一个消费者，每一个线程把持一个channel，多个线程复用connection的tcp连接，减少性能开销。 当rabbitmq队列拥有多个消费者的时候，队列收到的消息将以轮询的分发方式发送给消费者。每条消息只会发送给订阅列表里的一个消费者，不会重复。 这种方式非常适合扩展，而且是专门为并发程序设计的。 如果某些消费者的任务比较繁重，那么可以设置basicQos限制信道上消费者能保持的最大未确认消息的数量，在达到上限时，rabbitmq不再向这个消费者发送任何消息。 zeromq：高 rocketmq：高 1&gt;rocketmq限制消费者的个数少于等于队列数，但是可以在消费者中再开启多线程，这一点和kafka是一致的，提高并行度的方法相同。 修改消费并行度方法 a) 同一个 ConsumerGroup 下，通过增加 Consumer 实例数量来提高并行度，超过订阅队列数的 Consumer实例无效。 b) 提高单个 Consumer 的消费并行线程，通过修改参数consumeThreadMin、consumeThreadMax 2&gt;同一个网络连接connection，客户端多个线程可以同时发送请求，连接会被复用，减少性能开销。 activemq：高 单个ActiveMQ的接收和消费消息的速度在1万笔/秒（持久化 一般为1-2万， 非持久化 2 万以上），在生产环境中部署10个Activemq就能达到10万笔/秒以上的性能，部署越多的activemq broker 在MQ上latency也就越低，系统吞吐量也就越高。 参考https://www.zhihu.com/question/43557507 https://mp.weixin.qq.com/s/y3CheyPMJpLpD3pB3lTT9g]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>事务</tag>
        <tag>负载均衡</tag>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud各组件的作用]]></title>
    <url>%2F2018%2F12%2F07%2FSpring-Cloud%E5%90%84%E7%BB%84%E4%BB%B6%E7%9A%84%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[概述 毫无疑问，Spring Cloud是目前微服务架构领域的翘楚，无数的书籍博客都在讲解这个技术。不过大多数讲解还停留在对Spring Cloud功能使用的层面，其底层的很多原理，很多人可能并不知晓。因此本文将通过大量的手绘图，给大家谈谈Spring Cloud微服务架构的底层原理。 实际上，Spring Cloud是一个全家桶式的技术栈，包含了很多组件。本文先从其最核心的几个组件入手，来剖析一下其底层的工作原理。也就是Eureka、Ribbon、Feign、Hystrix、Zuul这几个组件。 业务场景介绍先来给大家说一个业务场景，假设咱们现在开发一个电商网站，要实现支付订单的功能，流程如下： 创建一个订单之后，如果用户立刻支付了这个订单，我们需要将订单状态更新为“已支付” 扣减相应的商品库存 通知仓储中心，进行发货 给用户的这次购物增加相应的积分 针对上述流程，我们需要有订单服务、库存服务、仓储服务、积分服务。整个流程的大体思路如下： 用户针对一个订单完成支付之后，就会去找订单服务，更新订单状态 订单服务调用库存服务，完成相应功能 订单服务调用仓储服务，完成相应功能 订单服务调用积分服务，完成相应功能 至此，整个支付订单的业务流程结束 下图这张图，清晰表明了各服务间的调用过程： 有了业务场景之后，咱们就一起来看看Spring Cloud微服务架构中，这几个组件如何相互协作，各自发挥的作用以及其背后的原理。 核心组件：Eureka咱们来考虑第一个问题：订单服务想要调用库存服务、仓储服务，或者是积分服务，怎么调用？ 订单服务压根儿就不知道人家库存服务在哪台机器上啊！他就算想要发起一个请求，都不知道发送给谁，有心无力！ 这时候，就轮到Spring Cloud Eureka出场了。Eureka是微服务架构中的注册中心，专门负责服务的注册与发现。 咱们来看看下面的这张图，结合图来仔细剖析一下整个流程： 如上图所示，库存服务、仓储服务、积分服务中都有一个Eureka Client组件，这个组件专门负责将这个服务的信息注册到Eureka Server中。说白了，就是告诉Eureka Server，自己在哪台机器上，监听着哪个端口。而Eureka Server是一个注册中心，里面有一个注册表，保存了各服务所在的机器和端口号 订单服务里也有一个Eureka Client组件，这个Eureka Client组件会找Eureka Server问一下：库存服务在哪台机器啊？监听着哪个端口啊？仓储服务呢？积分服务呢？然后就可以把这些相关信息从Eureka Server的注册表中拉取到自己本地缓存起来。 这时如果订单服务想要调用库存服务，不就可以找自己本地的Eureka Client问一下库存服务在哪台机器？监听哪个端口吗？收到响应后，紧接着就可以发送一个请求过去，调用库存服务扣减库存的那个接口！同理，如果订单服务要调用仓储服务、积分服务，也是如法炮制。 总结一下： Eureka Client：负责将这个服务的信息注册到Eureka Server中 Eureka Server：注册中心，里面有一个注册表，保存了各个服务所在的机器和端口号 核心组件：Feign现在订单服务确实知道库存服务、积分服务、仓库服务在哪里了，同时也监听着哪些端口号了。但是新问题又来了：难道订单服务要自己写一大堆代码，跟其他服务建立网络连接，然后构造一个复杂的请求，接着发送请求过去，最后对返回的响应结果再写一大堆代码来处理吗？ 这是上述流程翻译的代码片段，咱们一起来看看，体会一下这种绝望而无助的感受！！！ 友情提示，前方高能： 看完上面那一大段代码，有没有感到后背发凉、一身冷汗？实际上你进行服务间调用时，如果每次都手写代码，代码量比上面那段要多至少几倍，所以这个事儿压根儿就不是地球人能干的。 既然如此，那怎么办呢？别急，Feign早已为我们提供好了优雅的解决方案。来看看如果用Feign的话，你的订单服务调用库存服务的代码会变成啥样？ 看完上面的代码什么感觉？是不是感觉整个世界都干净了，又找到了活下去的勇气！没有底层的建立连接、构造请求、解析响应的代码，直接就是用注解定义一个 FeignClient接口，然后调用那个接口就可以了。人家Feign Client会在底层根据你的注解，跟你指定的服务建立连接、构造请求、发起靕求、获取响应、解析响应，等等。这一系列脏活累活，人家Feign全给你干了。 那么问题来了，Feign是如何做到这么神奇的呢？很简单，Feign的一个关键机制就是使用了动态代理。咱们一起来看看下面的图，结合图来分析： 首先，如果你对某个接口定义了@FeignClient注解，Feign就会针对这个接口创建一个动态代理 接着你要是调用那个接口，本质就是会调用 Feign创建的动态代理，这是核心中的核心 Feign的动态代理会根据你在接口上的@RequestMapping等注解，来动态构造出你要请求的服务的地址 最后针对这个地址，发起请求、解析响应 核心组件：Ribbon说完了Feign，还没完。现在新的问题又来了，如果人家库存服务部署在了5台机器上，如下所示： 192.168.169:9000 192.168.170:9000 192.168.171:9000 192.168.172:9000 192.168.173:9000 这下麻烦了！人家Feign怎么知道该请求哪台机器呢？ 这时Spring Cloud Ribbon就派上用场了。Ribbon就是专门解决这个问题的。它的作用是负载均衡，会帮你在每次请求时选择一台机器，均匀的把请求分发到各个机器上 Ribbon的负载均衡默认使用的最经典的Round Robin轮询算法。这是啥？简单来说，就是如果订单服务对库存服务发起10次请求，那就先让你请求第1台机器、然后是第2台机器、第3台机器、第4台机器、第5台机器，接着再来—个循环，第1台机器、第2台机器。。。以此类推。 此外，Ribbon是和Feign以及Eureka紧密协作，完成工作的，具体如下： 首先Ribbon会从 Eureka Client里获取到对应的服务注册表，也就知道了所有的服务都部署在了哪些机器上，在监听哪些端口号。 然后Ribbon就可以使用默认的Round Robin算法，从中选择一台机器 Feign就会针对这台机器，构造并发起请求。 对上述整个过程，再来一张图，帮助大家更深刻的理解： 核心组件：Hystrix在微服务架构里，一个系统会有很多的服务。以本文的业务场景为例：订单服务在一个业务流程里需要调用三个服务。现在假设订单服务自己最多只有100个线程可以处理请求，然后呢，积分服务不幸的挂了，每次订单服务调用积分服务的时候，都会卡住几秒钟，然后抛出—个超时异常。 咱们一起来分析一下，这样会导致什么问题？ 如果系统处于高并发的场景下，大量请求涌过来的时候，订单服务的100个线程都会卡在请求积分服务这块。导致订单服务没有一个线程可以处理请求 然后就会导致别人请求订单服务的时候，发现订单服务也挂了，不响应任何请求了 上面这个，就是微服务架构中恐怖的服务雪崩问题，如下图所示： 如上图，这么多服务互相调用，要是不做任何保护的话，某一个服务挂了，就会引起连锁反应，导致别的服务也挂。比如积分服务挂了，会导致订单服务的线程全部卡在请求积分服务这里，没有一个线程可以工作，瞬间导致订单服务也挂了，别人请求订单服务全部会卡住，无法响应。 但是我们思考一下，就算积分服务挂了，订单服务也可以不用挂啊！**为什么？** 我们结合业务来看：支付订单的时候，只要把库存扣减了，然后通知仓库发货就OK了 如果积分服务挂了，大不了等他恢复之后，慢慢人肉手工恢复数据！为啥一定要因为一个积分服务挂了，就直接导致订单服务也挂了呢？不可以接受！ 现在问题分析完了，如何解决？ 这时就轮到Hystrix闪亮登场了。Hystrix是隔离、熔断以及降级的一个框架。啥意思呢？说白了，Hystrix会搞很多个小小的线程池，比如订单服务请求库存服务是一个线程池，请求仓储服务是一个线程池，请求积分服务是一个线程池。每个线程池里的线程就仅仅用于请求那个服务。 打个比方：现在很不幸，积分服务挂了，会咋样？ 当然会导致订单服务里的那个用来调用积分服务的线程都卡死不能工作了啊！但是由于订单服务调用库存服务、仓储服务的这两个线程池都是正常工作的，所以这两个服务不会受到任何影响。 这个时候如果别人请求订单服务，订单服务还是可以正常调用库存服务扣减库存，调用仓储服务通知发货。只不过调用积分服务的时候，每次都会报错。但是如果积分服务都挂了，每次调用都要去卡住几秒钟干啥呢？**有意义吗？当然没有！**所以我们直接对积分服务熔断不就得了，比如在5分钟内请求积分服务直接就返回了，不要去走网络请求卡住几秒钟，这个过程，就是所谓的熔断！ 那人家又说，兄弟，积分服务挂了你就熔断，好歹你干点儿什么啊！别啥都不干就直接返回啊？没问题，咱们就来个降级：每次调用积分服务，你就在数据库里记录一条消息，说给某某用户增加了多少积分，因为积分服务挂了，导致没增加成功！这样等积分服务恢复了，你可以根据这些记录手工加一下积分。这个过程，就是所谓的降级。 为帮助大家更直观的理解，接下来用一张图，梳理一下Hystrix隔离、熔断和降级的全流程： 核心组件：Zuul说完了Hystrix，接着给大家说说最后一个组件：Zuul，也就是微服务网关。这个组件是负责网络路由的。不懂网络路由？行，那我给你说说，如果没有Zuul的日常工作会怎样？ 假设你后台部署了几百个服务，现在有个前端兄弟，人家请求是直接从浏览器那儿发过来的。打个比方：人家要请求一下库存服务，你难道还让人家记着这服务的名字叫做inventory-service？部署在5台机器上？就算人家肯记住这一个，你后台可有几百个服务的名称和地址呢？难不成人家请求一个，就得记住一个？你要这样玩儿，那真是友谊的小船，说翻就翻！ 上面这种情况，压根儿是不现实的。所以一般微服务架构中都必然会设计一个网关在里面，像android、ios、pc前端、微信小程序、H5等等，不用去关心后端有几百个服务，就知道有一个网关，所有请求都往网关走，网关会根据请求中的一些特征，将请求转发给后端的各个服务。 而且有一个网关之后，还有很多好处，比如可以做统一的降级、限流、认证授权、安全，等等。 总结最后再来总结一下，上述几个Spring Cloud核心组件，在微服务架构中，分别扮演的角色： Eureka：各个服务启动时，Eureka Client都会将服务注册到Eureka Server，并且Eureka Client还可以反过来从Eureka Server拉取注册表，从而知道其他服务在哪里 Ribbon：服务间发起请求的时候，基于Ribbon做负载均衡，从一个服务的多台机器中选择一台 Feign：基于Feign的动态代理机制，根据注解和选择的机器，拼接请求URL地址，发起请求 Hystrix：发起请求是通过Hystrix的线程池来走的，不同的服务走不同的线程池，实现了不同服务调用的隔离，避免了服务雪崩的问题 Zuul：如果前端、移动端要调用后端系统，统一从Zuul网关进入，由Zuul网关转发请求给对应的服务 以上就是我们通过一个电商业务场景，阐述了Spring Cloud微服务架构几个核心组件的底层原理。 文字总结还不够直观？没问题！我们将Spring Cloud的5个核心组件通过一张图串联起来，再来直观的感受一下其底层的架构原理： 参考石杉的架构笔记(公众号ID：shishan100)（原文出处） https://mp.weixin.qq.com/s/7cIpSV0dHV5jHdxF4Wdtgw]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>分布式</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat 并发量配置以及优化]]></title>
    <url>%2F2018%2F12%2F04%2FTomcat-%E5%B9%B6%E5%8F%91%E9%87%8F%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[为提高Tomcat性能，可从以下两个方面进行优化配置 首先,修改tomcat/conf/server.xml配置文件。 1234567&lt;Executor name="tomcatThreadPool" namePrefix="catalina-exec-" maxThreads="500" minSpareThreads="400" /&gt; &lt;Connector executor="tomcatThreadPool" port="80" protocol="HTTP/1.1" connectionTimeout="20000" enableLookups="false" redirectPort="8443" URIEncoding="UTF-8" acceptCount="1000" /&gt; 其次，修改{tomcat_home}/bin/catalina.bat或者是{tomcat_home}/bin/catalina.sh配置文件为以下。 123456rem ---------------------------------------------------------------------------rem Guess CATALINA_HOME if not defined之间的位置,不要加到那些if里面去了,否则不一定会生效.set JAVA_OPTS=-Xms1024m -Xmx1024m -XX:PermSize=128M -XX:MaxPermSize=256m如果有疑问.可以用echo %JAVA_OPTS%&gt;d:\JAVA_OPTS.log 修改Tomcat内存分配JVM参数配置， 这个会导致严重的stop world时间。 如果你想应用响应平缓， 一般看你的应用对于临时内存的需求， 一般来说， -Xmn128-256m就够了， 这个要看你的停顿时间的计算， 你把gc的收集打印出来，再研究下， 最大停顿时间。 这个我的BLOG说的比较详细。 你去看看吧。 -Xss128k 这个参数， 建议你设置成256k, 不然容易造成不够用， 特别是你的程序有比较多的递归行为。 比如排序。 另外如果想提高内存的性能，可以看看大内存设置。不是很好操作， 我没有测试过。 在性能提升上， 我建议你使用Linux kernel 2.6.22+版本， JAVA6 是不是32位的不是很要紧。这个提升是非常大的。 Heap Size 最大不要超过可用物理内存的80％，一般的要将-Xms和-Xmx选项设置为相同堆内存分配 (访问量比较大时设为一致)。 JVM初始分配的内存由-Xms指定，默认是物理内存的1/64；JVM最大分配的内存由-Xmx指定，默认是物理内存的1/4。默认空余堆内存小于 40%时，JVM就会增大堆直到-Xmx的最大限制；空余堆内存大于70%时，JVM会减少堆直到-Xms的最小限制。因此服务器一般设置-Xms、 -Xmx相等以避免在每次GC 后调整堆的大小。 非堆内存分配JVM使用-XX:PermSize设置非堆内存初始值，默认是物理内存的1/64；由XX:MaxPermSize设置最大非堆内存的大小，默认是物理内存的1/4。 JVM内存限制(最大值)首先JVM内存限制于实际的最大物理内存，假设物理内存无限大的话，JVM内存的最大值跟操作系统有很大的关系。简单的说就32位 处理器虽然可控内存空间有4GB,但是具体的操作系统会给一个限制，这个限制一般是2GB-3GB（一般来说Windows系统下为1.5G- 2G，Linux系统下为2G-3G），而64bit以上的处理器就不会有限制了。(使用java命令测试出支持的最大值)。 修改连接数、线程数主要修改了maxThreads、acceptCount。Google资料说“如果要加大并发连接数，应同时加大这两个参数。 Tomcat的线程数量有待商榷。 thread太多，导致切换过多，性能下降严重。这个数量应该是你单个机器的承载能力， 压力测试下得出的结果。 不可任意加大。一般情况下， 256－512个已经非常高的数值了。 Tomcat的server.xml中Context元素的以下参数应该怎么配合适 ? 123456&lt;Connector port="8080" maxThreads="150" minSpareThreads="25" maxSpareThreads="75" acceptCount="100" /&gt; （第一种方法） maxThreads=”150” 表示最多同时处理150个连接 ； minSpareThreads=”25” 表示即使没有人使用也开这么多空线程等待 ； maxSpareThreads=”75” 表示如果最多可以空75个线程，例如某时刻有80人访问，之后没有人访问了，则tomcat不会保留80个空线程，而是关闭5个空的； acceptCount=”100” 当同时连接的人数达到maxThreads时，还可以接收排队的连接，超过这个连接的则直接返回拒绝连接。 根据你的配置建议 maxThreads=”500” minSpareThreads=”100” 如果你的网站经常访问量都很大的话，缺省就开比较大 maxSpareThreads=”300” acceptCount=”100” 这只是说你的服务器可以支持这么多用户，但还要看你安装了哪些东西，还有你的程序是否足够高效率。 (第二种方法) 1.如何加大tomcat连接数 在tomcat配置文件server.xml中的配置中，和连接数相关的参数有： minProcessors：最小空闲连接线程数，用于提高系统处理性能，默认值为10； maxProcessors：最大连接线程数，即：并发处理的最大请求数，默认值为75 acceptCount：允许的最大连接数，应大于等于maxProcessors，默认值为100 enableLookups：是否反查域名，取值为：true或false。为了提高处理能力，应设置为false； connectionTimeout：网络连接超时，单位：毫秒。设置为0表示永不超时，这样设置有隐患的。通常可设置为30000毫秒。 其中和最大连接数相关的参数为maxProcessors和acceptCount。如果要加大并发连接数，应同时加大这两个参数。 web server允许的最大连接数还受制于操作系统的内核参数设置，通常Windows是2000个左右，Linux是1000个左右。Unix中如何设置这些参数，请参阅Unix常用监控和管理命令。 tomcat4中的配置示例： 12345678&lt;Connector port="8080" minProcessors="10" maxProcessors="1024"enableLookups="false" redirectPort="8443"acceptCount="1024" debug="0" connectionTimeout="30000" /&gt; 对于其他端口的侦听配置，以此类推。 2. tomcat中如何禁止列目录下的文件 在{tomcat_home}/conf/web.xml中，把listings参数设置成false即可，如下： … listings false]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>垃圾回收</tag>
        <tag>内存</tag>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL高性能优化原理]]></title>
    <url>%2F2018%2F12%2F02%2FMySQL%E9%AB%98%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[说起 MySQL 的查询优化，相信大家收藏了一堆奇技淫巧：不能使用 SELECT *、不使用 NULL 字段、合理创建索引、为字段选择合适的数据类型….. 你是否真的理解这些优化技巧？是否理解其背后的工作原理？在实际场景下性能真有提升吗？我想未必。因而理解这些优化建议背后的原理就尤为重要，希望本文能让你重新审视这些优化建议，并在实际业务场景下合理的运用。 MySQL 逻辑架构如果能在头脑中构建一幅 MySQL 各组件之间如何协同工作的架构图，有助于深入理解 MySQL 服务器。下图展示了 MySQL 的逻辑架构图。 MySQL 逻辑架构整体分为三层，最上层为客户端层，并非 MySQL 所独有，诸如：连接处理、授权认证、安全等功能均在这一层处理。 MySQL 大多数核心服务均在中间这一层，包括查询解析、分析、优化、缓存、内置函数 (比如：时间、数学、加密等函数)。所有的跨存储引擎的功能也在这一层实现：存储过程、触发器、视图等。 最下层为存储引擎，其负责 MySQL 中的数据存储和提取。和 Linux 下的文件系统类似，每种存储引擎都有其优势和劣势。中间的服务层通过 API 与存储引擎通信，这些 API 接口屏蔽了不同存储引擎间的差异。 MySQL 查询过程我们总是希望 MySQL 能够获得更高的查询性能，最好的办法是弄清楚 MySQL 是如何优化和执行查询的。一旦理解了这一点，就会发现：很多的查询优化工作实际上就是遵循一些原则让 MySQL 的优化器能够按照预想的合理方式运行而已。当向 MySQL 发送一个请求的时候，MySQL 到底做了些什么呢？ 客户端 / 服务端通信协议MySQL 客户端 / 服务端通信协议是 “半双工” 的：在任一时刻，要么是服务器向客户端发送数据，要么是客户端向服务器发送数据，这两个动作不能同时发生。一旦一端开始发送消息，另一端要接收完整个消息才能响应它，所以我们无法也无须将一个消息切成小块独立发送，也没有办法进行流量控制。 客户端用一个单独的数据包将查询请求发送给服务器，所以当查询语句很长的时候，需要设置 max_allowed_packet 参数。但是需要注意的是，如果查询实在是太大，服务端会拒绝接收更多数据并抛出异常。 与之相反的是，服务器响应给用户的数据通常会很多，由多个数据包组成。但是当服务器响应客户端请求时，客户端必须完整的接收整个返回结果，而不能简单的只取前面几条结果，然后让服务器停止发送。因而在实际开发中，尽量保持查询简单且只返回必需的数据，减小通信间数据包的大小和数量是一个非常好的习惯，这也是查询中尽量避免使用 SELECT * 以及加上 LIMIT 限制的原因之一。 查询缓存在解析一个查询语句前，如果查询缓存是打开的，那么 MySQL 会检查这个查询语句是否命中查询缓存中的数据。如果当前查询恰好命中查询缓存，在检查一次用户权限后直接返回缓存中的结果。这种情况下，查询不会被解析，也不会生成执行计划，更不会执行。 MySQL 将缓存存放在一个引用表（不要理解成 table，可以认为是类似于 HashMap 的数据结构），通过一个哈希值索引，这个哈希值通过查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息计算得来。所以两个查询在任何字符上的不同（例如：空格、注释），都会导致缓存不会命中。 如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、MySQL 库中的系统表，其查询结果都不会被缓存。比如函数 NOW() 或者 CURRENT_DATE() 会因为不同的查询时间，返回不同的查询结果，再比如包含 CURRENT_USER 或者 CONNECION_ID() 的查询语句会因为不同的用户而返回不同的结果，将这样的查询结果缓存起来没有任何的意义。 既然是缓存，就会失效，那查询缓存何时失效呢？ MySQL 的查询缓存系统会跟踪查询中涉及的每个表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效。正因为如此，在任何的写操作时，MySQL 必须将对应表的所有缓存都设置为失效。如果查询缓存非常大或者碎片很多，这个操作就可能带来很大的系统消耗，甚至导致系统僵死一会儿。而且查询缓存对系统的额外消耗也不仅仅在写操作，读操作也不例外： 1、任何的查询语句在开始之前都必须经过检查，即使这条 SQL 语句永远不会命中缓存 2、如果查询结果可以被缓存，那么执行完成后，会将结果存入缓存，也会带来额外的系统消耗 基于此，我们要知道并不是什么情况下查询缓存都会提高系统性能，缓存和失效都会带来额外消耗，只有当缓存带来的资源节约大于其本身消耗的资源时，才会给系统带来性能提升。但要如何评估打开缓存是否能够带来性能提升是一件非常困难的事情，也不在本文讨论的范畴内。如果系统确实存在一些性能问题，可以尝试打开查询缓存，并在数据库设计上做一些优化，比如： 1、用多个小表代替一个大表，注意不要过度设计 2、批量插入代替循环单条插入 3、合理控制缓存空间大小，一般来说其大小设置为几十兆比较合适 4、可以通过 SQL_CACHE 和 SQL_NO_CACHE 来控制某个查询语句是否需要进行缓存 最后的忠告是不要轻易打开查询缓存，特别是写密集型应用。如果你实在是忍不住，可以将 query_cache_type 设置为 DEMAND，这时只有加入 SQL_CACHE 的查询才会走缓存，其他查询则不会，这样可以非常自由地控制哪些查询需要被缓存。 当然查询缓存系统本身是非常复杂的，这里讨论的也只是很小的一部分，其他更深入的话题，比如：缓存是如何使用内存的？如何控制内存的碎片化？事务对查询缓存有何影响等等，读者可以自行阅读相关资料，这里权当抛砖引玉吧。 语法解析和预处理MySQL 通过关键字将 SQL 语句进行解析，并生成一棵对应的解析树。这个过程解析器主要通过语法规则来验证和解析。比如 SQL 中是否使用了错误的关键字或者关键字的顺序是否正确等等。预处理则会根据 MySQL 规则进一步检查解析树是否合法。比如检查要查询的数据表和数据列是否存在等。 查询优化经过前面的步骤生成的语法树被认为是合法的了，并且由优化器将其转化成查询计划。多数情况下，一条查询可以有很多种执行方式，最后都返回相应的结果。优化器的作用就是找到这其中最好的执行计划。 MySQL 使用基于成本的优化器，它尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个。在 MySQL 可以通过查询当前会话的 last_query_cost 的值来得到其计算当前查询的成本。 12345678910111213141516mysql&gt; select * from t_message limit 10;...省略结果集mysql&gt; show status like &apos;last_query_cost&apos;;+-----------------+-------------+| Variable_name | Value |+-----------------+-------------+| Last_query_cost | 6391.799000 |+-----------------+-------------+ 示例中的结果表示优化器认为大概需要做 6391 个数据页的随机查找才能完成上面的查询。这个结果是根据一些列的统计信息计算得来的，这些统计信息包括：每张表或者索引的页面个数、索引的基数、索引和数据行的长度、索引的分布情况等等。 有非常多的原因会导致 MySQL 选择错误的执行计划，比如统计信息不准确、不会考虑不受其控制的操作成本（用户自定义函数、存储过程）、MySQL 认为的最优跟我们想的不一样（我们希望执行时间尽可能短，但 MySQL 值选择它认为成本小的，但成本小并不意味着执行时间短）等等。 MySQL 的查询优化器是一个非常复杂的部件，它使用了非常多的优化策略来生成一个最优的执行计划： 1、重新定义表的关联顺序（多张表关联查询时，并不一定按照 SQL 中指定的顺序进行，但有一些技巧可以指定关联顺序） 2、优化 MIN() 和 MAX() 函数（找某列的最小值，如果该列有索引，只需要查找 B+Tree 索引最左端，反之则可以找到最大值，具体原理见下文） 3、提前终止查询（比如：使用 Limit 时，查找到满足数量的结果集后会立即终止查询） 4、优化排序（在老版本 MySQL 会使用两次传输排序，即先读取行指针和需要排序的字段在内存中对其排序，然后再根据排序结果去读取数据行，而新版本采用的是单次传输排序，也就是一次读取所有的数据行，然后根据给定的列排序。对于 I/O 密集型应用，效率会高很多） 随着 MySQL 的不断发展，优化器使用的优化策略也在不断的进化，这里仅仅介绍几个非常常用且容易理解的优化策略，其他的优化策略，大家自行查阅吧。 查询执行引擎在完成解析和优化阶段以后，MySQL 会生成对应的执行计划，查询执行引擎根据执行计划给出的指令逐步执行得出结果。整个执行过程的大部分操作均是通过调用存储引擎实现的接口来完成，这些接口被称为 handler API。查询过程中的每一张表由一个 handler 实例表示。实际上，MySQL 在查询优化阶段就为每一张表创建了一个 handler 实例，优化器可以根据这些实例的接口来获取表的相关信息，包括表的所有列名、索引统计信息等。存储引擎接口提供了非常丰富的功能，但其底层仅有几十个接口，这些接口像搭积木一样完成了一次查询的大部分操作。 返回结果给客户端查询执行的最后一个阶段就是将结果返回给客户端。即使查询不到数据，MySQL 仍然会返回这个查询的相关信息，比如该查询影响到的行数以及执行时间等。 如果查询缓存被打开且这个查询可以被缓存，MySQL 也会将结果存放到缓存中。 结果集返回客户端是一个增量且逐步返回的过程。有可能 MySQL 在生成第一条结果时，就开始向客户端逐步返回结果集了。这样服务端就无须存储太多结果而消耗过多内存，也可以让客户端第一时间获得返回结果。需要注意的是，结果集中的每一行都会以一个满足①中所描述的通信协议的数据包发送，再通过 TCP 协议进行传输，在传输过程中，可能对 MySQL 的数据包进行缓存然后批量发送。 回头总结一下 MySQL 整个查询执行过程，总的来说分为以下个步骤： 1、客户端向 MySQL 服务器发送一条查询请求 2、服务器首先检查查询缓存，如果命中缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段 3、服务器进行 SQL 解析、预处理、再由优化器生成对应的执行计划 4、MySQL 根据执行计划，调用存储引擎的 API 来执行查询 5、将结果返回给客户端，同时缓存查询结果 性能优化建议看了这么多，你可能会期待给出一些优化手段，是的，下面会从 3 个不同方面给出一些优化建议。但请等等，还有一句忠告要先送给你：不要听信你看到的关于优化的 “绝对真理”，包括本文所讨论的内容，而应该是在实际的业务场景下通过测试来验证你关于执行计划以及响应时间的假设。 Scheme 设计与数据类型优化选择数据类型只要遵循小而简单的原则就好，越小的数据类型通常会更快，占用更少的磁盘、内存，处理时需要的 CPU 周期也更少。越简单的数据类型在计算时需要更少的 CPU 周期，比如，整型就比字符操作代价低，因而会使用整型来存储 ip 地址，使用 DATETIME 来存储时间，而不是使用字符串。 这里总结几个可能容易理解错误的技巧： 1、通常来说把可为 NULL 的列改为 NOT NULL 不会对性能提升有多少帮助，只是如果计划在列上创建索引，就应该将该列设置为 NOT NULL。 2、对整数类型指定宽度，比如 INT(11)，没有任何卵用。INT 使用 32 位（4 个字节）存储空间，那么它的表示范围已经确定，所以 INT(1) 和 INT(20) 对于存储和计算是相同的。 3、UNSIGNED 表示不允许负值，大致可以使正数的上限提高一倍。比如 TINYINT 存储范围是 - 128 ~ 127，而 UNSIGNED TINYINT 存储的范围却是 0 – 255。 4、通常来讲，没有太大的必要使用 DECIMAL 数据类型。即使是在需要存储财务数据时，仍然可以使用 BIGINT。比如需要精确到万分之一，那么可以将数据乘以一百万然后使用 BIGINT 存储。这样可以避免浮点数计算不准确和 DECIMAL 精确计算代价高的问题。 5、TIMESTAMP 使用 4 个字节存储空间，DATETIME 使用 8 个字节存储空间。因而，TIMESTAMP 只能表示 1970 – 2038 年，比 DATETIME 表示的范围小得多，而且 TIMESTAMP 的值因时区不同而不同。 6、大多数情况下没有使用枚举类型的必要，其中一个缺点是枚举的字符串列表是固定的，添加和删除字符串（枚举选项）必须使用 ALTER TABLE（如果只是在列表末尾追加元素，不需要重建表）。 7、schema 的列不要太多。原因是存储引擎的 API 工作时需要在服务器层和存储引擎层之间通过行缓冲格式拷贝数据，然后在服务器层将缓冲内容解码成各个列，这个转换过程的代价是非常高的。如果列太多而实际使用的列又很少的话，有可能会导致 CPU 占用过高。 8、大表 ALTER TABLE 非常耗时，MySQL 执行大部分修改表结果操作的方法是用新的结构创建一个张空表，从旧表中查出所有的数据插入新表，然后再删除旧表。尤其当内存不足而表又很大，而且还有很大索引的情况下，耗时更久。当然有一些奇技淫巧可以解决这个问题，有兴趣可自行查阅。 创建高性能索引索引是提高 MySQL 查询性能的一个重要途径，但过多的索引可能会导致过高的磁盘使用率以及过高的内存占用，从而影响应用程序的整体性能。应当尽量避免事后才想起添加索引，因为事后可能需要监控大量的 SQL 才能定位到问题所在，而且添加索引的时间肯定是远大于初始添加索引所需要的时间，可见索引的添加也是非常有技术含量的。 接下来将向你展示一系列创建高性能索引的策略，以及每条策略其背后的工作原理。但在此之前，先了解与索引相关的一些算法和数据结构，将有助于更好的理解后文的内容。 索引相关的数据结构和算法通常我们所说的索引是指 B-Tree 索引，它是目前关系型数据库中查找数据最为常用和有效的索引，大多数存储引擎都支持这种索引。使用 B-Tree 这个术语，是因为 MySQL 在 CREATE TABLE 或其它语句中使用了这个关键字，但实际上不同的存储引擎可能使用不同的数据结构，比如 InnoDB 就是使用的 B+Tree。 B+Tree 中的 B 是指 balance，意为平衡。需要注意的是，B + 树索引并不能找到一个给定键值的具体行，它找到的只是被查找数据行所在的页，接着数据库会把页读入到内存，再在内存中进行查找，最后得到要查找的数据。 在介绍 B+Tree 前，先了解一下二叉查找树，它是一种经典的数据结构，其左子树的值总是小于根的值，右子树的值总是大于根的值，如下图①。如果要在这课树中查找值为 5 的记录，其大致流程：先找到根，其值为 6，大于 5，所以查找左子树，找到 3，而 5 大于 3，接着找 3 的右子树，总共找了 3 次。同样的方法，如果查找值为 8 的记录，也需要查找 3 次。所以二叉查找树的平均查找次数为 (3 + 3 + 3 + 2 + 2 + 1) / 6 = 2.3 次，而顺序查找的话，查找值为 2 的记录，仅需要 1 次，但查找值为 8 的记录则需要 6 次，所以顺序查找的平均查找次数为：(1 + 2 + 3 + 4 + 5 + 6) / 6 = 3.3 次，因此大多数情况下二叉查找树的平均查找速度比顺序查找要快。 由于二叉查找树可以任意构造，同样的值，可以构造出如图②的二叉查找树，显然这棵二叉树的查询效率和顺序查找差不多。若想二叉查找数的查询性能最高，需要这棵二叉查找树是平衡的，也即平衡二叉树（AVL 树）。 平衡二叉树首先需要符合二叉查找树的定义，其次必须满足任何节点的两个子树的高度差不能大于 1。显然图②不满足平衡二叉树的定义，而图①是一课平衡二叉树。平衡二叉树的查找性能是比较高的（性能最好的是最优二叉树），查询性能越好，维护的成本就越大。比如图①的平衡二叉树，当用户需要插入一个新的值 9 的节点时，就需要做出如下变动。 通过一次左旋操作就将插入后的树重新变为平衡二叉树是最简单的情况了，实际应用场景中可能需要旋转多次。至此我们可以考虑一个问题，平衡二叉树的查找效率还不错，实现也非常简单，相应的维护成本还能接受，为什么 MySQL 索引不直接使用平衡二叉树？ 随着数据库中数据的增加，索引本身大小随之增加，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘 I/O 消耗，相对于内存存取，I/O 存取的消耗要高几个数量级。可以想象一下一棵几百万节点的二叉树的深度是多少？如果将这么大深度的一颗二叉树放磁盘上，每读取一个节点，需要一次磁盘的 I/O 读取，整个查找的耗时显然是不能够接受的。那么如何减少查找过程中的 I/O 存取次数？ 一种行之有效的解决方法是减少树的深度，将二叉树变为 m 叉树（多路搜索树），而 B+Tree 就是一种多路搜索树。理解 B+Tree 时，只需要理解其最重要的两个特征即可：第一，所有的关键字（可以理解为数据）都存储在叶子节点（Leaf Page），非叶子节点（Index Page）并不存储真正的数据，所有记录节点都是按键值大小顺序存放在同一层叶子节点上。其次，所有的叶子节点由指针连接。如下图为高度为 2 的简化了的 B+Tree。 怎么理解这两个特征？MySQL 将每个节点的大小设置为一个页的整数倍（原因下文会介绍），也就是在节点空间大小一定的情况下，每个节点可以存储更多的内结点，这样每个结点能索引的范围更大更精确。所有的叶子节点使用指针链接的好处是可以进行区间访问，比如上图中，如果查找大于 20 而小于 30 的记录，只需要找到节点 20，就可以遍历指针依次找到 25、30。如果没有链接指针的话，就无法进行区间查找。这也是 MySQL 使用 B+Tree 作为索引存储结构的重要原因。 MySQL 为何将节点大小设置为页的整数倍，这就需要理解磁盘的存储原理。磁盘本身存取就比主存慢很多，在加上机械运动损耗（特别是普通的机械硬盘），磁盘的存取速度往往是主存的几百万分之一，为了尽量减少磁盘 I/O，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存，预读的长度一般为页的整数倍。 “页是计算机管理存储器的逻辑块，硬件及 OS 往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（许多 OS 中，页的大小通常为 4K）。主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后一起返回，程序继续运行。” MySQL 巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次 I/O 就可以完全载入。为了达到这个目的，每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了读取一个节点只需一次 I/O。假设 B+Tree 的高度为 h，一次检索最多需要 h-1I/O（根节点常驻内存），复杂度 $O(h) = O(\log_{M}N)$。实际应用场景中，M 通常较大，常常超过 100，因此树的高度一般都比较小，通常不超过 3。 最后简单了解下 B+Tree 节点的操作，在整体上对索引的维护有一个大概的了解，虽然索引可以大大提高查询效率，但维护索引仍要花费很大的代价，因此合理的创建索引也就尤为重要。 仍以上面的树为例，我们假设每个节点只能存储 4 个内节点。首先要插入第一个节点 28，如下图所示。 接着插入下一个节点 70，在 Index Page 中查询后得知应该插入到 50 – 70 之间的叶子节点，但叶子节点已满，这时候就需要进行也分裂的操作，当前的叶子节点起点为 50，所以根据中间值来拆分叶子节点，如下图所示。 最后插入一个节点 95，这时候 Index Page 和 Leaf Page 都满了，就需要做两次拆分，如下图所示。 拆分后最终形成了这样一颗树。 B+Tree 为了保持平衡，对于新插入的值需要做大量的拆分页操作，而页的拆分需要 I/O 操作，为了尽可能的减少页的拆分操作，B+Tree 也提供了类似于平衡二叉树的旋转功能。当 Leaf Page 已满但其左右兄弟节点没有满的情况下，B+Tree 并不急于去做拆分操作，而是将记录移到当前所在页的兄弟节点上。通常情况下，左兄弟会被先检查用来做旋转操作。就比如上面第二个示例，当插入 70 的时候，并不会去做页拆分，而是左旋操作。 通过旋转操作可以最大限度的减少页分裂，从而减少索引维护过程中的磁盘的 I/O 操作，也提高索引维护效率。需要注意的是，删除节点跟插入节点类似，仍然需要旋转和拆分操作，这里就不再说明。 高性能策略通过上文，相信你对 B+Tree 的数据结构已经有了大致的了解，但 MySQL 中索引是如何组织数据的存储呢？以一个简单的示例来说明，假如有如下数据表： 12345678910111213CREATE TABLE People(last_name varchar(50) not null,first_name varchar(50) not null,dob date not null,gender enum(`m`,`f`) not null,key(last_name,first_name,dob)); 对于表中每一行数据，索引中包含了 last_name、first_name、dob 列的值，下图展示了索引是如何组织数据存储的。 可以看到，索引首先根据第一个字段来排列顺序，当名字相同时，则根据第三个字段，即出生日期来排序，正是因为这个原因，才有了索引的 “最左原则”。 MySQL 不会使用索引的情况：非独立的列“独立的列” 是指索引列不能是表达式的一部分，也不能是函数的参数。比如： 1select * from where id + 1 = 5 我们很容易看出其等价于 id = 4，但是 MySQL 无法自动解析这个表达式，使用函数是同样的道理。 前缀索引如果列很长，通常可以索引开始的部分字符，这样可以有效节约索引空间，从而提高索引效率。 多列索引和索引顺序在多数情况下，在多个列上建立独立的索引并不能提高查询性能。理由非常简单，MySQL 不知道选择哪个索引的查询效率更好，所以在老版本，比如 MySQL5.0 之前就会随便选择一个列的索引，而新的版本会采用合并索引的策略。举个简单的例子，在一张电影演员表中，在 actor_id 和 film_id 两个列上都建立了独立的索引，然后有如下查询： 老版本的 MySQL 会随机选择一个索引，但新版本做如下的优化： 12345select film_id,actor_id from film_actor where actor_id = 1union allselect film_id,actor_id from film_actor where film_id = 1 and actor_id &lt;&gt; 1 1、当出现多个索引做相交操作时（多个 AND 条件），通常来说一个包含所有相关列的索引要优于多个独立索引。 2、当出现多个索引做联合操作时（多个 OR 条件），对结果集的合并、排序等操作需要耗费大量的 CPU 和内存资源，特别是当其中的某些索引的选择性不高，需要返回合并大量数据时，查询成本更高。所以这种情况下还不如走全表扫描。 因此 explain 时如果发现有索引合并（Extra 字段出现 Using union），应该好好检查一下查询和表结构是不是已经是最优的，如果查询和表都没有问题，那只能说明索引建的非常糟糕，应当慎重考虑索引是否合适，有可能一个包含所有相关列的多列索引更适合。 前面我们提到过索引如何组织数据存储的，从图中可以看到多列索引时，索引的顺序对于查询是至关重要的，很明显应该把选择性更高的字段放到索引的前面，这样通过第一个字段就可以过滤掉大多数不符合条件的数据。 索引选择性是指不重复的索引值和数据表的总记录数的比值，选择性越高查询效率越高，因为选择性越高的索引可以让 MySQL 在查询时过滤掉更多的行。唯一索引的选择性是 1，这时最好的索引选择性，性能也是最好的。 理解索引选择性的概念后，就不难确定哪个字段的选择性较高了，查一下就知道了，比如： 1SELECT * FROM payment where staff_id = 2 and customer_id = 584 是应该创建 (staff_id,customer_id) 的索引还是应该颠倒一下顺序？执行下面的查询，哪个字段的选择性更接近 1 就把哪个字段索引前面就好。 12345select count(distinct staff_id)/count(*) as staff_id_selectivity,count(distinct customer_id)/count(*) as customer_id_selectivity,count(*) from payment 多数情况下使用这个原则没有任何问题，但仍然注意你的数据中是否存在一些特殊情况。举个简单的例子，比如要查询某个用户组下有过交易的用户信息： 1select user_id from trade where user_group_id = 1 and trade_amount &gt; 0 MySQL 为这个查询选择了索引 (user_group_id,trade_amount)，如果不考虑特殊情况，这看起来没有任何问题，但实际情况是这张表的大多数数据都是从老系统中迁移过来的，由于新老系统的数据不兼容，所以就给老系统迁移过来的数据赋予了一个默认的用户组。这种情况下，通过索引扫描的行数跟全表扫描基本没什么区别，索引也就起不到任何作用。 推广开来说，经验法则和推论在多数情况下是有用的，可以指导我们开发和设计，但实际情况往往会更复杂，实际业务场景下的某些特殊情况可能会摧毁你的整个设计。 避免多个范围条件实际开发中，我们会经常使用多个范围条件，比如想查询某个时间段内登录过的用户： 1select user.* from user where login_time &gt; &apos;2017-04-01&apos; and age between 18 and 30; 这个查询有一个问题：它有两个范围条件，login_time 列和 age 列，MySQL 可以使用 login_time 列的索引或者 age 列的索引，但无法同时使用它们。 覆盖索引 如果一个索引包含或者说覆盖所有需要查询的字段的值，那么就没有必要再回表查询，这就称为覆盖索引。覆盖索引是非常有用的工具，可以极大的提高性能，因为查询只需要扫描索引会带来许多好处： 1、索引条目远小于数据行大小，如果只读取索引，极大减少数据访问量 2、索引是有按照列值顺序存储的，对于 I/O 密集型的范围查询要比随机从磁盘读取每一行数据的 IO 要少的多 使用索引扫描来排序 MySQL 有两种方式可以生产有序的结果集，其一是对结果集进行排序的操作，其二是按照索引顺序扫描得出的结果自然是有序的。如果 explain 的结果中 type 列的值为 index 表示使用了索引扫描来做排序。 扫描索引本身很快，因为只需要从一条索引记录移动到相邻的下一条记录。但如果索引本身不能覆盖所有需要查询的列，那么就不得不每扫描一条索引记录就回表查询一次对应的行。这个读取操作基本上是随机 I/O，因此按照索引顺序读取数据的速度通常要比顺序地全表扫描要慢。 在设计索引时，如果一个索引既能够满足排序，又满足查询，是最好的。 只有当索引的列顺序和 ORDER BY 子句的顺序完全一致，并且所有列的排序方向也一样时，才能够使用索引来对结果做排序。如果查询需要关联多张表，则只有 ORDER BY 子句引用的字段全部为第一张表时，才能使用索引做排序。ORDER BY 子句和查询的限制是一样的，都要满足最左前缀的要求（有一种情况例外，就是最左的列被指定为常数，下面是一个简单的示例），其它情况下都需要执行排序操作，而无法利用索引排序。 123// 最左列为常数，索引：(date,staff_id,customer_id)select staff_id,customer_id from demo where date = &apos;2015-06-01&apos; order by staff_id,customer_id 冗余和重复索引冗余索引是指在相同的列上按照相同的顺序创建的相同类型的索引，应当尽量避免这种索引，发现后立即删除。比如有一个索引 (A,B)，再创建索引 (A) 就是冗余索引。冗余索引经常发生在为表添加新索引时，比如有人新建了索引 (A,B)，但这个索引不是扩展已有的索引 (A)。 大多数情况下都应该尽量扩展已有的索引而不是创建新索引。但有极少情况下出现性能方面的考虑需要冗余索引，比如扩展已有索引而导致其变得过大，从而影响到其他使用该索引的查询。 删除长期未使用的索引定期删除一些长时间未使用过的索引是一个非常好的习惯。 关于索引这个话题打算就此打住，最后要说一句，索引并不总是最好的工具，只有当索引帮助提高查询速度带来的好处大于其带来的额外工作时，索引才是有效的。对于非常小的表，简单的全表扫描更高效。对于中到大型的表，索引就非常有效。对于超大型的表，建立和维护索引的代价随之增长，这时候其他技术也许更有效，比如分区表。最后的最后，explain 后再提测是一种美德。 特定类型查询优化优化 COUNT() 查询COUNT() 可能是被大家误解最多的函数了，它有两种不同的作用，其一是统计某个列值的数量，其二是统计行数。统计列值时，要求列值是非空的，它不会统计 NULL。如果确认括号中的表达式不可能为空时，实际上就是在统计行数。最简单的就是当使用 COUNT(*) 时，并不是我们所想象的那样扩展成所有的列，实际上，它会忽略所有的列而直接统计所有的行数。 我们最常见的误解也就在这儿，在括号内指定了一列却希望统计结果是行数，而且还常常误以为前者的性能会更好。但实际并非这样，如果要统计行数，直接使用 COUNT(*)，意义清晰，且性能更好。 有时候某些业务场景并不需要完全精确的 COUNT 值，可以用近似值来代替，EXPLAIN 出来的行数就是一个不错的近似值，而且执行 EXPLAIN 并不需要真正地去执行查询，所以成本非常低。通常来说，执行 COUNT() 都需要扫描大量的行才能获取到精确的数据，因此很难优化，MySQL 层面还能做得也就只有覆盖索引了。如果不还能解决问题，只有从架构层面解决了，比如添加汇总表，或者使用 redis 这样的外部缓存系统。 优化关联查询在大数据场景下，表与表之间通过一个冗余字段来关联，要比直接使用 JOIN 有更好的性能。如果确实需要使用关联查询的情况下，需要特别注意的是： 1、确保 ON 和 USING 字句中的列上有索引。在创建索引的时候就要考虑到关联的顺序。当表 A 和表 B 用列 c 关联的时候，如果优化器关联的顺序是 A、B，那么就不需要在 A 表的对应列上创建索引。没有用到的索引会带来额外的负担，一般来说，除非有其他理由，只需要在关联顺序中的第二张表的相应列上创建索引（具体原因下文分析）。 2、确保任何的 GROUP BY 和 ORDER BY 中的表达式只涉及到一个表中的列，这样 MySQL 才有可能使用索引来优化。 要理解优化关联查询的第一个技巧，就需要理解 MySQL 是如何执行关联查询的。当前 MySQL 关联执行的策略非常简单，它对任何的关联都执行嵌套循环关联操作，即先在一个表中循环取出单条数据，然后在嵌套循环到下一个表中寻找匹配的行，依次下去，直到找到所有表中匹配的行为为止。然后根据各个表匹配的行，返回查询中需要的各个列。 太抽象了？以上面的示例来说明，比如有这样的一个查询： 12345SELECT A.xx,B.yyFROM A INNER JOIN B USING(c)WHERE A.xx IN (5,6) 假设 MySQL 按照查询中的关联顺序 A、B 来进行关联操作，那么可以用下面的伪代码表示 MySQL 如何完成这个查询： 123456789101112131415161718192021outer_iterator = SELECT A.xx,A.c FROM A WHERE A.xx IN (5,6);outer_row = outer_iterator.next;while(outer_row) &#123;inner_iterator = SELECT B.yy FROM B WHERE B.c = outer_row.c;inner_row = inner_iterator.next;while(inner_row) &#123;output[inner_row.yy,outer_row.xx];inner_row = inner_iterator.next;&#125;outer_row = outer_iterator.next;&#125; 可以看到，最外层的查询是根据 A.xx 列来查询的，A.c 上如果有索引的话，整个关联查询也不会使用。再看内层的查询，很明显 B.c 上如果有索引的话，能够加速查询，因此只需要在关联顺序中的第二张表的相应列上创建索引即可。 优化 LIMIT 分页当需要分页操作时，通常会使用 LIMIT 加上偏移量的办法实现，同时加上合适的 ORDER BY 字句。如果有对应的索引，通常效率会不错，否则，MySQL 需要做大量的文件排序操作。 一个常见的问题是当偏移量非常大的时候，比如：LIMIT 10000 20 这样的查询，MySQL 需要查询 10020 条记录然后只返回 20 条记录，前面的 10000 条都将被抛弃，这样的代价非常高。 优化这种查询一个最简单的办法就是尽可能的使用覆盖索引扫描，而不是查询所有的列。然后根据需要做一次关联查询再返回所有的列。对于偏移量很大时，这样做的效率会提升非常大。考虑下面的查询： 1SELECT film_id,description FROM film ORDER BY title LIMIT 50,5; 如果这张表非常大，那么这个查询最好改成下面的样子： 1234567SELECT film.film_id,film.descriptionFROM film INNER JOIN (SELECT film_id FROM film ORDER BY title LIMIT 50,5) AS tmp USING(film_id); 这里的延迟关联将大大提升查询效率，让 MySQL 扫描尽可能少的页面，获取需要访问的记录后在根据关联列回原表查询所需要的列。 有时候如果可以使用书签记录上次取数据的位置，那么下次就可以直接从该书签记录的位置开始扫描，这样就可以避免使用 OFFSET，比如下面的查询： 1SELECT id FROM t LIMIT 10000, 10; 改为： 1SELECT id FROM t WHERE id &gt; 10000 LIMIT 10; 其它优化的办法还包括使用预先计算的汇总表，或者关联到一个冗余表，冗余表中只包含主键列和需要做排序的列。 优化 UNIONMySQL 处理 UNION 的策略是先创建临时表，然后再把各个查询结果插入到临时表中，最后再来做查询。因此很多优化策略在 UNION 查询中都没有办法很好的时候。经常需要手动将 WHERE、LIMIT、ORDER BY 等字句 “下推” 到各个子查询中，以便优化器可以充分利用这些条件先优化。 除非确实需要服务器去重，否则就一定要使用 UNION ALL，如果没有 ALL 关键字，MySQL 会给临时表加上 DISTINCT 选项，这会导致整个临时表的数据做唯一性检查，这样做的代价非常高。当然即使使用 ALL 关键字，MySQL 总是将结果放入临时表，然后再读出，再返回给客户端。虽然很多时候没有这个必要，比如有时候可以直接把每个子查询的结果返回给客户端。 结语理解查询是如何执行以及时间都消耗在哪些地方，再加上一些优化过程的知识，可以帮助大家更好的理解 MySQL，理解常见优化技巧背后的原理。希望本文中的原理、示例能够帮助大家更好的将理论和实践联系起来，更多的将理论知识运用到实践中。其他也没啥说的了，给大家留两个思考题吧，可以在脑袋里想想答案，这也是大家经常挂在嘴边的，但很少有人会思考为什么？ 1、有非常多的程序员在分享时都会抛出这样一个观点：尽可能不要使用存储过程，存储过程非常不容易维护，也会增加使用成本，应该把业务逻辑放到客户端。既然客户端都能干这些事，那为什么还要存储过程？ 2、JOIN 本身也挺方便的，直接查询就好了，为什么还需要视图呢？ 为什么要使用存储过程？ 存储过程是经过编译的一些列SQL语句的集合，如果用程序来实现，可能需要多次连接数据库，这样降低了程序和数据库的耦合。 为什么需要视图？ 视图是一张虚拟表，简化了用户的操作，而且可以对机密数据提供安全保护。 参考https://www.jianshu.com/p/d7665192aaaf https://mp.weixin.qq.com/s/yKYeI071r-g_7sJyN5Q-Aw 姜承尧 著；MySQL技术内幕-InnoDB存储引擎；机械工业出版社，2013 Baron Scbwartz 等著；宁海元 周振兴等译；高性能MySQL（第三版）; 电子工业出版社， 2013 由 B-/B+树看 MySQL索引结构]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>索引</tag>
        <tag>数据结构</tag>
        <tag>CPU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里员工的Java问题排查工具单]]></title>
    <url>%2F2018%2F11%2F25%2F%E9%98%BF%E9%87%8C%E5%91%98%E5%B7%A5%E7%9A%84Java%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E5%B7%A5%E5%85%B7%E5%8D%95%2F</url>
    <content type="text"><![CDATA[前言这是一篇来源于阿里内部技术论坛的文章，原文在阿里内部获得一致好评。作者已经把这篇文章开放到云栖社区中供外网访问。本文中包含一些只有阿里内部才能使用的工具，还有些只有阿里内部才能访问的链接。 摘要： # 我的问题排查工具箱 ## 前言 平时的工作中经常碰到很多疑难问题的处理，在解决问题的同时，有一些工具起到了相当大的作用，在此书写下来，一是作为笔记，可以让自己后续忘记了可快速翻阅，二是分享，希望看到此文的同学们可以拿出自己日常觉得帮助很大的工具，大家一起进步。 闲话不多说，开搞。 ## Linux命令类 ###tail 最常用的tail -f tail -3 问题排查工具箱平时的工作中经常碰到很多疑难问题的处理，在解决问题的同时，有一些工具起到了相当大的作用，在此书写下来，一是作为笔记，可以让自己后续忘记了可快速翻阅，二是分享，希望看到此文的同学们可以拿出自己日常觉得帮助很大的工具，大家一起进步。 闲话不多说，开搞。 Linux命令类tail最常用的tail -f 1tail -300f shopbase.log #倒数300行并进入实时监听文件写入模式 grep12345678910grep forest f.txt #文件查找grep forest f.txt cpf.txt #多文件查找grep 'log' /home/admin -r -n #目录下查找所有符合关键字的文件cat f.txt | grep -i shopbase grep 'shopbase' /home/admin -r -n --include *.&#123;vm,java&#125; #指定文件后缀grep 'shopbase' /home/admin -r -n --exclude *.&#123;vm,java&#125; #反匹配seq 10 | grep 5 -A 3 #上匹配seq 10 | grep 5 -B 3 #下匹配seq 10 | grep 5 -C 3 #上下匹配，平时用这个就妥了cat f.txt | grep -c 'SHOPBASE' awk1 基础命令 123456awk '&#123;print $4,$6&#125;' f.txtawk '&#123;print NR,$0&#125;' f.txt cpf.txt awk '&#123;print FNR,$0&#125;' f.txt cpf.txtawk '&#123;print FNR,FILENAME,$0&#125;' f.txt cpf.txtawk '&#123;print FILENAME,"NR="NR,"FNR="FNR,"$"NF"="$NF&#125;' f.txt cpf.txtecho 1:2:3:4 | awk -F: '&#123;print $1,$2,$3,$4&#125;' 2 匹配 1234awk '/ldb/ &#123;print&#125;' f.txt #匹配ldbawk '!/ldb/ &#123;print&#125;' f.txt #不匹配ldbawk '/ldb/ &amp;&amp; /LISTEN/ &#123;print&#125;' f.txt #匹配ldb和LISTENawk '$5 ~ /ldb/ &#123;print&#125;' f.txt #第五列匹配ldb 3 内建变量 NR:NR表示从awk开始执行后，按照记录分隔符读取的数据次数，默认的记录分隔符为换行符，因此默认的就是读取的数据行数，NR可以理解为Number of Record的缩写。 FNR:在awk处理多个输入文件的时候，在处理完第一个文件后，NR并不会从1开始，而是继续累加，因此就出现了FNR，每当处理一个新文件的时候，FNR就从1开始计数，FNR可以理解为File Number of Record。 NF: NF表示目前的记录被分割的字段的数目，NF可以理解为Number of Field。 find12345678910111213sudo -u admin find /home/admin /tmp /usr -name \*.log(多个目录去找)find . -iname \*.txt(大小写都匹配)find . -type d(当前目录下的所有子目录)find /usr -type l(当前目录下所有的符号链接)find /usr -type l -name "z*" -ls(符号链接的详细信息 eg:inode,目录)find /home/admin -size +250000k(超过250000k的文件，当然+改成-就是小于了)find /home/admin f -perm 777 -exec ls -l &#123;&#125; \; (按照权限查询文件)find /home/admin -atime -1 1天内访问过的文件find /home/admin -ctime -1 1天内状态改变过的文件 find /home/admin -mtime -1 1天内修改过的文件find /home/admin -amin -1 1分钟内访问过的文件find /home/admin -cmin -1 1分钟内状态改变过的文件 find /home/admin -mmin -1 1分钟内修改过的文件 pgm批量查询vm-shopbase满足条件的日志 1pgm -A -f vm-shopbase 'cat /home/admin/shopbase/logs/shopbase.log.2017-01-17|grep 2069861630' tsartsar是咱公司自己的采集工具。很好用, 将历史收集到的数据持久化在磁盘上，所以我们快速来查询历史的系统数据。当然实时的应用情况也是可以查询的啦。大部分机器上都有安装。 1tsar ##可以查看最近一天的各项指标 1tsar --live ##可以查看实时指标，默认五秒一刷 1tsar -d 20161218 ##指定查看某天的数据，貌似最多只能看四个月的数据 1234tsar --memtsar --loadtsar --cpu##当然这个也可以和-d参数配合来查询某天的单个指标的情况 toptop除了看一些基本信息之外，剩下的就是配合来查询vm的各种问题了 12ps -ef | grep javatop -H -p pid 获得线程10进制转16进制后jstack去抓看这个线程到底在干啥 其他12netstat -nat|awk '&#123;print $6&#125;'|sort|uniq -c|sort -rn #查看当前连接，注意close_wait偏高的情况，比如如下 排查利器btrace首当其冲的要说的是btrace。真是生产环境&amp;预发的排查问题大杀器。 简介什么的就不说了。直接上代码干。 1、查看当前谁调用了ArrayList的add方法，同时只打印当前ArrayList的size大于500的线程调用栈 12345678910@OnMethod(clazz = "java.util.ArrayList", method="add", location = @Location(value = Kind.CALL, clazz = "/.*/", method = "/.*/"))public static void m(@ProbeClassName String probeClass, @ProbeMethodName String probeMethod, @TargetInstance Object instance, @TargetMethodOrField String method) &#123; if(getInt(field("java.util.ArrayList", "size"), instance) &gt; 479)&#123; println("check who ArrayList.add method:" + probeClass + "#" + probeMethod + ", method:" + method + ", size:" + getInt(field("java.util.ArrayList", "size"), instance)); jstack(); println(); println("==========================="); println(); &#125;&#125; 2、监控当前服务方法被调用时返回的值以及请求的参数 1234@OnMethod(clazz = "com.taobao.sellerhome.transfer.biz.impl.C2CApplyerServiceImpl", method="nav", location = @Location(value = Kind.RETURN))public static void mt(long userId, int current, int relation, String check, String redirectUrl, @Return AnyType result) &#123; println("parameter# userId:" + userId + ", current:" + current + ", relation:" + relation + ", check:" + check + ", redirectUrl:" + redirectUrl + ", result:" + result);&#125; 更多内容，感兴趣的请移步：https://github.com/btraceio/btrace 注意: 经过观察，1.3.9的release输出不稳定，要多触发几次才能看到正确的结果 正则表达式匹配trace类时范围一定要控制，否则极有可能出现跑满CPU导致应用卡死的情况 由于是字节码注入的原理，想要应用恢复到正常情况，需要重启应用。 GreysGreys是@杜琨的大作吧。说几个挺棒的功能(部分功能和btrace重合): sc -df xxx: 输出当前类的详情,包括源码位置和classloader结构 trace class method: 相当喜欢这个功能! 很早前可以早JProfiler看到这个功能。打印出当前方法调用的耗时情况，细分到每个方法。对排查方法性能时很有帮助，比如我之前这篇就是使用了trace命令来的:http://www.atatech.org/articles/52947。 其他功能部分和btrace重合，可以选用，感兴趣的请移步。http://www.atatech.org/articles/26247 另外相关联的是arthas，他是基于Greys的，感兴趣的再移步http://mw.alibaba-inc.com/products/arthas/docs/middleware-container/arthas.wiki/home.html?spm=a1z9z.8109794.header.32.1lsoMc javOSize就说一个功能classes：通过修改了字节码，改变了类的内容，即时生效。 所以可以做到快速的在某个地方打个日志看看输出，缺点是对代码的侵入性太大。但是如果自己知道自己在干嘛，的确是不错的玩意儿。 其他功能Greys和btrace都能很轻易做的到，不说了。 可以看看我之前写的一篇javOSize的简介http://www.atatech.org/articles/38546官网请移步http://www.javosize.com/ JProfiler之前判断许多问题要通过JProfiler，但是现在Greys和btrace基本都能搞定了。再加上出问题的基本上都是生产环境(网络隔离)，所以基本不怎么使用了，但是还是要标记一下。官网请移步https://www.ej-technologies.com/products/jprofiler/overview.html 大杀器eclipseMAT可作为eclipse的插件，也可作为单独的程序打开。详情请移步http://www.eclipse.org/mat/ zprofiler集团内的开发应该是无人不知无人不晓了。简而言之一句话:有了zprofiler还要mat干嘛详情请移步zprofiler.alibaba-inc.com java三板斧，噢不对，是七把jps我只用一条命令： 1sudo -u admin /opt/taobao/java/bin/jps -mlvV jstack普通用法: 1sudo -u admin /opt/taobao/install/ajdk-8_1_1_fp1-b52/bin/jstack 2815 native+java栈: 1sudo -u admin /opt/taobao/install/ajdk-8_1_1_fp1-b52/bin/jstack -m 2815 jinfo可看系统启动的参数，如下 1sudo -u admin /opt/taobao/install/ajdk-8_1_1_fp1-b52/bin/jinfo -flags 2815 jmap两个用途 1.查看堆的情况 1sudo -u admin /opt/taobao/install/ajdk-8_1_1_fp1-b52/bin/jmap -heap 2815 2.dump 1sudo -u admin /opt/taobao/install/ajdk-8_1_1_fp1-b52/bin/jmap -dump:live,format=b,file=/tmp/heap2.bin 2815 或者 1sudo -u admin /opt/taobao/install/ajdk-8_1_1_fp1-b52/bin/jmap -dump:format=b,file=/tmp/heap3.bin 2815 3.看看堆都被谁占了? 再配合zprofiler和btrace，排查问题简直是如虎添翼 1sudo -u admin /opt/taobao/install/ajdk-8_1_1_fp1-b52/bin/jmap -histo 2815 | head -10 jstatjstat参数众多，但是使用一个就够了 1sudo -u admin /opt/taobao/install/ajdk-8_1_1_fp1-b52/bin/jstat -gcutil 2815 1000 jdb时至今日，jdb也是经常使用的。jdb可以用来预发debug,假设你预发的java_home是/opt/taobao/java/，远程调试端口是8000.那么 1sudo -u admin /opt/taobao/java/bin/jdb -attach 8000 出现以上代表jdb启动成功。后续可以进行设置断点进行调试。具体参数可见oracle官方说明http://docs.oracle.com/javase/7/docs/technotes/tools/windows/jdb.html CHLSDBCHLSDB感觉很多情况下可以看到更好玩的东西，不详细叙述了。 查询资料听说jstack和jmap等工具就是基于它的。 1sudo -u admin /opt/taobao/java/bin/java -classpath /opt/taobao/java/lib/sa-jdi.jar sun.jvm.hotspot.CLHSDB 更详细的可见R大此贴http://rednaxelafx.iteye.com/blog/1847971 plugin of intellij ideakey promoter快捷键一次你记不住，多来几次你总能记住了吧？ maven helper分析maven依赖的好帮手。 VM options 你的类到底是从哪个文件加载进来的？ 12-XX:+TraceClassLoading结果形如[Loaded java.lang.invoke.MethodHandleImpl$Lazy from D:\programme\jdk\jdk8U74\jre\lib\rt.jar] 应用挂了输出dump文件 12-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/admin/logs/java.hprof集团的vm参数里边基本都有这个选项 jar包冲突把这个单独写个大标题不过分吧？每个人或多或少都处理过这种烦人的case。我特么下边这么多方案不信就搞不定你? mvn dependency:tree &gt; ~/dependency.txt打出所有依赖 mvn dependency:tree -Dverbose -Dincludes=groupId:artifactId只打出指定groupId和artifactId的依赖关系 -XX:+TraceClassLoadingvm启动脚本加入。在tomcat启动脚本中可见加载类的详细信息 -verbosevm启动脚本加入。在tomcat启动脚本中可见加载类的详细信息 greys:scgreys的sc命令也能清晰的看到当前类是从哪里加载过来的 tomcat-classloader-locate通过以下url可以获知当前类是从哪里加载的curl http://localhost:8006/classloader/locate?class=org.apache.xerces.xs.XSObject ALI-TOMCAT带给我们的惊喜(感谢@务观) 列出容器加载的jar列表 curl http://localhost:8006/classloader/jars 列出当前当当前类加载的实际jar包位置，解决类冲突时有用 curl http://localhost:8006/classloader/locate?class=org.apache.xerces.xs.XSObject 其他gprefhttp://www.atatech.org/articles/33317 dmesg如果发现自己的java进程悄无声息的消失了，几乎没有留下任何线索，那么dmesg一发，很有可能有你想要的。 1sudo dmesg|grep -i kill|less 去找关键字oom_killer。找到的结果类似如下: 12345[6710782.021013] java invoked oom-killer: gfp_mask=0xd0, order=0, oom_adj=0, oom_scoe_adj=0[6710782.070639] [&lt;ffffffff81118898&gt;] ? oom_kill_process+0x68/0x140 [6710782.257588] Task in /LXC011175068174 killed as a result of limit of /LXC011175068174 [6710784.698347] Memory cgroup out of memory: Kill process 215701 (java) score 854 or sacrifice child [6710784.707978] Killed process 215701, UID 679, (java) total-vm:11017300kB, anon-rss:7152432kB, file-rss:1232kB 以上表明，对应的java进程被系统的OOM Killer给干掉了，得分为854.解释一下OOM killer（Out-Of-Memory killer），该机制会监控机器的内存资源消耗。当机器内存耗尽前，该机制会扫描所有的进程（按照一定规则计算，内存占用，时间等），挑选出得分最高的进程，然后杀死，从而保护机器。 dmesg日志时间转换公式:log实际时间=格林威治1970-01-01+(当前时间秒数-系统启动至今的秒数+dmesg打印的log时间)秒数： 1date -d "1970-01-01 UTC `echo "$(date +%s)-$(cat /proc/uptime|cut -f 1 -d' ')+12288812.926194"|bc ` seconds" 剩下的，就是看看为什么内存这么大，触发了OOM-Killer了。 新技能getRateLimiter想要精细的控制QPS? 比如这样一个场景，你调用某个接口，对方明确需要你限制你的QPS在400之内你怎么控制？这个时候RateLimiter就有了用武之地。详情可移步http://ifeve.com/guava-ratelimiter/ 参考https://yq.aliyun.com/articles/69520?utm_content=m_10360#comment]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>类</tag>
        <tag>线程</tag>
        <tag>CPU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql Innodb 中的锁]]></title>
    <url>%2F2018%2F11%2F18%2FMysql-Innodb-%E4%B8%AD%E7%9A%84%E9%94%81%2F</url>
    <content type="text"><![CDATA[本文中介绍的Mysql 数据库版本是5.7，隔离级别是Repeatable-Read(可重复读)，不同的数据库版本和隔离级别对语句的执行结果影响很大。讨论锁的时候需指明版本和隔离级别。 数据库是一个多用户使用的共享资源。当多个用户并发地存取数据时，在数据库中就会产生多个事务同时存取同一数据的情况。若对并发操作不加控制就可能会读取和存储不正确的数据，破坏数据库的一致性。 锁是用于管理对公共资源的并发控制。也就是说并发的情况下，会出现资源竞争，所以需要加锁。 举个例子，转账操作。简单来说，张三给李四转账x元，可以分为三步： 先查询张三的账户余额y是否大于x张三的余额 y = y - x元李四的余额 x = z + x元 假设张三账户余额有1000元，李四余额也有1000元，如果不加锁的话，同时有两个请求，A要求转500元，B要求转600元，第一步查询余额都是足够的，第2步和第3步也能执行成功，但是最终结果却是错误，第二个请求可能会覆盖掉第一个请求。 这种问题叫做 丢失更新： 多个事务操作同一行，后面的更新覆盖了前面的更新值。需要在应用级别加锁来避免。 数据库有ACID原则，其中I是隔离性，标准SQL规范中定义了四种隔离级别，关于事务的隔离级别见这篇博客 。 越往下，隔离级别越高，问题越少，同时并发度也越低。隔离级别和并发度成反比的。 脏读：事务A读取了事务B未提交的数据不可重复读：对于一条记录，事务A两次读取的数据变了幻读：事务A按照相同的查询条件，读取到了新增的数据 MySQL中的隔离级别如下： 和标准SQL规范相比，MySQL中可重复读解决了幻读，实现了串行化隔离级别的功能，同时没有严重影响并发。是通过加锁、阻止插入新数据，来解决幻读的。 锁的分类 听说过读锁、写锁、共享锁、互斥锁、行锁等等各种名词，简单对这些锁进行了分类。 加锁机制乐观锁：先修改，保存时判断是够被更新过，应用级别悲观锁：先获取锁，再操作修改，数据库级别 锁粒度表级锁：开销小，加锁快，粒度大，锁冲突概率大，并发度低，适用于读多写少的情况。页级锁：BDB存储引擎行级锁：Innodb存储引擎，默认选项 兼容性S锁，也叫做读锁、共享锁，对应于我们常用的 select * from users where id =1 lock in share mode X锁，也叫做写锁、排它锁、独占锁、互斥锁，对应对于select * from users where id =1 for update 下面这个表格是锁冲突矩阵，可以看到只有读锁和读锁之间兼容的，写锁和读锁、写锁都是冲突的。 冲突的时候会阻塞当前会话，直到拿到锁或者超时 这里要提到的一点是，S锁 和 X锁是可以是表锁，也可以是行锁 锁模式先理解下索引组织表。Innodb中的索引数据结构是 B+ 树，数据是有序排列的，从根节点到叶子节点一层层找到对应的数据。普通索引，也叫做辅助索引，叶子节点存放的是主键值。主键上的索引叫做聚集索引，表里的每一条记录都存放在主键的叶子节点上。当通过辅助索引select 查询数据的时候，会先在辅助索引中找到对应的主键值，然后用主键值在聚集索引中找到该条记录。关于Mysql 中的索引见这篇博客 。 表中每一行的数据，是组织存放在聚集索引中的，所以叫做索引组织表。 了解索引数据结构的目的是为了说明，行锁是加在索引上的。 select * from user where id=10 for update 一条简单的SQL。在user表中查找id为10的记录，并用for update加X锁。 这里User表中，有3个字段， 主键id 和 另外一个字段name。下面的表格是B+树索引的简化表达。第一行id是索引的节点，第二行和第三行是这行记录，包含了姓名和性别。 如图所示，通过锁住聚集索引中的节点来锁住这条记录。 聚集索引上的锁，比较好理解，锁住id=10的索引，即锁住了这条记录。 select * from user where name=‘b’ for update 查询user表中name为d的记录，并用for update加X锁 这里的name上加了唯一索引，唯一索引本质上是辅助索引，加了唯一约束。所以会先在辅助索引上找到name为d的索引记录，在辅助索引中加锁，然后查找聚集索引，锁住对应索引记录。 为什么聚簇索引上的记录也要加锁？试想一下，如果有并发的另外一个SQL，是直接通过主键索引id=30来更新，会先在聚集索引中请求加锁。如果只在辅助索引中加锁的话，两个并发SQL之间是互相感知不到的。 select * from user where name=‘b’ for update 查询user表中name为b的记录，并用for update加X锁。这里name上加了普通的索引，不是唯一索引。普通索引的值是可以重复的。会先在辅助索引中找到name为b的两条记录，加X锁，然后得到主键值7和30，到聚集索引中加X锁。 事情并没有那么简单，如果这时有另一个事务，插入了name=b,id=40的记录，却发现是可以插入的。 位置在途中红色线条标注的间隙内，这样就会出现幻读，两次查询得到的结果是不一致的，第一次查到两条数据，插入之后得到三条数据。 为了防止这种情况，出现了另一种锁，gap lcok 间隙锁。锁住的是索引的间隙。 即图中，红色线条标识的空隙。因为新插入name=b的记录，可能出现在这三个间隙内。 这张图里出现了三种锁 记录锁：单行记录上的锁 间隙锁：锁定记录之间的范围，但不包含记录本身。 Next Key Lock: 记录锁+ 间隙锁，锁定一个范围，包含记录本身。 意向锁( Intention Locks )InnoDB为了支持多粒度(表锁与行锁)的锁并存，引入意向锁。意向锁是表级锁， IS: 意向共享锁IX: 意向排他锁 事务在请求某一行的S锁和X锁前，需要先获得对应表的IS、IX锁。 意向锁产生的主要目的是为了处理行锁和表锁之间的冲突，用于表明“某个事务正在某一行上持有了锁，或者准备去持有锁”。比如，表中的某一行上加了X锁，就不能对这张表加X锁。 意向锁和意向锁之间是完全兼容的，但是意向锁和共享锁以及排它锁可能是有互斥性的。因为意向锁的锁粒度是表级锁，所以在全表扫描是往往会对表加锁，那么此时就会发生锁冲突。 如果不在表上加意向锁，对表加锁的时候，都要去检查表中的某一行上是否加有行锁，多麻烦。 插入意向锁（Insert Intention Lock）Gap Lock中存在一种插入意向锁，在insert操作时产生。 有两个作用： 和next-key互斥，阻塞next-key 锁，防止插入数据，这样就不会幻读。 插入意向锁互相是兼容的，允许相同间隙、不同数据的并发插入 常见语句的加锁分析后面会有多个SQL语句，先说明一下表结构 1234567891011CREATE TABLE `user` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `id_no` varchar(255) DEFAULT NULL COMMENT &apos;身份证号&apos;, `name` varchar(255) DEFAULT NULL COMMENT &apos;姓名&apos;, `mobile` varchar(255) DEFAULT NULL COMMENT &apos;手机号&apos;, `age` int(11) DEFAULT NULL COMMENT &apos;年龄&apos;, `address` varchar(255) DEFAULT NULL COMMENT &apos;地址&apos;, PRIMARY KEY (`id`), UNIQUE KEY `uniq_id_no` (`id_no`), KEY `idx_name` (`name`)) ENGINE=InnoDB AUTO_INCREMENT=10002 DEFAULT CHARSET=utf8 COMMENT=&apos;用户表&apos;; 这里有一个user表，5个字段，其中id是主键，id_no是身份证号，加了唯一索引，name是用户姓名，可以重复的，加了普通索引，手机号、年龄、地址都没有索引。 普通select12345select * from user where id =1;begin;select * from user where id =1;commit: 普通的select 语句是不加锁的。select包裹在事务中，同样也是不加锁的。where后面的条件不管多少，普通的select是不加锁的。 显式加锁123select * from user where id =1 lock in share mode;select * from user where id =1 for update; 显式指出要加什么样的锁。上面一个加的是共享锁，下面的是互斥锁。 这里需要强调的一点，需要明确在事务中是用这些锁，不在事务中是没有意义的。 隐式加锁12update user set address &apos;北京&apos; where id=1;delete from user where id=1; update和delete也会对查询出的记录加X锁，隐式加互斥锁。加锁类型和for update 类似 后面只按照显式加锁的select for update 举例子，更新和删除的加锁方式是一样的。 按索引类型1234567select * from user where id =1 for update;select * from user where id_no =&apos;a22&apos; for update;select * from user where name =&apos;王二&apos; for update;select * from user where address =&apos;杭州&apos; for update; 四条SQL，区别在于where条件的过滤列，分别是主键、唯一索引、普通索引、无索引。 主键：之前提到过索引组织表，这里会在聚集索引上对查询出的记录，加X锁 唯一索引：会在辅助索引上，把在对应的id_no=a22的索引加X锁，因为是唯一的，所以不是next-key锁。然后在主键上，也会在这条记录上加X锁。 普通索引：因为不是唯一的，会在辅助索引上，把对应的id_no=a22的索引加next-key锁。然后在主键加X锁。 无索引：首先，是不推荐这种写法，没有索引的话，因为会全表扫描，数据量大的话查询会很慢。这里讨论的是，这种情况下，会加什么锁？ 答案： 首先，聚簇索引上的所有记录，都被加上了X锁。其次，聚簇索引每条记录间的间隙(GAP)，也同时被加上了GAP锁。在这种情况下，这个表上，除了不加锁的快照度，其他任何加锁的并发SQL，均不能执行，不能更新，不能删除，不能插入，全表被锁死。这是一个很恐怖的事情，请注意。 记录不存在的情况前面几个例子中，都是可以查到结果的。如果对应记录不存在会怎样？答案是锁住间隙，不允许插入。mysql要保证没有其他人可以插入，所以锁住间隙。 普通 insert 语句在插入之前，会先在插入记录所在的间隙加上一个插入意向锁。 insert会对插入成功的行加上排它锁，这个排它锁是个记录锁，而非next-key锁（当然更不是gap锁了），不会阻止其他并发的事务往这条记录之前插入 。 先查询后插入类似于这样的insert 12insert into target_table select * from source_table ...create target_table select * from source_table ... 将select查询的结果集，插入到另一张表中，或者使用结果集，创建一个新表。 和之前简单插入的情况类似，已插入成功的数据加X锁，间隙加上一个插入意向锁。 对于select的源表中的记录，会加共享的 next-key 锁。这是为了防止主从同步出问题。 分析锁的情况先说一下死锁的定义，死锁是指两个或两个以上的事务在执行过程中，因争夺资源而造成的一种互相等待的现象。这个定义适用于数据库，有几个重点，两个或两个以上的事务，一个事务是不会出现死锁的。争夺的资源一般都是表或者记录。 出现死锁了会怎样，正常情况下，mysql会检查出死锁，并回滚某一个事务，让另一个事务正常运行。 Mysql 会回滚副作用小的事务，判定的标准是执行的时间以及影响的范围。 查看发生过的锁 show status like ‘innodb_row_lock%’; 从系统启动到现在的数据 Innodb_row_lock_current_waits：当前正在等待锁的数量； Innodb_row_lock_time ：锁定的总时间长度，单位ms； Innodb_row_lock_time_avg ：每次等待所花平均时间； Innodb_row_lock_time_max：从系统启动到现在等待最长的一次所花的时间； Innodb_row_lock_waits ：从系统启动到现在总共等待的次数。 平均时间和锁等待次数比较大的话，说明可能会存在锁争用情况 show engine innodb status 展示innodb存储引擎的运行状态 通过这个命令显示的内容比较多，其中有一项lasted detected deadlock 显示最近发生的死锁。 图中红色线条标注的是执行的SQL，以及加了什么锁，可以看出是在这行记录上加了X锁，没有gap锁。 错误日志中查看历史发生过的死锁1set global innodb_print_all_deadlocks=1; 上一个命令，只能看到最近发生的锁，如果我想看历史发生的锁怎么办？ 执行这一句，更改innodb 的一个配置，innodb_print_all_deadlocks，打印所有的死锁。会将死锁的信息输出到mysql的错误日志中，默认是不输出，格式和show engine innodb status 是差不多的。 information_schema.innodb_locks information_schema 数据库是mysql自带的，保存着关于MySQL服务器所维护的所有其他数据库的信息。其中innodb_locks表，记录了事务请求但是还没获得的锁，即等待获得的锁。 lock_id：锁的id，由锁住的空间id编号、页编号、行编号组成 lock_trx_id：锁的事务id。 lock_mode：锁的模式。S[,GAP], X[,GAP], IS[,GAP], IX[,GAP] lock_type：锁的类型，表锁还是行锁 lock_table：要加锁的表。 lock_index：锁住的索引。 lock_space：innodb存储引擎表空间的id号码 lock_page：被锁住的页的数量，如果是表锁，则为null值。 lock_rec：被锁住的行的数量，如果表锁，则为null值。 lock_data：被锁住的行的主键值，如果表锁，则为null值。 information_schema.innodb_lock_waits 查看等待中的锁 requesting_trx_id：申请锁资源的事务id。 requested_lock_id：申请的锁的id。 blocking_trx_id：阻塞的事务id，当前拥有锁的事务ID。 blocking_lock_id：阻塞的锁的id，当前拥有锁的锁ID information_schema.innodb_trx 查看已开启的事务 trx_id：innodb存储引擎内部事务唯一的事务id。 trx_state：当前事务的状态。 trx_started：事务开始的时间。 trx_requested_lock_id：等待事务的锁id，如trx_state的状态为LOCK WAIT，那么该值代表当前事务之前占用锁资源的id，如果trx_state不是LOCK WAIT的话，这个值为null。 trx_wait_started：事务等待开始的时间。 trx_weight：事务的权重，反映了一个事务修改和锁住的行数。在innodb的存储引擎中，当发生死锁需要回滚时，innodb存储引擎会选择该值最小的事务进行回滚。 trx_mysql_thread_id：正在运行的mysql中的线程id，show full processlist显示的记录中的thread_id。 trx_query：事务运行的sql语句 预防死锁以相同的顺序更新不同的表，这样执行的话，会出现锁等待，但不容易出现死锁。 预先对数据进行排序 直接申请足够级别的锁，而非先共享锁，再申请排他锁。 事务的粒度及时间尽量保持小，这样锁冲突的概率就小了，也就不容易出现死锁。不建议在数据库的事务中执行API调用。 正确加索引。没有索引会引起全表扫描，类似于锁表。 总结正确的加索引，尽量先查询，然后使用主键去加锁，等于操作来加锁，而尽量避免辅助索引，或者不是范围比较来加锁。 出现了锁的问题，根据数据库已有的信息，分析死锁。 举了几个例子，可能很多都是上线之后才发现的，最好能在开发阶段就避免死锁。 参考https://www.jianshu.com/p/edbe22beaecb https://zhuanlan.zhihu.com/p/31875702]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>事务</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 持久化]]></title>
    <url>%2F2018%2F11%2F11%2FRedis-%E6%8C%81%E4%B9%85%E5%8C%96%2F</url>
    <content type="text"><![CDATA[Redis 提供了多种不同级别的持久化方式： RDB 持久化可以在指定的时间间隔内生成数据集的时间点快照（point-in-time snapshot）。 AOF 持久化记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。 AOF 文件中的命令全部以 Redis 协议的格式来保存，新命令会被追加到文件的末尾。 Redis 还可以在后台对 AOF 文件进行重写（rewrite），使得 AOF 文件的体积不会超出保存数据集状态所需的实际大小。 Redis 还可以同时使用 AOF 持久化和 RDB 持久化。 在这种情况下， 当 Redis 重启时， 它会优先使用 AOF 文件来还原数据集， 因为 AOF 文件保存的数据集通常比 RDB 文件所保存的数据集更完整。 甚至可以关闭持久化功能，让数据只在服务器运行时存在。 了解 RDB 持久化和 AOF 持久化之间的异同是非常重要的， 以下几个小节将详细地介绍这这两种持久化功能， 并对它们的相同和不同之处进行说明。 RDBRDB 快照在默认情况下， Redis 将数据库快照保存在名字为 dump.rdb 的二进制文件中。 你可以对 Redis 进行设置， 让它在“ N 秒内数据集至少有 M 个改动”这一条件被满足时， 自动保存一次数据集。 你也可以通过调用 SAVE 或者 BGSAVE ， 手动让 Redis 进行数据集保存操作。 比如说， 以下设置会让 Redis 在满足“ 60 秒内有至少有 1000 个键被改动”这一条件时， 自动保存一次数据集： 1save 60 1000 这种持久化方式被称为快照（snapshot）。 快照的运作方式当 Redis 需要保存 dump.rdb 文件时， 服务器执行以下操作： Redis 调用 fork() ，同时拥有父进程和子进程。 子进程将数据集写入到一个临时 RDB 文件中。 当子进程完成对新 RDB 文件的写入时，Redis 用新 RDB 文件替换原来的 RDB 文件，并删除旧的 RDB 文件。 这种工作方式使得 Redis 可以从写时复制（copy-on-write）机制中获益。 RDB 的优点 RDB 是一个非常紧凑（compact）的文件，它保存了 Redis 在某个时间点上的数据集。 这种文件非常适合用于进行备份： 比如说，你可以在最近的 24 小时内，每小时备份一次 RDB 文件，并且在每个月的每一天，也备份一个 RDB 文件。 这样的话，即使遇上问题，也可以随时将数据集还原到不同的版本。 RDB 非常适用于灾难恢复（disaster recovery）：它只有一个文件，并且内容都非常紧凑，可以（在加密后）将它传送到别的数据中心，或者亚马逊 S3 中。 RDB 可以最大化 Redis 的性能：父进程在保存 RDB 文件时唯一要做的就是 fork 出一个子进程，然后这个子进程就会处理接下来的所有保存工作，父进程无须执行任何磁盘 I/O 操作。 RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 RDB 的缺点 如果你需要尽量避免在服务器故障时丢失数据，那么 RDB 不适合你。 虽然 Redis 允许你设置不同的保存点（save point）来控制保存 RDB 文件的频率， 但是， 因为RDB 文件需要保存整个数据集的状态， 所以它并不是一个轻松的操作。 因此你可能会至少 5 分钟才保存一次 RDB 文件。 在这种情况下， 一旦发生故障停机， 你就可能会丢失好几分钟的数据。 每次保存 RDB 的时候，Redis 都要 fork() 出一个子进程，并由子进程来进行实际的持久化工作。 在数据集比较庞大时， fork() 可能会非常耗时，造成服务器在某某毫秒内停止处理客户端； 如果数据集非常巨大，并且 CPU 时间非常紧张的话，那么这种停止时间甚至可能会长达整整一秒。 虽然 AOF 重写也需要进行 fork() ，但无论 AOF 重写的执行间隔有多长，数据的耐久性都不会有任何损失。 AOF（append-only file）快照功能并不是非常耐久（durable）： 如果 Redis 因为某些原因而造成故障停机， 那么服务器将丢失最近写入、且仍未保存到快照中的那些数据。 尽管对于某些程序来说， 数据的耐久性并不是最重要的考虑因素， 但是对于那些追求完全耐久能力（full durability）的程序来说， 快照功能就不太适用了。 从 1.1 版本开始， Redis 增加了一种完全耐久的持久化方式： AOF 持久化。 你可以通过修改配置文件来打开 AOF 功能： 1appendonly yes 从现在开始， 每当 Redis 执行一个改变数据集的命令时（比如 SET）， 这个命令就会被追加到 AOF 文件的末尾。 这样的话， 当 Redis 重新启时， 程序就可以通过重新执行 AOF 文件中的命令来达到重建数据集的目的。 AOF 重写因为 AOF 的运作方式是不断地将命令追加到文件的末尾， 所以随着写入命令的不断增加， AOF 文件的体积也会变得越来越大。 举个例子， 如果你对一个计数器调用了 100 次 INCR ， 那么仅仅是为了保存这个计数器的当前值， AOF 文件就需要使用 100 条记录（entry）。 然而在实际上， 只使用一条 SET 命令已经足以保存计数器的当前值了， 其余 99 条记录实际上都是多余的。 为了处理这种情况， Redis 支持一种有趣的特性： 可以在不打断服务客户端的情况下， 对 AOF 文件进行重建（rebuild）。 执行 BGREWRITEAOF 命令， Redis 将生成一个新的 AOF 文件， 这个文件包含重建当前数据集所需的最少命令。 Redis 2.2 需要自己手动执行 BGREWRITEAOF 命令； Redis 2.4 则可以自动触发 AOF 重写， 具体信息请查看 2.4 的示例配置文件。 AOF 有多耐久？你可以配置 Redis 多久才将数据 fsync 到磁盘一次。 有三个选项： 每次有新命令追加到 AOF 文件时就执行一次 fsync ：非常慢，也非常安全。 每秒 fsync 一次：足够快（和使用 RDB 持久化差不多），并且在故障时只会丢失 1 秒钟的数据。 从不 fsync ：将数据交给操作系统来处理。更快，也更不安全的选择。 推荐（并且也是默认）的措施为每秒 fsync 一次， 这种 fsync 策略可以兼顾速度和安全性。 总是 fsync 的策略在实际使用中非常慢， 即使在 Redis 2.0 对相关的程序进行了改进之后仍是如此 —— 频繁调用 fsync 注定了这种策略不可能快得起来。 如果 AOF 文件出错了，怎么办？服务器可能在程序正在对 AOF 文件进行写入时停机， 如果停机造成了 AOF 文件出错（corrupt）， 那么 Redis 在重启时会拒绝载入这个 AOF 文件， 从而确保数据的一致性不会被破坏。 当发生这种情况时， 可以用以下方法来修复出错的 AOF 文件： 为现有的 AOF 文件创建一个备份。 使用 Redis 附带的 redis-check-aof 程序，对原来的 AOF 文件进行修复。 12&gt; $ redis-check-aof --fix&gt; （可选）使用 diff -u 对比修复后的 AOF 文件和原始 AOF 文件的备份，查看两个文件之间的不同之处。 重启 Redis 服务器，等待服务器载入修复后的 AOF 文件，并进行数据恢复。 AOF 的运作方式AOF 重写和 RDB 创建快照一样，都巧妙地利用了写时复制机制。 以下是 AOF 重写的执行步骤： Redis 执行 fork() ，现在同时拥有父进程和子进程。 子进程开始将新 AOF 文件的内容写入到临时文件。 对于所有新执行的写入命令，父进程一边将它们累积到一个内存缓存中，一边将这些改动追加到现有 AOF 文件的末尾： 这样即使在重写的中途发生停机，现有的 AOF 文件也还是安全的。 当子进程完成重写工作时，它给父进程发送一个信号，父进程在接收到信号之后，将内存缓存中的所有数据追加到新 AOF 文件的末尾。 搞定！现在 Redis 原子地用新文件替换旧文件，之后所有命令都会直接追加到新 AOF 文件的末尾。 AOF 的优点 使用 AOF 持久化会让 Redis 变得非常耐久（much more durable）：你可以设置不同的 fsync 策略，比如无 fsync ，每秒钟一次 fsync ，或者每次执行写入命令时 fsync 。 AOF 的默认策略为每秒钟 fsync 一次，在这种配置下，Redis 仍然可以保持良好的性能，并且就算发生故障停机，也最多只会丢失一秒钟的数据（ fsync 会在后台线程执行，所以主线程可以继续努力地处理命令请求）。 AOF 文件是一个只进行追加操作的日志文件（append only log）， 因此对 AOF 文件的写入不需要进行 seek ， 即使日志因为某些原因而包含了未写入完整的命令（比如写入时磁盘已满，写入中途停机，等等）， redis-check-aof 工具也可以轻易地修复这种问题。 Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写： 重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。 整个重写操作是绝对安全的，因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。 而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。 AOF 文件有序地保存了对数据库执行的所有写入操作， 这些写入操作以 Redis 协议的格式保存， 因此 AOF 文件的内容非常容易被人读懂， 对文件进行分析（parse）也很轻松。 导出（export） AOF 文件也非常简单： 举个例子， 如果你不小心执行了 FLUSHALL 命令， 但只要 AOF 文件未被重写， 那么只要停止服务器， 移除 AOF 文件末尾的 FLUSHALL 命令， 并重启 Redis ， 就可以将数据集恢复到 FLUSHALL 执行之前的状态。 AOF 的缺点 对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。 根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB 。 在一般情况下， 每秒 fsync 的性能依然非常高， 而关闭 fsync 可以让 AOF 的速度和 RDB 一样快， 即使在高负荷之下也是如此。 不过在处理巨大的写入载入时，RDB 可以提供更有保证的最大延迟时间（latency）。 AOF 在过去曾经发生过这样的 bug ： 因为个别命令的原因，导致 AOF 文件在重新载入时，无法将数据集恢复成保存时的原样。 （举个例子，阻塞命令 BRPOPLPUSH 就曾经引起过这样的 bug 。） 测试套件里为这种情况添加了测试： 它们会自动生成随机的、复杂的数据集， 并通过重新载入这些数据来确保一切正常。 虽然这种 bug 在 AOF 文件中并不常见， 但是对比来说， RDB 几乎是不可能出现这种 bug 的。 RDB与AOFRDB 和 AOF ，应该用哪一个？一般来说， 如果想达到足以媲美 PostgreSQL 的数据安全性， 你应该同时使用两种持久化功能。 如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失， 那么你可以只使用 RDB 持久化。 有很多用户都只使用 AOF 持久化， 但我们并不推荐这种方式： 因为定时生成 RDB 快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度要快， 除此之外， 使用 RDB 还可以避免之前提到的 AOF 程序的 bug 。 因为以上提到的种种原因， 未来 AOF 和 RDB可能会整合成单个持久化模型。 （这是一个长期计划。） 怎么从 RDB 持久化切换到 AOF 持久化在 Redis 2.2 或以上版本，可以在不重启的情况下，从 RDB 切换到 AOF ： 为最新的 dump.rdb 文件创建一个备份。 将备份放到一个安全的地方。 执行以下两条命令： 1234&gt; redis-cli&gt; CONFIG SET appendonly yes&gt; &gt; redis-cli&gt; CONFIG SET save ""&gt; 确保命令执行之后，数据库的键的数量没有改变。 确保写命令会被正确地追加到 AOF 文件的末尾。 步骤 3 执行的第一条命令开启了 AOF 功能： Redis 会阻塞直到初始 AOF 文件创建完成为止， 之后 Redis 会继续处理命令请求， 并开始将写入命令追加到 AOF 文件末尾。 步骤 3 执行的第二条命令用于关闭 RDB 功能。 这一步是可选的， 如果你愿意的话， 也可以同时使用 RDB 和 AOF 这两种持久化功能。 别忘了在 redis.conf 中打开 AOF 功能！ 否则的话， 服务器重启之后， 之前通过 CONFIG SET 设置的配置就会被遗忘， 程序会按原来的配置来启动服务器。 译注： 原文这里还有介绍 2.0 版本的切换方式， 考虑到 2.0 已经很老旧了， 这里省略了对那部分文档的翻译， 有需要的请参考原文。 RDB 和 AOF 之间的相互作用在版本号大于等于 2.4 的 Redis 中， BGSAVE 执行的过程中， 不可以执行 BGREWRITEAOF 。 反过来说， 在 BGREWRITEAOF 执行的过程中， 也不可以执行 BGSAVE 。 这可以防止两个 Redis 后台进程同时对磁盘进行大量的 I/O 操作。 如果 BGSAVE 正在执行， 并且用户显示地调用 BGREWRITEAOF 命令， 那么服务器将向用户回复一个 OK 状态， 并告知用户，BGREWRITEAOF 已经被预定执行： 一旦 BGSAVE 执行完毕， BGREWRITEAOF 就会正式开始。 当 Redis 启动时， 如果 RDB 持久化和 AOF 持久化都被打开了， 那么程序会优先使用 AOF 文件来恢复数据集， 因为 AOF 文件所保存的数据通常是最完整的。 Redis备份备份 Redis 数据在阅读这个小节前， 先将下面这句话铭记于心： 一定要备份你的Redis数据库！ 磁盘故障， 节点失效， 诸如此类的问题都可能让你的数据消失不见， 不进行备份是非常危险的。 Redis 对于数据备份是非常友好的， 因为你可以在服务器运行的时候对 RDB 文件进行复制： RDB 文件一旦被创建， 就不会进行任何修改。 当服务器要创建一个新的 RDB 文件时， 它先将文件的内容保存在一个临时文件里面， 当临时文件写入完毕时， 程序才使用 rename(2) 原子地用临时文件替换原来的 RDB 文件。 这也就是说， 无论何时， 复制 RDB 文件都是绝对安全的。 以下是我们的建议： 创建一个定期任务（cron job）， 每小时将一个 RDB 文件备份到一个文件夹， 并且每天将一个 RDB 文件备份到另一个文件夹。 确保快照的备份都带有相应的日期和时间信息， 每次执行定期任务脚本时， 使用 find 命令来删除过期的快照： 比如说， 你可以保留最近 48 小时内的每小时快照， 还可以保留最近一两个月的每日快照。 至少每天一次， 将 RDB 备份到你的数据中心之外， 或者至少是备份到你运行 Redis 服务器的物理机器之外。 容灾备份Redis 的容灾备份基本上就是对数据进行备份， 并将这些备份传送到多个不同的外部数据中心。 容灾备份可以在 Redis 运行并产生快照的主数据中心发生严重的问题时， 仍然让数据处于安全状态。 因为很多 Redis 用户都是创业者， 他们没有大把大把的钱可以浪费， 所以下面介绍的都是一些实用又便宜的容债备份方法： Amazon S3 ，以及其他类似 S3 的服务，是一个构建灾难备份系统的好地方。 最简单的方法就是将你的每小时或者每日 RDB 备份加密并传送到 S3 。 对数据的加密可以通过 gpg -c 命令来完成（对称加密模式）。 记得把你的密码放到几个不同的、安全的地方去（比如你可以把密码复制给你组织里最重要的人物）。 同时使用多个储存服务来保存数据文件，可以提升数据的安全性。 传送快照可以使用 SCP 来完成（SSH 的组件）。 以下是简单并且安全的传送方法： 买一个离你的数据中心非常远的 VPS ， 装上 SSH ， 创建一个无口令的 SSH 客户端 key ， 并将这个 key 添加到 VPS 的 authorized_keys 文件中， 这样就可以向这个 VPS 传送快照备份文件了。 为了达到最好的数据安全性，至少要从两个不同的提供商那里各购买一个 VPS 来进行数据容灾备份。 需要注意的是， 这类容灾系统如果没有小心地进行处理的话， 是很容易失效的。 最低限度下， 你应该在文件传送完毕之后， 检查所传送备份文件的体积和原始快照文件的体积是否相同。 如果你使用的是 VPS ， 那么还可以通过比对文件的 SHA1 校验和来确认文件是否传送完整。 另外， 你还需要一个独立的警报系统， 让它在负责传送备份文件的传送器（transfer）失灵时通知你。 参考http://doc.redisfans.com/topic/persistence.html https://redis.io/topics/persistence]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>内存</tag>
        <tag>Redis</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发之Condition]]></title>
    <url>%2F2018%2F11%2F11%2FJava%E5%B9%B6%E5%8F%91%E4%B9%8BCondition%2F</url>
    <content type="text"><![CDATA[在java中，对于任意一个java对象，它都拥有一组定义在java.lang.Object上监视器方法，包括wait()，wait(long timeout)，notify()，notifyAll()，这些方法配合synchronized关键字一起使用可以实现等待/通知模式。 wait()、notify()和notifyAll()方法是本地方法，并且为final方法，无法被重写。 调用某个对象的wait()方法能让当前线程阻塞，并且当前线程必须拥有此对象的monitor（即锁） 调用某个对象的notify()方法能够唤醒一个正在等待这个对象的monitor的线程，如果有多个线程都在等待这个对象的monitor，则只能唤醒其中一个线程； 调用notifyAll()方法能够唤醒所有正在等待这个对象的monitor的线程； notify()和notifyAll()方法只是唤醒等待该对象的monitor的线程，并不决定哪个线程能够获取到monitor。 同样，Condition接口也提供了类似Object监视器的方法，通过与Lock配合来实现等待/通知模式。 为了更好的了解Condition的特性，我们来对比一下两者的使用方式以及功能特性： 对比项 Object监视器 Condition 前置条件 获取对象的锁 调用Lock.lock获取锁，调用Lock.newCondition获取Condition对象 调用方式 直接调用，比如object.notify() 直接调用，比如condition.await() 等待队列的个数 一个 多个 当前线程释放锁进入等待状态 支持 支持 当前线程释放锁进入等待状态，在等待状态中不响应中断 不支持 支持 当前线程释放锁并进入超时等待状态 支持 支持 当前线程释放锁并进入等待状态直到将来的某个时间 不支持 支持 唤醒等待队列中的一个线程 支持 支持 唤醒等待队列中的全部线程 支持 支持 Condition的概念synchronized关键字，它配合Object的wait()、notify()系列方法可以实现等待/通知模式。 对于Lock，通过Condition也可以实现等待/通知模式。 AQS同步器维护了一个同步队列，其实还维护了多个等待队列，两种队列均为FIFO队列 Condition是一个接口，Condition接口的实现类是Lock（AQS）中的ConditionObject。 Lock接口中有个 newCondition()方法，通过这个方法可以获得Condition对象（其实就是ConditionObject）。因此，通过Lock对象可以获得Condition对象。 123Lock lock = new ReentrantLock();Condition c1 = lock.newCondition();Condition c2 = lock.newCondition(); Condition的实现分析实现ConditionObject类是AQS的内部类，实现了Condition接口。 1234public class ConditionObject implements Condition, java.io.Serializable &#123; private transient Node firstWaiter; private transient Node lastWaiter; ... 可以看到，等待队列和同步队列一样，使用的都是同步器AQS中的节点类Node。同样拥有首节点和尾节点，每个Condition对象都包含着一个FIFO队列。结构图： 等待调用 Condition 的 await() 方法会使线程进入等待队列，并释放锁，线程状态变为等待状态。 12345678910111213141516171819202122public final void await() throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); Node node = addConditionWaiter(); //释放同步状态（锁） int savedState = fullyRelease(node); int interruptMode = 0; //判断节点是否放入同步对列 while (!isOnSyncQueue(node)) &#123; //阻塞 LockSupport.park(this); //如果已经中断了，则退出 if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); &#125; 分析上述方法的大概过程： 将当前线程创建为节点，加入等待队列。 释放锁，唤醒同步队列中的后继节点。 while 循环判断节点是否放入同步队列；如果没有放入则阻塞继续 while 循环（如果已经中断则退出）；如果放入则退出 while 循环执行后面的判断。 退出 while 说明节点已经在同步队列中，调用 acquireQueued() 方法加入同步状态竞争。 竞争到锁后从 await() 方法返回，即退出该方法。 addConditionWaiter() 方法： 12345678910111213141516private Node addConditionWaiter() &#123; Node t = lastWaiter; if (t != null &amp;&amp; t.waitStatus != Node.CONDITION) &#123; //清除条件队列中所有状态不为Condition的节点 unlinkCancelledWaiters(); t = lastWaiter; &#125; //将该线程创建节点，放入等待队列 Node node = new Node(Thread.currentThread(), Node.CONDITION); if (t == null) firstWaiter = node; else t.nextWaiter = node; lastWaiter = node; return node; &#125; 过程分析：同步队列的首节点移动到等待队列。加入尾节点之前会清除所有状态不为 Condition 的节点。 通知调用 Condition 的 signal() 方法可以唤醒等待队列的首节点（等待时间最长），唤醒之前会将该节点移动到同步队列中。 12345678public final void signal() &#123; //判断是否获取了锁 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignal(first); &#125; 过程： 先判断当前线程是否获取了锁。 然后对首节点调用 doSignal() 方法。 12345678private void doSignal(Node first) &#123; do &#123; if ( (firstWaiter = first.nextWaiter) == null) lastWaiter = null; first.nextWaiter = null; &#125; while (!transferForSignal(first) &amp;&amp; (first = firstWaiter) != null); &#125; 过程： 修改首节点。 调用 transferForSignal() 方法将节点移动到同步队列。 123456789101112final boolean transferForSignal(Node node) &#123; //将节点状态变为0 if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; //将该节点加入同步队列 Node p = enq(node); int ws = p.waitStatus; //如果结点p的状态为cancel 或者修改waitStatus失败，则直接唤醒 if (ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) LockSupport.unpark(node.thread); return true; &#125; 调用同步器的 enq 方法，将节点移动到同步队列，满足条件后使用 LockSupport 唤醒该线程。 当 Condition 调用 signalAll() 方法： 1234567891011121314151617public final void signalAll() &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignalAll(first);&#125;private void doSignalAll(Node first) &#123; lastWaiter = firstWaiter = null; do &#123; Node next = first.nextWaiter; first.nextWaiter = null; transferForSignal(first); first = next; &#125; while (first != null);&#125; 可以看到 doSignalAll() 方法使用了 do-while 循环来唤醒每一个等待队列中的节点，直到 first 为 null 时停止循环。 一句话总结 signalAll() 的作用：将等待队列中的全部节点移动到同步队列中，并唤醒每个节点的线程。 Conditon中的await()对应Object的wait()； Condition中的signal()对应Object的notify()； Condition中的signalAll()对应Object的notifyAll()。 总结整个过程可以分为三步： 第一步：一个线程获取锁后，通过调用 Condition 的 await() 方法，会将当前线程先加入到等待队列中，并释放锁。然后就在 await() 中的一个 while 循环中判断节点是否已经在同步队列，是则尝试获取锁，否则一直阻塞。 第二步：当线程调用 signal() 方法后，程序首先检查当前线程是否获取了锁，然后通过 doSignal(Node first) 方法将节点移动到同步队列，并唤醒节点中的线程。 第三步：被唤醒的线程，将从 await() 中的 while 循环中退出来，然后调用 acquireQueued() 方法竞争同步状态。竞争成功则退出 await() 方法继续执行。 参考https://juejin.im/post/5b69a5a151882563522b7e42 https://www.cnblogs.com/csuwater/p/5411693.html http://www.importnew.com/30150.html https://www.jb51.net/article/134496.htm]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>线程</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异步任务——FutureTask]]></title>
    <url>%2F2018%2F11%2F05%2F%E5%BC%82%E6%AD%A5%E4%BB%BB%E5%8A%A1%E2%80%94%E2%80%94FutureTask%2F</url>
    <content type="text"><![CDATA[Runnable、Callable、Future、FutureTask和Java异步打交道就不能回避掉Runnable，Callable，Future，FutureTask等类，首先来介绍下这几个类的区别。 RunnableRunnable接口是我们最熟悉的，它只有一个run函数。然后使用某个线程去执行该runnable即可实现多线程，Thread类在调用start()函数后就是执行的是Runnable的run()函数。Runnable最大的缺点在于run函数没有返回值。 CallableCallable接口和Runnable接口类似，它有一个call函数。使用某个线程执行Callable接口实质就是执行其call函数。call方法和run方法最大的区别就是call方法有返回值： 123456789public interface Callable&lt;V&gt; &#123; /** * Computes a result, or throws an exception if unable to do so. * * @return computed result * @throws Exception if unable to compute a result */ V call() throws Exception;&#125; Callable 接口有返回值，泛型 V 就是要返回的结果类型，可以返回子任务的执行结果。 Future接口Future 接口表示异步计算的结果，通过 Future 接口提供的方法，可以很方便的查询异步计算任务是否执行完成，获取异步计算的结果，取消未执行的异步任务，或者中断异步任务的执行，接口定义如下： 12345678910111213public interface Future&lt;V&gt; &#123; boolean cancel(boolean mayInterruptIfRunning); boolean isCancelled(); boolean isDone(); V get() throws InterruptedException, ExecutionException; V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; cancel(boolean mayInterruptIfRunning)：取消子任务的执行，如果这个子任务已经执行结束，或者已经被取消，或者不能被取消，这个方法就会执行失败并返回false；如果子任务还没有开始执行，那么子任务会被取消，不会再被执行；如果子任务已经开始执行了，但是还没有执行结束，根据mayInterruptIfRunning的值，如果mayInterruptIfRunning = true，那么会中断执行任务的线程，然后返回true，如果参数为false，会返回true，不会中断执行任务的线程。 需要注意，这个方法执行结束，返回结果之后，再调用isDone()会返回true。 isCancelled()：判断任务是否被取消，如果任务执行结束（正常执行结束和发生异常结束，都算执行结束）前被取消，也就是调用了cancel()方法，并且cancel()返回true，则该方法返回true，否则返回false。 isDone()：判断任务是否执行结束，正常执行结束，或者发生异常结束，或者被取消，都属于结束，该方法都会返回true. V get()：获取结果，如果这个计算任务还没有执行结束，该调用线程会进入阻塞状态。如果计算任务已经被取消，调用get()会抛出CancellationException，如果计算过程中抛出异常，该方法会抛出ExecutionException，如果当前线程在阻塞等待的时候被中断了，该方法会抛出InterruptedException。 V get(long timeout, TimeUnit unit)：带超时限制的get()，等待超时之后，该方法会抛出TimeoutException，如果子任务执行结束了，但是超时时间还没有到，这个方法也会返回结果。 FutureTaskFuture只是一个接口，在实际使用过程中，诸如ThreadPoolExecutor返回的都是一个FutureTask实例。 123public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; FutureTask可以像Runnable一下，封装异步任务，然后提交给Thread或线程池执行，然后获取任务执行结果。原因在于 FutureTask 实现了 RunnableFuture 接口，RunnableFuture是什么呢，其实就是 Runnable 和 Future 的结合，实现了 Runnable 和 Future 两个接口。 FutureTask的成员变量123456789101112131415161718192021222324252627public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; &#123; /* * FutureTask中定义了一个state变量，用于记录任务执行的相关状态 ，状态的变化过程如下 * NEW -&gt; COMPLETING -&gt; NORMAL * NEW -&gt; COMPLETING -&gt; EXCEPTIONAL * NEW -&gt; CANCELLED * NEW -&gt; INTERRUPTING -&gt; INTERRUPTED */ private volatile int state; //主流程状态 private static final int NEW = 0; //当FutureTask实例刚刚创建到callbale的call方法执行完成前，处于此状态 private static final int COMPLETING = 1; //callable的call方法执行完成或出现异常时，首先进行此状态 private static final int NORMAL = 2;//callable的call方法正常结束时，进入此状态，将outcom设置为正常结果 private static final int EXCEPTIONAL = 3;//callable的call方法异常结束时，进入此状态，将outcome设置为抛出的异常 //取消任务执行时可能处于的状态 private static final int CANCELLED= 4;// FutureTask任务尚未执行，即还在任务队列的时候，调用了cancel方法，进入此状态 private static final int INTERRUPTING = 5;// FutureTask的run方法已经在执行，收到中断信号，进入此状态 private static final int INTERRUPTED = 6;// 任务成功中断后，进入此状态 private Callable&lt;V&gt; callable;//需要执行的任务，提示：如果提交的是Runnable对象，会先转换为Callable对象，这是构造方法参数 private Object outcome; //任务运行的结果 private volatile Thread runner;//执行此任务的线程 //等待该FutureTask的线程链表，对于同一个FutureTask，如果多个线程调用了get方法，对应的线程都会加入到waiters链表中，同时当FutureTask执行完成后，也会告知所有waiters中的线程 private volatile WaitNode waiters; ......&#125; private volatile int state，state用来标识当前任务的运行状态。FutureTask的所有方法都是围绕这个状态进行的，需要注意，这个值用volatile（易变的）来标记，如果有多个子线程在执行FutureTask，那么它们看到的都会是同一个state，有如下几个值： NEW：表示这是一个新的任务，或者还没有执行完的任务，是初始状态。COMPLETING：表示任务执行结束（正常执行结束，或者发生异常结束），但是还没有将结果保存到outcome中。是一个中间状态。NORMAL：表示任务正常执行结束，并且已经把执行结果保存到outcome字段中。是一个最终状态。EXCEPTIONAL：表示任务发生异常结束，异常信息已经保存到outcome中，这是一个最终状态。CANCELLED：任务在新建之后，执行结束之前被取消了，但是不要求中断正在执行的线程，也就是调用了cancel(false)，任务就是CANCELLED状态，这时任务状态变化是NEW -&gt; CANCELLED。INTERRUPTING：任务在新建之后，执行结束之前被取消了，并要求中断线程的执行，也就是调用了cancel(true)，这时任务状态就是INTERRUPTING。这是一个中间状态。INTERRUPTED：调用cancel(true)取消异步任务，会调用interrupt()中断线程的执行，然后状态会从INTERRUPTING变到INTERRUPTED。 状态变化有如下4种情况： NEW -&gt; COMPLETING -&gt; NORMAL ————————————— 正常执行结束的流程 NEW -&gt; COMPLETING -&gt; EXCEPTIONAL ———————执行过程中出现异常的流程 NEW -&gt; CANCELLED ——————————————-被取消，即调用了cancel(false) NEW -&gt; INTERRUPTING -&gt; INTERRUPTED ————-被中断，即调用了cancel(true) FutureTask的构造方法123456789101112131415161718192021222324252627282930313233//接受Callable对象作为参数public FutureTask(Callable&lt;V&gt; callable) &#123; if (callable == null) throw new NullPointerException(); this.callable = callable; this.state = NEW; &#125;//接受Runnable对象作为参数public FutureTask(Runnable runnable, V result) &#123; this.callable = Executors.callable(runnable, result);//将Runnable转为Callable对象 this.state = NEW; &#125;//callable方法，将Runnable转为一个Callable对象，包装设计模式public static &lt;T&gt; Callable&lt;T&gt; callable(Runnable task, T result) &#123; if (task == null) throw new NullPointerException(); return new RunnableAdapter&lt;T&gt;(task, result);&#125; //RunnableAdapter是Executors的一个内部类，实现了Callable接口static final class RunnableAdapter&lt;T&gt; implements Callable&lt;T&gt; &#123; final Runnable task; final T result; RunnableAdapter(Runnable task, T result) &#123; this.task = task; this.result = result; &#125; public T call() &#123; task.run(); return result; &#125;&#125; 可以看到，构造FutureTask时，无论传入的是Runnable还是Callable，最终都实现了Callable接口。 FutureTask的执行过程run12345678910111213141516171819202122232425262728293031public void run() &#123; //保证callable任务只被运行一次 if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return; try &#123; Callable&lt;V&gt; c = callable; if (c != null &amp;&amp; state == NEW) &#123; V result; boolean ran; try &#123; //执行任务 result = c.call(); ran = true; &#125; catch (Throwable ex) &#123; result = null; ran = false; setException(ex); &#125; if (ran) set(result); &#125; &#125; finally &#123; runner = null; int s = state; //判断该任务是否正在响应中断，如果中断没有完成，则等待中断操作完成 if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); &#125;&#125; 如果状态不为new或者运行线程runner失败，说明当前任务已经被其他线程启动或者已经被执行过，直接返回false 调用call方法执行核心任务逻辑。如果调用成功则执行set(result)方法，将state状态设置成NORMAL。如果调用失败抛出异常则执行setException(ex)方法，将state状态设置成EXCEPTIONAL，唤醒所有在get()方法上等待的线程 如果当前状态为INTERRUPTING(步骤2已CAS失败)，则一直调用Thread.yield()直至状态不为INTERRUPTING set、setException方法123456789101112131415protected void set(V v) &#123; if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) &#123; outcome = v; UNSAFE.putOrderedInt(this, stateOffset, NORMAL); // final state finishCompletion(); &#125;&#125;protected void setException(Throwable t) &#123; if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) &#123; outcome = t; UNSAFE.putOrderedInt(this, stateOffset, EXCEPTIONAL); // final state finishCompletion(); &#125;&#125; 两个方法的逻辑基本一致，先通过CAS操作将状态从NEW置为COMPLETING，然后再将最终状态分别置为NORMAL或者EXCEPTIONAL，最后再调用finishCompletion方法。 get在finishCompletion方法中，FutureTask会通知waiters链表中的每一个等待线程，那么这些线程是怎么被加入到waiters链表中的呢？上文已经讲过，当在一个线程中调用了get方法，该线程就会被加入到waiters链表中。所以接下来看下get方法： 12345678910public V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException &#123; if (unit == null) throw new NullPointerException(); int s = state; if (s &lt;= COMPLETING &amp;&amp; (s = awaitDone(true, unit.toNanos(timeout))) &lt;= COMPLETING) throw new TimeoutException(); return report(s);&#125; 其方法主要是调用awaitDone方法 123456789101112131415161718192021222324252627282930313233343536373839404142private int awaitDone(boolean timed, long nanos) throws InterruptedException &#123; final long deadline = timed ? System.nanoTime() + nanos : 0L; WaitNode q = null; boolean queued = false; for (;;) &#123; //如果该线程执行interrupt()方法，则从队列中移除该节点，并抛出异常 if (Thread.interrupted()) &#123; removeWaiter(q); throw new InterruptedException(); &#125; int s = state; //如果state状态大于COMPLETING 则说明任务执行完成，或取消 if (s &gt; COMPLETING) &#123; if (q != null) q.thread = null; return s; &#125; //如果state=COMPLETING，则使用yield，因为此状态的时间特别短，通过yield比挂起响应更快。 else if (s == COMPLETING) // cannot time out yet Thread.yield(); //构建节点 else if (q == null) q = new WaitNode(); //把当前节点入栈 else if (!queued) queued = UNSAFE.compareAndSwapObject(this, waitersOffset, q.next = waiters, q); //如果需要阻塞指定时间，则使用LockSupport.parkNanos阻塞指定时间 //如果到指定时间还没执行完，则从队列中移除该节点，并返回当前状态 else if (timed) &#123; nanos = deadline - System.nanoTime(); if (nanos &lt;= 0L) &#123; removeWaiter(q); return state; &#125; LockSupport.parkNanos(this, nanos); &#125; //阻塞当前线程 else LockSupport.park(this); &#125;&#125; 整个方法的大致逻辑主要分为以下几步： 如果当前状态值大于COMPLETING，说明已经执行完成或者取消，直接返回 如果state=COMPLETING，则使用yield，因为此状态的时间特别短，通过yield比挂起响应更快 如果当前线程是首次进入循环，为当前线程创建wait节点加入到waiters链表中 根据是否定时将当前线程挂起（LockSupport.parkNanos， LockSupport.park）来阻塞当前线程，直到超时或者线程被finishCompletion方法唤醒 当线程挂起超时或者被唤醒后，重新循环执行上述逻辑 get方法是FutureTask中的关键方法，了解了get方法逻辑也就了解为什么当调用get方法时线程会被阻塞直到任务运行完成。 finishCompletion12345678910111213141516171819202122private void finishCompletion() &#123; for (WaitNode q; (q = waiters) != null;) &#123; //通过CAS把栈顶的元素置为null，相当于弹出栈顶元素 if (UNSAFE.compareAndSwapObject(this, waitersOffset, q, null)) &#123; for (;;) &#123; Thread t = q.thread; if (t != null) &#123; q.thread = null; LockSupport.unpark(t); &#125; WaitNode next = q.next; if (next == null) break; q.next = null; // unlink to help gc q = next; &#125; break; &#125; &#125; done(); callable = null; // to reduce footprint&#125; finishCompletion的逻辑也比较简单： 遍历waiters链表，取出每一个节点：每个节点都代表一个正在等待该FutureTask结果（即调用过get方法）的线程 通过 LockSupport.unpark(t)唤醒每一个节点，通知每个线程，该任务执行完成 cancelcancel方法用于结束当前任务： 1234567891011121314151617181920public boolean cancel(boolean mayInterruptIfRunning) &#123; if (!(state == NEW &amp;&amp; UNSAFE.compareAndSwapInt(this, stateOffset, NEW, mayInterruptIfRunning ? INTERRUPTING : CANCELLED))) return false; try &#123; // in case call to interrupt throws exception if (mayInterruptIfRunning) &#123; try &#123; Thread t = runner; if (t != null) t.interrupt(); &#125; finally &#123; // final state UNSAFE.putOrderedInt(this, stateOffset, INTERRUPTED); &#125; &#125; &#125; finally &#123; finishCompletion(); &#125; return true;&#125; 根据mayInterruptIfRunning是否为true，CAS设置状态为INTERRUPTING或CANCELLED，设置成功，继续第二步，否则返回false 如果mayInterruptIfRunning为true，调用runner.interupt()，设置状态为INTERRUPTED 唤醒所有在get()方法等待的线程 其他方法1234567public boolean isCancelled() &#123; return state &gt;= CANCELLED;&#125;public boolean isDone() &#123; return state != NEW;&#125; 这两个方法其实就是判断state，没有过多的步骤。 参考https://www.jianshu.com/p/55221d045f39 https://yq.aliyun.com/articles/128032 https://blog.csdn.net/chenliguan/article/details/54345993 http://www.cnblogs.com/dolphin0520/p/3949310.html http://www.threadworld.cn/archives/39.html]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发队列BlockingQueue]]></title>
    <url>%2F2018%2F11%2F04%2FJava%E5%B9%B6%E5%8F%91%E9%98%9F%E5%88%97BlockingQueue%2F</url>
    <content type="text"><![CDATA[在Concurrent包中(在Java5版本开始提供)，BlockingQueue很好的解决了多线程中，如何高效安全“传输”数据的问题。通过这些高效并且线程安全的队列类，为我们快速搭建高质量的多线程程序带来极大的便利。本文详细介绍了BlockingQueue家庭中的所有成员，包括他们各自的功能以及常见使用场景。 BlockingQueue阻塞队列，顾名思义，首先它是一个队列，而一个队列在数据结构中所起的作用大致如下图所示： 从上图我们可以很清楚看到，通过一个共享的队列，可以使得数据由队列的一端输入，从另外一端输出； 常用的队列主要有以下两种：（当然通过不同的实现方式，还可以延伸出很多不同类型的队列，DelayQueue就是其中的一种） 先进先出（FIFO）：先插入的队列的元素也最先出队列，类似于排队的功能。从某种程度上来说这种队列也体现了一种公平性。 后进先出（LIFO）：后插入队列的元素最先出队列，这种队列优先处理最近发生的事件。 ​ 多线程环境中，通过队列可以很容易实现数据共享，比如经典的“生产者”和“消费者”模型中，通过队列可以很便利地实现两者之间的数据共享。假设我们有若干生产者线程，另外又有若干个消费者线程。如果生产者线程需要把准备好的数据共享给消费者线程，利用队列的方式来传递数据，就可以很方便地解决他们之间的数据共享问题。但如果生产者和消费者在某个时间段内，万一发生数据处理速度不匹配的情况呢？理想情况下，如果生产者产出数据的速度大于消费者消费的速度，并且当生产出来的数据累积到一定程度的时候，那么生产者必须暂停等待一下（阻塞生产者线程），以便等待消费者线程把累积的数据处理完毕，反之亦然。然而，在concurrent包发布以前，在多线程环境下，我们每个程序员都必须去自己控制这些细节，尤其还要兼顾效率和线程安全，而这会给我们的程序带来不小的复杂度。好在此时，强大的concurrent包横空出世了，而他也给我们带来了强大的BlockingQueue。（在多线程领域：所谓阻塞，在某些情况下会挂起线程（即阻塞），一旦条件满足，被挂起的线程又会自动被唤醒），下面两幅图演示了BlockingQueue的两个常见阻塞场景： 如上图所示：当队列中没有数据的情况下，消费者端的所有线程都会被自动阻塞（挂起），直到有数据放入队列。 如上图所示：当队列中填满数据的情况下，生产者端的所有线程都会被自动阻塞（挂起），直到队列中有空的位置，线程被自动唤醒。 这也是我们在多线程环境下，为什么需要BlockingQueue的原因。作为BlockingQueue的使用者，我们再也不需要关心什么时候需要阻塞线程，什么时候需要唤醒线程，因为这一切BlockingQueue都给你一手包办了。既然BlockingQueue如此神通广大，让我们一起来见识下它的常用方法 BlockingQueue的核心方法放入数据offer(anObject):表示如果可能的话,将anObject加到BlockingQueue里,即如果BlockingQueue可以容纳,则返回true,否则返回false.（本方法不阻塞当前执行方法的线程）； offer(E o, long timeout, TimeUnit unit)：可以设定等待的时间，如果在指定的时间内，还不能往队列中加入BlockingQueue，则返回失败。 put(anObject):把anObject加到BlockingQueue里,如果BlockQueue没有空间,则调用此方法的线程被阻断直到BlockingQueue里面有空间再继续. 获取数据poll():取走BlockingQueue里排在首位的对象,取不到时返回null; poll(long timeout, TimeUnit unit)：从BlockingQueue取出一个队首的对象，如果在指定时间内，队列一旦有数据可取，则立即返回队列中的数据。否则直到时间超时还没有数据可取，返回失败。 take():取走BlockingQueue里排在首位的对象,若BlockingQueue为空,阻断进入等待状态直到BlockingQueue有新的数据被加入; drainTo():一次性从BlockingQueue获取所有可用的数据对象（还可以指定获取数据的个数），通过该方法，可以提升获取数据效率；不需要多次分批加锁或释放锁。 BlockingQueue 对插入操作、移除操作、获取元素操作提供了四种不同的方法用于不同的场景中使用：1、抛出异常；2、返回特殊值（null 或 true/false，取决于具体的操作）；3、阻塞等待此操作，直到这个操作成功；4、阻塞等待此操作，直到成功或者超时指定时间。总结如下： Throws exception Special value Blocks Times out Insert add(e) offer(e) put(e) offer(e, time, unit) Remove remove() poll() take() poll(time, unit) Examine element() peek() not applicable not applicable 对于 BlockingQueue，我们的关注点应该在 put(e) 和 take() 这两个方法，因为这两个方法是带阻塞的。 ThrowsException：如果操作不能马上进行，则抛出异常 SpecialValue：如果操作不能马上进行，将会返回一个特殊的值，一般是true或者false Blocks: 如果操作不能马上进行，操作会被阻塞 TimesOut: 如果操作不能马上进行，操作会被阻塞指定的时间，如果指定时间没执行，则返回一个特殊值，一般是true或者false BlockingQueue 不接受 null 值的插入，相应的方法在碰到 null 的插入时会抛出 NullPointerException 异常。null 值在这里通常用于作为特殊值返回（表格中的第三列），代表 poll 失败。所以，如果允许插入 null 值的话，那获取的时候，就不能很好地用 null 来判断到底是代表失败，还是获取的值就是 null 值。 常见的几种BlockingQueue ArrayBlockingQueue 基于数组的阻塞队列实现，在ArrayBlockingQueue内部，维护了一个定长数组，以便缓存队列中的数据对象，这是一个常用的阻塞队列，除了一个定长数组外，ArrayBlockingQueue内部还保存着两个整形变量，分别标识着队列的头部和尾部在数组中的位置。 ArrayBlockingQueue在生产者放入数据和消费者获取数据，都是共用同一个锁对象，由此也意味着两者无法真正并行运行，这点尤其不同于LinkedBlockingQueue；按照实现原理来分析，ArrayBlockingQueue完全可以采用分离锁，从而实现生产者和消费者操作的完全并行运行。Doug Lea之所以没这样去做，也许是因为ArrayBlockingQueue的数据写入和获取操作已经足够轻巧，以至于引入独立的锁机制，除了给代码带来额外的复杂性外，其在性能上完全占不到任何便宜。 ArrayBlockingQueue和LinkedBlockingQueue间还有一个明显的不同之处在于，前者在插入或删除元素时不会产生或销毁任何额外的对象实例，而后者则会生成一个额外的Node对象。这在长时间内需要高效并发地处理大批量数据的系统中，其对于GC的影响还是存在一定的区别。而在创建ArrayBlockingQueue时，我们还可以控制对象的内部锁是否采用公平锁，默认采用非公平锁。 对于 ArrayBlockingQueue，我们可以在构造的时候指定以下三个参数： 队列容量，其限制了队列中最多允许的元素个数； 指定独占锁是公平锁还是非公平锁。非公平锁的吞吐量比较高，公平锁可以保证每次都是等待最久的线程获取到锁； 可以指定用一个集合来初始化，将此集合中的元素在构造方法期间就先添加到队列中。 LinkedBlockingQueue底层基于单向链表实现的阻塞队列，可以当做无界队列也可以当做有界队列来使用。 基于链表的阻塞队列，同ArrayListBlockingQueue类似，其内部也维持着一个数据缓冲队列（该队列由一个链表构成），当生产者往队列中放入一个数据时，队列会从生产者手中获取数据，并缓存在队列内部，而生产者立即返回；只有当队列缓冲区达到最大值缓存容量时（LinkedBlockingQueue可以通过构造函数指定该值），才会阻塞生产者队列，直到消费者从队列中消费掉一份数据，生产者线程会被唤醒，反之对于消费者这端的处理也基于同样的原理。而LinkedBlockingQueue之所以能够高效的处理并发数据，还因为其对于生产者端和消费者端分别采用了独立的锁来控制数据同步，这也意味着在高并发的情况下生产者和消费者可以并行地操作队列中的数据，以此来提高整个队列的并发性能。 作为开发者，我们需要注意的是，如果构造一个LinkedBlockingQueue对象，而没有指定其容量大小，LinkedBlockingQueue会默认一个类似无限大小的容量（Integer.MAX_VALUE），这样的话，如果生产者的速度一旦大于消费者的速度，也许还没有等到队列满阻塞产生，系统内存就有可能已被消耗殆尽了。 ArrayBlockingQueue和LinkedBlockingQueue是两个最普通也是最常用的阻塞队列，一般情况下，在处理多线程间的生产者消费者问题，使用这两个类足以。 下面的代码演示了如何使用BlockingQueue： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.util.Random;import java.util.concurrent.BlockingQueue;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicInteger; /** * 生产者线程 * */public class Producer implements Runnable &#123; public Producer(BlockingQueue queue) &#123; this.queue = queue; &#125; public void run() &#123; String data = null; Random r = new Random(); System.out.println("启动生产者线程！"); try &#123; while (isRunning) &#123; System.out.println("正在生产数据..."); Thread.sleep(r.nextInt(DEFAULT_RANGE_FOR_SLEEP)); data = "data:" + count.incrementAndGet(); System.out.println("将数据：" + data + "放入队列..."); if (!queue.offer(data, 2, TimeUnit.SECONDS)) &#123; System.out.println("放入数据失败：" + data); &#125; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); Thread.currentThread().interrupt(); &#125; finally &#123; System.out.println("退出生产者线程！"); &#125; &#125; public void stop() &#123; isRunning = false; &#125; private volatile boolean isRunning = true; private BlockingQueue queue; private static AtomicInteger count = new AtomicInteger(); private static final int DEFAULT_RANGE_FOR_SLEEP = 1000; &#125; 123456789101112131415161718192021222324252627282930313233343536373839404142import java.util.Random;import java.util.concurrent.BlockingQueue;import java.util.concurrent.TimeUnit; /** * 消费者线程 * */public class Consumer implements Runnable &#123; public Consumer(BlockingQueue&lt;String&gt; queue) &#123; this.queue = queue; &#125; public void run() &#123; System.out.println("启动消费者线程！"); Random r = new Random(); boolean isRunning = true; try &#123; while (isRunning) &#123; System.out.println("正从队列获取数据..."); String data = queue.poll(2, TimeUnit.SECONDS); if (null != data) &#123; System.out.println("拿到数据：" + data); System.out.println("正在消费数据：" + data); Thread.sleep(r.nextInt(DEFAULT_RANGE_FOR_SLEEP)); &#125; else &#123; // 超过2s还没数据，认为所有生产线程都已经退出，自动退出消费线程。 isRunning = false; &#125; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); Thread.currentThread().interrupt(); &#125; finally &#123; System.out.println("退出消费者线程！"); &#125; &#125; private BlockingQueue&lt;String&gt; queue; private static final int DEFAULT_RANGE_FOR_SLEEP = 1000;&#125; 1234567891011121314151617181920212223242526272829303132333435import java.util.concurrent.BlockingQueue;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.LinkedBlockingQueue;public class BlockingQueueTest &#123; public static void main(String[] args) throws InterruptedException &#123; // 声明一个容量为10的缓存队列 BlockingQueue&lt;String&gt; queue = new LinkedBlockingQueue&lt;String&gt;(10); Producer producer1 = new Producer(queue); Producer producer2 = new Producer(queue); Producer producer3 = new Producer(queue); Consumer consumer = new Consumer(queue); // 借助Executors ExecutorService service = Executors.newCachedThreadPool(); // 启动线程 service.execute(producer1); service.execute(producer2); service.execute(producer3); service.execute(consumer); // 执行10s Thread.sleep(10 * 1000); producer1.stop(); producer2.stop(); producer3.stop(); Thread.sleep(2000); // 退出Executor service.shutdown(); &#125;&#125; DelayQueue​ DelayQueue中的元素只有当其指定的延迟时间到了，才能够从队列中获取到该元素。DelayQueue是一个没有大小限制的队列，因此往队列中插入数据的操作（生产者）永远不会被阻塞，而只有获取数据的操作（消费者）才会被阻塞。使用场景： DelayQueue使用场景较少，但都相当巧妙，常见的例子比如使用一个DelayQueue来管理一个超时未响应的连接队列。 PriorityBlockingQueue​ 基于优先级的阻塞队列（优先级的判断通过构造函数传入的Compator对象来决定），但需要注意的是PriorityBlockingQueue并不会阻塞数据生产者，而只会在没有可消费的数据时，阻塞数据的消费者。因此使用的时候要特别注意，生产者生产数据的速度绝对不能快于消费者消费数据的速度，否则时间一长，会最终耗尽所有的可用堆内存空间。在实现PriorityBlockingQueue时，内部控制线程同步的锁采用的是公平锁。 SynchronousQueue​ 一种无缓冲的等待队列，类似于无中介的直接交易，有点像原始社会中的生产者和消费者，生产者拿着产品去集市销售给产品的最终消费者，而消费者必须亲自去集市找到所要商品的直接生产者，如果一方没有找到合适的目标，那么对不起，大家都在集市等待。相对于有缓冲的BlockingQueue来说，少了一个中间经销商的环节（缓冲区），如果有经销商，生产者直接把产品批发给经销商，而无需在意经销商最终会将这些产品卖给那些消费者，由于经销商可以库存一部分商品，因此相对于直接交易模式，总体来说采用中间经销商的模式会吞吐量高一些（可以批量买卖）；但另一方面，又因为经销商的引入，使得产品从生产者到消费者中间增加了额外的交易环节，单个产品的及时响应性能可能会降低。 声明一个SynchronousQueue有两种不同的方式，它们之间有着不太一样的行为。公平模式和非公平模式的区别: 如果采用公平模式：SynchronousQueue会采用公平锁，并配合一个FIFO队列来阻塞多余的生产者和消费者，从而体系整体的公平策略； 但如果是非公平模式（SynchronousQueue默认）：SynchronousQueue采用非公平锁，同时配合一个LIFO队列来管理多余的生产者和消费者，而后一种模式，如果生产者和消费者的处理速度有差距，则很容易出现饥渴的情况，即可能有某些生产者或者是消费者的数据永远都得不到处理。 总结BlockingQueue不光实现了一个完整队列所具有的基本功能，同时在多线程环境下，他还自动管理了多线间的自动等待于唤醒功能，从而使得程序员可以忽略这些细节，关注更高级的功能。 ArrayBlockingQueue 底层是数组，有界队列，如果我们要使用生产者-消费者模式，这是非常好的选择。 LinkedBlockingQueue 底层是链表，可以当做无界和有界队列来使用，所以大家不要以为它就是无界队列。 SynchronousQueue 本身不带有空间来存储任何元素，使用上可以选择公平模式和非公平模式。 PriorityBlockingQueue 是无界队列，基于数组，数据结构为二叉堆，数组第一个也是树的根节点总是最小值。 参考http://www.importnew.com/28053.html https://blog.csdn.net/suifeng3051/article/details/48807423 https://blog.csdn.net/suifeng3051/article/details/48807423 http://www.cnblogs.com/jackyuj/archive/2010/11/24/1886553.html https://www.jianshu.com/p/a42b89287359]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux的iptables原理]]></title>
    <url>%2F2018%2F11%2F03%2FLinux%E7%9A%84iptables%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[在Linux系统中，对于防火墙的实现一般分为包过滤防火墙，TCP-Wrapper即程序管控，代理服务器等几种方式。其中，iptables作为一种基于包过滤方式的防火墙工具，在实际中应用非常广泛，是非常重要的一个安全工具。真正实现防火墙功能的是 netfilter，它是一个 linux 内核模块，做实际的包过滤。实际上，除了 iptables 以外，还有很多类似的用户空间工具。 iptables的“链”与“表”netfilter 使用表（table）和 链（chain）来组织网络包的处理规则（rule）。它默认定义了以下表和链： 表 表功能 链 链功能 raw PREROUTING OUTPUT RAW 拥有最高的优先级，它使用PREROUTING和OUTPUT两个链，因此 RAW 可以覆盖所有包。在raw表中支持一个特殊的目标:TRACE，使内核记录下每条匹配该包的对应iptables规则信息。使用raw表内的TRACE target 即可实现对iptables规则的跟踪调试。比如：# iptables -t raw -A OUTPUT -p icmp -j TRACE # ipt ables -t raw -A PREROUTING -p icmp -j TRACE Filter 包过滤 FORWARD 过滤目的地址和源地址都不是本机的包 INPUT 过滤目的地址是本机的包 OUTPUT 过滤源地址是本机的包 Nat 网络地址转换 PREROUTING 在路由前做地址转换，使得目的地址能够匹配上防火墙的路由表，常用于转换目的地址。 POSTROUTING 在路由后做地址转换。这意味着不需要在路由前修改目的地址。常用于转换源地址。 OUTPUT 对防火墙产生的包做地址转换（很少量地用于 SOHO 环境中） Mangle TCP 头修改 PREROUTING POSTROUTING OUTPUT INPUT FORWARD 在路由器修改 TCP 包的 QoS（很少量地用在 SOHO 环境中） 先是透过路由判断， 决定了输出的路径后，再透过 filter 的 OUTPUT 链来传送的， mangle 这个表格很少被使用，如果将上图的mangle 拿掉的话，那就容易看的多了： 如果你的防火墙事实上是用来管制 LAN 内的其他主机的话，那么你就必须要再针对 filter 的 FORWARD 这条链，还有 nat 的 PREROUTING, POSTROUTING 以及 OUTPUT 进行额外的规则订定才行。 iptables实现SNAT与DNATNAT 服务器的重点就在于上面流程NAT table 的两条重要的链：PREROUTING 与 POSTROUTING。 举例如下： SNAT封包传送和封包接收 客户端所发出的封包表头中，来源会是 192.168.1.100 ，然后传送到 NAT 这部主机；NAT 这部主机的内部接口 (192.168.1.2) 接收到这个封包后，会主动分析表头数据， 因为表头数据显示目的并非 Linux 本机，所以开始经过路由， 将此封包转到可以连接到 Internet 的 Public IP 处；由于 private IP 与 public IP 不能互通，所以 Linux 主机透过 iptables 的 NAT table 内的 Postrouting 链将封包表头的来源伪装成为 Linux 的 Public IP ，并且将两个不同来源 (192.168.1.100 及 public IP) 的封包对应写入暂存内存当中， 然后将此封包传送出去了； 此时 Internet 上面看到这个封包时，都只会知道这个封包来自那个 Public IP 而不知道其实是来自内部啦。 好了，那么如果 Internet 回传封包呢？又会怎么作？ 在 Internet 上面的主机接到这个封包时，会将响应数据传送给那个 Public IP 的主机；当 Linux NAT 服务器收到来自 Internet 的回应封包后，会分析该封包的序号，并比对刚刚记录到内存当中的数据， 由于发现该封包为后端主机之前传送出去的，因此在 NAT Prerouting 链中，会将目标 IP 修改成为后端主机，亦即那部 192.168.1.100，然后发现目标已经不是本机 (public IP)， 所以开始透过路由分析封包流向；封包会传送到 192.168.1.2 这个内部接口，然后再传送到最终目标 192.168.1.100 机器上去！ SNAT 主要是应付内部 LAN 连接到 Internet 的使用方式，至于 DNAT 则主要用在内部主机想要架设可以让 Internet 存取的服务器啦！ DNAT封包传送 假设我的内部主机 192.168.1.210 启动了 WWW 服务，这个服务的 port 开启在 port 80 ， 那么 Internet 上面的主机 (61.xx.xx.xx) 要如何连接到我的内部服务器呢？当然啦， 还是得要透过 Linux NAT 服务器嘛！所以这部 Internet 上面的机器必须要连接到我们的 NAT 的 public IP 才行。外部主机想要连接到目的端的 WWW 服务，则必须要连接到我们的 NAT 服务器上头；我们的 NAT 服务器已经设定好要分析出 port 80 的封包，所以当 NAT 服务器接到这个封包后， 会将目标 IP 由 public IP 改成 192.168.1.210 ，且将该封包相关信息记录下来，等待内部服务器的响应；上述的封包在经过路由后，来到 private 接口处，然后透过内部的 LAN 传送到 192.168.1.210 上头！ 192.186.1.210 会响应数据给 61.xx.xx.xx ，这个回应当然会传送到 192.168.1.2 上头去；经过路由判断后，来到 NAT Postrouting 的链，然后透过刚刚的记录，将来源 IP 由 192.168.1.210 改为 public IP 后，就可以传送出去了！ iptables常用命令 注释： 如果想查看特别的表时使用-t指定,如果查看单独的链需要在操作后面指定. 如果是查看规则定义,使用-S. -S比-L查看规则时更加清晰. 如果查看匹配状况使用-nvL.配合watch使用. --line-number用于查看规则号. iptables [-t tables][-L] [-nv] 选项与参数： -t ：后面接 table ，例如 nat 或 filter ，若省略此项目，则使用默认的 filter -L ：列出目前的 table 的规则 -n ：不进行 IP 与 HOSTNAME 的反查，显示讯息的速度会快很多！ -v ：列出更多的信息，包括通过该规则的封包总位数、相关的网络接口等 iptables-save [-t table] （列出完整的防火墙规则） 选项与参数： -t ：可以仅针对某些表格来输出，例如仅针对 nat 或 filter 等等 这个命令主要是把内存态的规则保存到文件,然后下次启动的时候用iptables-restore来载入规则. 但是这个命令常常用来查看防火墙规则。比iptables用得都多, 主要是输出结果的格式比较紧凑直观.而且能方便的能看到所有表的规则. iptables [-t tables][-FXZ] 选项与参数：-F ：清除所有的已订定的规则；-X ：杀掉所有使用者 “自定义” 的 chain (应该说的是 tables ）啰；-Z ：将所有的 chain 的计数与流量统计都归零 iptables命令举例iptables配置文件 配置文件位置： /etc/sysconfig/iptables iptables服务命令 在linux中关闭防火墙有两种状态一种永久关闭防火墙，另一种是暂时关闭防火墙的方法 123456789101112131415161718192021222324-- 启动服务# /etc/init.d/iptables start # service iptables start-- 停止服务# /etc/init.d/iptables stop# service iptables stop-- 重启服务# /etc/init.d/iptables restart# service iptables restart-- 保存设置# /etc/init.d/iptables save# service iptables save-- 查看防火墙状态# service iptables status# iptables -L -n-- 永久关闭防火墙# chkconfig iptables off -- 永久关闭后启用# chkconfig iptables on 清空当前的所有规则和计数 123iptables -F #清空所有的防火墙规则iptables -X #删除用户自定义的空链iptables -Z #清空计数 配置允许ssh端口连接 12iptables -A INPUT -s 192.168.1.0/24 -p tcp --dport 22 -j ACCEPT #22为你的ssh端口， -s 192.168.1.0/24表示允许这个网段的机器来连接，其它网段的ip地址是登陆不了你的机器的。 -j ACCEPT表示接受这样的请求 允许本地回环地址可以正常使用 123iptables -A INPUT -i lo -j ACCEPT #本地圆环地址就是那个127.0.0.1，是本机上使用的,它进与出都设置为允许iptables -A OUTPUT -o lo -j ACCEPT 设置默认的规则 123iptables -P INPUT DROP #配置默认的不让进iptables -P FORWARD DROP #默认的不允许转发iptables -P OUTPUT ACCEPT #默认的可以出去 一些例子 123456789iptables -A INPUT -s 10.10.10.10 -j DROP #丢弃从 10.10.10.10 主机来的所有包iptables -A INPUT -s 10.10.10.0/24 -j DROP #丢弃从 10.10.10.0/24 网段进来所有包iptables -A INPUT -p tcp --dport ssh -s 10.10.10.10 -j DROP # 如果协议是 tcp，目标端口是 ssh 端口，源IP 为 10.10.10.10，那么丢弃它iptables -A INPUT -i virbr0 -p udp -m udp --dport 53 -j ACCEPT #接受从 virbr0 进来的所有目标端口 53 的 udp 包iptables -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT #接受 RELEASED 和 ESTABLISHED 状态的连接。Linux 3.7 以后，--state 被替换成了 --conntrackiptables -A FORWARD -d 192.168.122.0/24 -o virbr0 -m state --state RELATED,ESTABLISHED -j ACCEPT #转发时接受这些包iptables -A FORWARD -p icmp -j ACCEPT #转发时接受所有 ICMP 路由包。iptables -A INPUT -i lo -j ACCEPT #使用 -i 过滤从 lo 设备进来的包iptables -A INPUT -i eth0 -j ACCEPT #使用 -i 过滤从网卡 eth0 进来的包。不指定网卡的话表示所有网卡。 iptables的常见问题1.你听说过Linux下面的iptables和Firewalld么？知不知道它们是什么，是用来干什么的？ iptables通常被用作类UNIX系统中的防火墙，更准确的说，可以称为iptables/netfilter。管理员通过终端/GUI工具与iptables打交道，来添加和定义防火墙规则到预定义的表中。Netfilter是内核中的一个模块，它执行包过滤的任务。 Firewalld是RHEL/CentOS 7中最新的过滤规则的实现。它已经取代了iptables接口，并与netfilter相连接。 2.请在iptables中添加一条规则，接受所有从一个信任的IP地址（例如，192.168.0.7）过来的包。 1iptables -A INPUT -s 192.168.0.7 -j ACCEPT 3.假如有一台电脑的本地IP地址是192.168.0.6。你需要封锁在21、22、23和80号端口上的连接，你会怎么做？ 1iptables -A INPUT -s 192.168.0.6 -p tcp -m multiport –dport 22,23,80,8080 -j DROP 参考http://fishcried.com/2016-02-19/iptables/ http://cn.linux.vbird.org/linux_server/0250simple_firewall_3.php https://linux.cn/article-5948-1.html#3_10137 https://www.jianshu.com/p/586da7c8fd42 https://blog.csdn.net/qq_21439971/article/details/51524711]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA打印Array数组内容的几种方法]]></title>
    <url>%2F2018%2F11%2F03%2FJAVA%E6%89%93%E5%8D%B0Array%E6%95%B0%E7%BB%84%E5%86%85%E5%AE%B9%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[下面是几种常见的打印Array数组内容的方式。 方法一：使用循环打印 1234567891011121314public class Demo &#123; public static void main(String[] args) &#123; String[] infos = new String[] &#123;"Java", "Android", "C/C++", "Kotlin"&#125;; StringBuffer strBuffer = new StringBuffer(); for(int i = 0; i&lt; infos.length; i++) &#123; if(i &gt; 0) &#123; strBuffer.append(", "); &#125; strBuffer.append(infos[i]); &#125; System.out.println(strBuffer); &#125;&#125; 方法二：使用 Arrays.toString() 打印 1234567public class Demo &#123; public static void main(String[] args) &#123; String[] infos = new String[] &#123;"Java", "Android", "C/C++", "Kotlin"&#125;; System.out.println(Arrays.toString(infos)); &#125;&#125; 方法三：使用 JDK 8 的 java.util.Arrays.stream() 打印 1234567public class Demo &#123; public static void main(String[] args) &#123; String[] infos = new String[] &#123;"Java", "Android", "C/C++", "Kotlin"&#125;; Arrays.stream(infos).forEach(System.out::println); &#125;&#125; 方法四：使用 Arrays.deepToString() 方法打印。如果数组中有其它数组，即多维数组，也会用同样的方法深度显示。 1234567public class Demo &#123; public static void main(String[] args) &#123; String[] infos = new String[] &#123;"Java", "Android", "C/C++", "Kotlin"&#125;; System.out.println(Arrays.deepToString(infos)); &#125;&#125; 茴香豆的写法有很多种，打开眼界最重要～]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>对象</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySql数据库中的索引]]></title>
    <url>%2F2018%2F10%2F28%2FMySql%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E7%9A%84%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[索引对于良好的性能非常关键。尤其是当表中的数据量越来越大时，索引对性能的影响愈发重要。 b-tree索引应该是mysql里最广泛的索引的了，除了archive基本所有的存储引擎都支持它。 创建索引的3种方法： 1、创建索引 1CREATE INDEX &lt;索引的名字&gt; ON tablename (列的列表)； 2、修改表 1ALTER TABLE tablename ADD INDEX [索引的名字] (列的列表)； 3、创建表的时候指定索引 123CREATE TABLE tablename ( [...], INDEX [索引的名字] (列的列表) )； --示例CREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, INDEX [indexName] (username(length)) ); MySQL索引类型mysql里目前只支持4种索引分别是：full-text，b-tree，hash，r-tree。 full-text索引（全文索引）full-text在mysql里仅有myisam支持它，而且支持full-text的字段只有char、varchar、text数据类型。 full-text主要是用来代替like “%***%”效率低下的问题。 全文索引就是使用倒排索引的方式实现的。 MySQL5.6版本后的InnoDB存储引擎开始支持全文索引，5.7版本后通过使用ngram插件开始支持中文。 b-tree索引b-tree在myisam里的形式和innodb稍有不同（下文会重点介绍） 在 innodb里，有两种形态：一是primary key形态，其leaf node里存放的是数据，而且不仅存放了索引键的数据，还存放了其他字段的数据。二是secondary index，其leaf node和普通的b-tree差不多，只是还存放了指向主键的信息. 而在myisam里，主键和其他的并没有太大区别。不过和innodb不太一样的地方是在myisam里，leaf node里存放的不是主键的信息，而是指向数据文件里的对应数据行的信息。 hash索引目前我所知道的就只有memory和ndb cluster支持这种索引。 hash索引由于其结构，所以在每次查询的时候直接一次到位，不像b-tree那样一点点的前进。所以hash索引的效率高于b-tree，但hash也有缺点（本篇后文介绍）。 r-tree索引r-tree在mysql很少使用，仅支持geometry数据类型，支持该类型的存储引擎只有myisam、bdb、innodb、ndb、archive几种。 相对于b-tree，r-tree的优势在于范围查找。 索引的数据结构B-Tree索引 维基百科对B树的定义为“在计算机科学中，B树（B-tree）是一种树状数据结构，它能够存储数据、对其进行排序并允许以O(log n)的时间复杂度运行进行查找、顺序读取、插入和删除的数据结构。B树，概括来说是一个节点可以拥有多于2个子节点的二叉查找树。与自平衡二叉查找树不同，B-树为系统最优化大块数据的读和写操作。B-tree算法减少定位记录时所经历的中间过程，从而加快存取速度。普遍运用在数据库和文件系统。” 目前大多数的存储引擎使用B-Tree索引，严格来说是 B+树。相比B树，二叉树，Hash，它有哪些优势呢? 相对于二叉树，明显的优势是避免树的深度过大而造成磁盘I/O读写过于频繁；相对于Hash，见下面Hash索引限制描述；相比较B树来说，B+树的非叶子结点只包含导航信息，不包含实际的值，所有的叶子结点和相连的节点使用链表相连，便于区间查找和遍历。 适用场景：等值匹配，全值匹配，匹配最左前缀，匹配列前缀，范围匹配，只访问索引的查询，如覆盖索引。 Hash 索引 哈希索引基于哈希表实现，只有精确匹配索引所有列时才有效。对于每一行数据，存储引擎都会根据索引列计算一个哈希值，哈希索引将所有的hash值存储在索引中，同时在哈希表中保存指向每个数据行的指针。 MySQL中只有Memory引擎显式支持哈希索引。这也是Memory引擎的默认存储引擎，Memory引擎同时也支持B-Tree索引，哈希索引解决碰撞的方式是使用链表。 Hash索引的限制： 1、哈希索引只包含哈希值和行指针，而不存储字段值，所以不能使用索引中的值来避免读取行； 2、哈希索引数据并不是按照索引列的值顺序存储的，所以也就无法用于排序； 3、哈希索引也不支持部分索引列匹配查找，因为哈希索引始终是使用索引的全部列值内容来计算哈希值的。如：数据列（a,b）上建立哈希索引，如果只查询数据列a，则无法使用该索引； 4、哈希索引只支持等值比较查询，如：=，in()， &lt;=&gt;，不支持任何范围查询； 5、访问哈希索引的数据非常快，除非有很多哈希冲突，当出现哈希冲突的时候，存储引擎必须遍历链表中所有的行指针，逐行进行比较，直到找到所有符合条件的行； 6、如果哈希冲突很多的话，一些索引维护操作的代价也很高，如：如果在某个选择性很低的列上建立哈希索引（即很多重复值的列），那么当从表中删除一行时，存储引擎需要遍历对应哈希值的链表中的每一行，找到并删除对应的引用，冲突越多，代价越大； 适用场景：只需要做等值比较查询，而不包含排序或范围查询的需求，都适合使用哈希索引。 MyISAM和InnoDB对B-Tree索引实现 MyISAM索引文件和数据文件是分离的，索引文件仅保存记录所在页的指针（物理位置），通过这些地址来读取页，进而读取被索引的行，对于二级（辅助）索引，与主索引在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复，可见MyISAM索引是“非聚合的”。 InnoDB的主索引是采用“聚集索引”的数据存储方式，所谓“聚集”，就是指数据行和键值紧凑地存储在一起（InnoDB 只能聚集一个叶子页（16K）的记录），因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形；对于二级（辅助）索引，InnoDB采用的方式是在叶子页中保存主键值，通过这个主键值来回表查询到一条完整记录，因此按辅助索引检索实际上进行了二次查询，效率肯定是没有按照主键检索高的。由于每个辅助索引都包含主键索引，因此，为了减小辅助索引所占空间，我们通常希望 InnoDB 表中的主键索引尽量定义得小一些（值得一提的是，MySIAM会使用前缀压缩技术使得索引变小，而InnoDB按照原数据格式进行存储），并且希望InnoDB的主键是自增长的，因为如果主键并非自增长，插入时，由于写入时乱序的，会使得插入效率变低。 索引的优点 最常见的B-Tree索引，按照顺序存储数据，索引可以做ORDER BY和GROUP BY操作。因为数据是有序的，所以B-Tree也会将相关的列存储在一起，因为索引中存储了实际的列值，所以某些查询只是用索引就能够完成全部查询（覆盖索引，索引包含所有满足查询需要的数据的索引，也就是平时所说的不需要回表操作）：索引大大减少了服务器需要扫描的数据量；索引可以帮助服务器避免排序和临时表；索引可以将随机IO变为顺序IO； 索引优化策略独立的列索引列不能是表达式的一部分，也不能是函数的参数，例如 1select * from table where id + 1 = 5; 这个就不对了。此外对于列的类型也要注意一些优化：字段类型优先级： 整型 &gt; date, time &gt; enum, char&gt;varchar &gt; blob；够用就行,不要慷慨 (如smallint,varchar(N))，原因是大的字段浪费内存,影响速度；尽量避免用NULL()，原因是NULL不利于索引,要用特殊的字节来标注； 前缀索引和索引的选择性 有时候需要索引很长的字符列，这会让索引变得很大且慢，除了模拟hash值存储的方式外，还可以索引开始部分的字符，这样可以大大节约索引空间，从而提高索引效率，但这样会降低索引的选择性（不重复的索引值(也称为基数)和数据表的记录总数(#T)的比值，范围从1/#T到1之间）。要注意以下几点： 唯一索引的选择性是1，这是最好的索引选择性，性能也是最好的； 一般情况下某个列前缀的选择性也是足够高的，足以满足查询性能。对于BLOB、TEXT或者很长的VARCHAR类型的列，必须使用前缀索引，因为MySQL不允许索引这些列的完整长度； 计算合适的前缀长度的一个方法是计算完整列的选择性，并使前缀的选择性接近于完整列的选择性； 前缀索引是一种能使索引更小、更快的有效办法，但另一方面也有其缺点：MySQL无法使用前缀索引做ORDER BY和GROUP BY , 也无法使用前缀索引做覆盖扫描； 有时候后缀索引也有用途(例如，找到某个域名的所有电子邮件地址)。MySQL原生并不支持反向索引，但是可以把字符串反转后存储，并基于此建立前缀索引。可以通过触发器来维护这种索引。 多列索引合理使用联合索引，不要在where后面用到的每一列都加索引。 选择合适的索引列顺序正确的索引顺序依赖于使用该索引的查询，并同时满足需要考虑如何更好地满足排序和分组的需要。 聚簇索引 InnoDB的聚簇索引实际上在同一个结构中保存了B-Tree索引和数据行。它的数据实际上存储在索引的叶子页中。”聚簇”表示把数据行和相邻的键值紧凑地存储在一起。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。 聚簇索引的优点:可以把相关的数据保存在一起;数据访问更快;使用覆盖索引扫描的查询可以直接使用叶节点中的主键值。 聚簇索引的缺点：更新聚簇索引列的代价很高，因为会强制将每个被更新的行移动到新的位置；可能会导致页分裂；导致全表扫描变慢，尤其是行比较稀疏；二级索引(非聚簇索引)可能比想象的要更大，因为在二级索引的叶子节点包含了引用行的主键列；二级索引访问需要两次索引查询，而不是一次。这是因为二级索引叶子节点保存的不是指向行的物理位置的指针，而是行的主键值。这意味着通过二级索引查找行，存储引擎需要找到二级索引的叶子 节点获得对应的主键值，然后根据这个值去聚簇索引中查找到对应的行。这里做了重复工作：两次B-Tree查找而不是一次。 索引覆盖 索引覆盖是指 如果查询的列恰好是索引的一部分,那么查询只需要在索引文件上进行,不需要回行到磁盘再找数据。这种查询速度非常快,称为”索引覆盖”。 索引与排序 MySQL有两种方式可以生成有序的结果：通过排序操作；或者按索引顺序扫描；如果EXPLAIN出来的type列的值为”index”,则说明MySQL使用了索引来做排序(不要和Extra列的 “Using index”搞混淆了)。 压缩(前缀压缩)索引 MyISAM使用前缀索引压缩来减少索引的大小，从而让更多的索引可以放入内存中，这在某些情况下能极大地提高性能。默认值压缩字符串，但通过参数设置可以对整数压缩。InnoDB按照原数据格式进行存储。 冗余和重复索引 MySQL允许在相同列上创建多个索引，无论是有意还是无意的。MySQL需要单独维护重复的索引，并且优化器在优化查询的是时候也需要逐个地进行考虑，这会影响性能。重复索引是指在相同的列上按照相同的顺序创建相同类型的索引。冗余索引，如果创建了索引(A,B)，再创建索引(A)就是冗余索引，因为这只是前一个索引的前缀索引。 未使用的索引对于服务器上一些永远不用的索引，完全是累赘，建议考虑删除。 索引和锁InnoDB只有在访问行的时候才会对其进行加锁，而索引能够减少InnoDB访问的行数，从而减少锁的数量。 排查SQL语句通过对查询语句的分析，可以了解查询语句的执行情况。MySQL中，可以使用EXPLAIN语句和DESCRIBE语句来分析查询语句。 EXPLAIN语句的基本语法如下:（DESCRIBE语法一致，DESCRIBE可简写为DESC） EXPLAIN Select 语句； 1 explain SELECT * FROM `user` where name = 'name6'; 结果如下： explain结果值及其含义： 参数值 含义 id 表示SELECT语句的编号； select_type 表示SELECT语句的类型。该参数有几个常用的取值：SIMPLE：表示简单查询，其中不包括连接查询和子查询；PRIMARY：表示主查询，或者是最外层的查询语句；UNION：表示连接查询的第二个或后面的查询语句; table 表示查询的表； type 表示表的连接类型。该参数有几个常用的取值：const：表示表中有多条记录，但只从表中查询一条记录；eq_ref ：表示多表连接时，后面的表使用了UNIQUE或者PRIMARY KEY；ref ：表示多表查询时，后面的表使用了普通索引；unique_ subquery：表示子查询中使用了UNIQUE或者PRIMARY KEY；index_ subquery：表示子查询中使用了普通索引； range：表示查询语句中给出了查询范围；index：表示对表中的索引进行了完整的扫描；all：表示此次查询进行了全表扫描；———– 该条SQL需要优化； possible_keys 表示查询中可能使用的索引；如果备选的数量大于3那说明已经太多了，因为太多会导致选择索引而损耗性能， 所以建表时字段最好精简，同时也要建立联合索引，避免无效的单列索引； key 表示查询使用到的索引； key_len 表示索引字段的一长度; ref 表示使用哪个列或常数与索引一起来查询记录; rows 表示查询的行数;试图分析所有存在于累计结果集中的行数，虽然只是一个估值，却也足以反映 出SQL执行所需要扫描的行数，因此这个值越小越好； Extra 表示查询过程的附件信息。 参考：http://www.alongsky.com/?p=184 https://www.cnblogs.com/luyucheng/p/6289048.html https://blog.csdn.net/mine_song/article/details/63251546 https://www.cnblogs.com/vincently/p/4526560.html https://blog.csdn.net/v_JULY_v/article/details/6530142 http://blog.codinglabs.org/articles/theory-of-mysql-index.html https://blog.csdn.net/stfphp/article/details/52827845]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle数据库中的索引]]></title>
    <url>%2F2018%2F10%2F28%2FOracle%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E7%9A%84%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[建立索引的优点：1、大大加快数据的检索速度;2、创建唯一性索引,保证数据库表中每一行数据的唯一性;3、加速表和表之间的连接;4、在使用分组和排序子句进行数据检索时,可以显著减少查询中分组和排序的时间。 索引的种类1、按照索引列值的唯一性，索引可分为唯一索引和非唯一索引； 非唯一索引： create index 索引名 on 表名（列名） tablespace 表空间名； 唯一索引： 建立主键或者唯一约束时会自动在对应的列上建立唯一索引； 注：创建主键时，默认在主键上创建了唯一索引，因此不能再在主键上创建索引。 2、索引列的个数：单列索引和复合索引； 3、按照索引列的物理组织方式。 B树索引 create index 索引名 on 表名（列名） tablespace 表空间名; 位图索引 create bitmap index 索引名 on 表名（列名） tablespace 表空间名; 反向键索引 create index 索引名 on 表名（列名） reverse tablespace 表空间名; 函数索引 create index 索引名 on 表名（函数名(列名)） tablespace 表空间名; 聚集索引（也叫聚簇索引）在聚集索引中,表中行的物理顺序与键值的逻辑（索引）顺序相同.一个表只能包含一个聚集索引.如果某索引不是聚集索引,则表中行的物理顺序与键值的逻辑顺序不匹配.与非聚集索引相比,聚集索引通常提供更快的数据访问速度. 删除索引 drop index 索引名 重建索引 alter index 索引名 rebuild 索引的创建格式： 1234567891011CREATE UNIQUE | BITMAP INDEX &lt;schema&gt;.&lt;index_name&gt; ON &lt;schema&gt;.&lt;table_name&gt; (&lt;column_name&gt; | &lt;expression&gt; ASC | DESC, &lt;column_name&gt; | &lt;expression&gt; ASC | DESC,...) TABLESPACE &lt;tablespace_name&gt; STORAGE &lt;storage_settings&gt; LOGGING | NOLOGGING COMPUTE STATISTICS NOCOMPRESS | COMPRESS&lt;nn&gt; NOSORT | REVERSE PARTITION | GLOBAL PARTITION&lt;partition_setting&gt; ​ UNIQUE | BITMAP：指定UNIQUE为唯一值索引，BITMAP为位图索引，省略为B-Tree索引。​ &lt;column_name&gt; | ASC | DESC：可以对多列进行联合索引，当为expression时即“基于函数的索引”​ TABLESPACE：指定存放索引的表空间(索引和原表不在一个表空间时效率更高)​ STORAGE：可进一步设置表空间的存储参数​ LOGGING | NOLOGGING：是否对索引产生重做日志(对大表尽量使用NOLOGGING来减少占用空间并提高效率)​ COMPUTE STATISTICS：创建新索引时收集统计信息​ NOCOMPRESS | COMPRESS：是否使用“键压缩”(使用键压缩可以删除一个键列中出现的重复值)​ NOSORT | REVERSE：NOSORT表示与表中相同的顺序创建索引，REVERSE表示相反顺序存储索引值​ PARTITION | NOPARTITION：可以在分区表和未分区表上对创建的索引进行分区 ​ 使用USER_IND_COLUMNS查询某个TABLE中的相应字段索引建立情况 ​ 使用DBA_INDEXES/USER_INDEXES查询所有索引的具体设置情况。 Oracle中的索引类型​ 在Oracle中的索引可以分为：B树索引、位图索引、反向键索引、基于函数的索引、簇索引、全局索引、局部索引、HASH索引、降序索引等，下面逐一讲解： B树索引​ 最常用的索引，各叶子节点中包括的数据有索引列的值和数据表中对应行的ROWID，简单的说，在B树索引中，是通过在索引中保存排过续的索引列值与相对应记录的ROWID来实现快速查询的目的。其逻辑结构如图： 可以保证无论用户要搜索哪个分支的叶子结点，都需要经过相同的索引层次，即都需要相同的I/O次数。 ​ B树索引的创建示例： ​ create index ind_t on t1(id) ; ​ 注1：索引的针对字段创建的，相同字段不能创建一个以上的索引； ​ 注2：默认的索引是不唯一的，但是也可以加上unique，表示该索引的字段上没有重复值(定义unique约束时会自动创建)； ​ create unique index ind_t on t1(id) ; ​ 注3：创建主键时，默认在主键上创建了B树索引，因此不能再在主键上创建索引。 位图索引​ 有些字段中使用B树索引的效率仍然不高，例如性别的字段中，只有“男、女”两个值，则即便使用了B树索引，在进行检索时也将返回接近一半的记录。 ​ 所以当字段的基数很低时，需要使用位图索引。(“低”的标准是取值数量 &lt; 行数*1%) 位图索引的逻辑结构如上图所示：索引中不再记录rowid和键值，而是将每个值作为一列，用0和1表示该行是否等于该键值(0表示否;1表示是)。其中位图索引的行顺序与原表的行顺序一致，可以在查询数据的过程中对应计算出行的原始物理位置。 ​ 位图索引的创建示例： ​ create bitmap index ind_t on t1(type); 注：位图索引不可能是唯一索引，也不能进行键值压缩。 反向键索引​ 考虑这个情况：某一字段的值是1-1000顺序排列，建立B树索引后依旧递增，到后来该B数索引不断在后面增加分支，会形成如下如的不对称树： 反向键索引是一种特殊的B树索引，在存储构造中与B树索引完全相同，但是针对数值时，反向键索引会先反向每个键值的字节，然后对反向后的新数据进行索引。例如输入2008则转换为8002，这样当数值一次增加时，其反向键在大小中的分布仍然是比较平均的。 ​ 反向键索引的创建示例： ​ create index ind_t on t1(id) reverse; ​ 注：键的反转由系统自行完成。对于用户是透明的。 基于函数的索引​ 有的时候，需要进行如下查询：select * from t1 where to_char(date,’yyyy’)&gt;’2007’; ​ 但是即便在date字段上建立了索引，还是不得不进行全表扫描。在这种情况下，可以使用基于函数的索引。其创建语法如下： ​ create index ind_t on t1(to_char(date,’yyyy’)); ​ 注：简单来说，基于函数的索引，就是将查询要用到的表达式作为索引项。 聚集索引（也叫聚簇索引）”聚簇”表示把数据行和相邻的键值紧凑地存储在一起。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。 聚簇索引的优点:可以把相关的数据保存在一起;数据访问更快;使用覆盖索引扫描的查询可以直接使用叶节点中的主键值。 聚簇索引的缺点：更新聚簇索引列的代价很高，因为会强制将每个被更新的行移动到新的位置； 全局索引和局部索引​ 这个索引貌似很复杂，其实很简单。总得来说一句话，就是无论怎么分区，都是为了方便管理。 ​ 具体索引和表的关系有三种： ​ 1、局部分区索引：分区索引和分区表1对1 ​ 2、全局分区索引：分区索引和分区表N对N ​ 3、全局非分区索引：非分区索引和分区表1对N ​ 创建示例： ​ 首先创建一个分区表 123456789101112131415161718192021create table student ( stuno number(5), sname vrvhar2(10), deptno number(5) ) partition by hash (deptno) ( partition part_01 tablespace A1, partition part_02 tablespace A2 ); 创建局部分区索引(1v1) 123456789create index ind_t on student(stuno)local( partition part_01 tablespace A1, partition part_02 tablespace A2); --local后面可以不加 创建全局分区索引(NvN)： 1234567891011create index ind_t on student(stuno) global partition by range(stuno) ( partition p1 values less than(1000) tablespace A1, partition p2 values less than(maxvalue) tablespace A2 ); --只可以进行range分区 创建全局非分区索引(1vN) 1create index ind_t on student(stuno) GLOBAL; HASH索引使用HASH索引必须要使用HASH群集。建立一个群集或HASH群集的同时，也就定义了一个群集键。这个键告诉Oracle如何在群集上存储表。在存储数据时，所有与这个群集键相关的行都被存储在一个数据库块上。若数据都存储在同一个数据库块上，并且使用了HASH索引，Oracle就可以通过执行一个HASH函数和I/O来访问数据——而通过适用一个二元高度为4的B-树索引来访问数据，则需要在检索数据时使用4个I/O。技巧：HASH索引在有限制条件（需要指定一个确定的值而不是一个值范围）的情况下非常有用。 降序索引(descending index)这是基于函数索引的一种特殊类型。降序索引可以显著优化ORDER BY x, y, z DESC子句查询的。 域索引(domain index)当我们创建为用户自定义数据类型(datatype)创建用户自定义索引类型(indextype)时就要使用域索引。 虚拟索引(virtual index)这是为测试人员和开发人员准备的又一个工具。虚拟索引(不分配段空间)可以让你在不需要实际创建索引的情况下，测试新索引及其对查询计划的影响。对于GB级的表来说，构建索引非常耗费资源而且还要占用大量时间。 总结虽然Oracle数据库的索引世界有点吓人，不过实际上你平常经常使用的索引就只有那么一些。而且，Oracle 的优化器都已经设计相当出色；总体而言，Oracle很擅长于让你的数据库运行地更有效率。虽然这并不意味着你不需要对自己的SQL进行调优。不过，如果 你一直保持着最新的统计信息，并让Oracle为你整理出你所需要的最小数据集的话，它能够以极快的速度满足你的需要。 参考：https://zhidao.baidu.com/question/306752012277622484.html https://www.zybang.com/question/66ecd24eab270cd65d1d38cf401213a6.html http://wolfgangkiefer.blog.163.com/blog/static/86265503200910102626725/ https://blog.csdn.net/s630730701/article/details/51779378 http://www.cnblogs.com/lsh0310/p/3542277.html]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring的事务管理]]></title>
    <url>%2F2018%2F10%2F27%2FSpring%E7%9A%84%E4%BA%8B%E5%8A%A1%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[事务管理对于企业应用来说是至关重要的，即使出现异常情况，它也可以保证数据的一致性。Spring Framework对事务管理提供了一致的抽象，其特点如下： 为不同的事务API提供一致的编程模型，比如JTA(Java Transaction API), JDBC, Hibernate, JPA(Java Persistence API和JDO(Java Data Objects) 支持声明式事务管理，特别是基于注解的声明式事务管理，简单易用 提供比其他事务API如JTA更简单的编程式事务管理API 与spring数据访问抽象的完美集成 事务的基本原理Spring事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，spring是无法提供事务功能的。对于纯JDBC操作数据库，想要用到事务，可以按照以下步骤进行： 获取连接 Connection con = DriverManager.getConnection() 开启事务con.setAutoCommit(true/false) 执行CRUD 提交事务/回滚事务 con.commit() / con.rollback() 关闭连接 conn.close() 事务管理方式spring支持编程式事务管理和声明式事务管理两种方式。 编程式事务管理使用TransactionTemplate或者直接使用底层的PlatformTransactionManager。对于编程式事务管理，spring推荐使用TransactionTemplate。 声明式事务管理建立在AOP之上的。其本质是对方法前后进行拦截，然后在目标方法开始之前创建或者加入一个事务，在执行完目标方法之后根据执行情况提交或者回滚事务。声明式事务最大的优点就是不需要通过编程的方式管理事务，这样就不需要在业务逻辑代码中掺杂事务管理的代码，只需在配置文件中做相关的事务规则声明(或通过基于@Transactional注解的方式)，便可以将事务规则应用到业务逻辑中。 显然声明式事务管理要优于编程式事务管理，这正是spring倡导的非侵入式的开发方式。声明式事务管理使业务代码不受污染，一个普通的POJO对象，只要加上注解就可以获得完全的事务支持。和编程式事务相比，声明式事务唯一不足地方是，后者的最细粒度只能作用到方法级别，无法做到像编程式事务那样可以作用到代码块级别。但是即便有这样的需求，也存在很多变通的方法，比如，可以将需要进行事务管理的代码块独立为方法等等。 声明式事务管理也有两种常用的方式，一种是基于tx和aop名字空间的xml配置文件，另一种就是基于@Transactional注解。显然基于注解的方式更简单易用，更清爽。 自动提交(AutoCommit)与连接关闭时的是否自动提交自动提交 默认情况下，数据库处于自动提交模式。每一条语句处于一个单独的事务中，在这条语句执行完毕时，如果执行成功则隐式的提交事务，如果执行失败则隐式的回滚事务。 对于正常的事务管理，是一组相关的操作处于一个事务之中，因此必须关闭数据库的自动提交模式。不过，这个我们不用担心，spring会将底层连接的自动提交特性设置为false。 12345678910 1 // switch to manual commit if necessary. this is very expensive in some jdbc drivers, 2 // so we don't want to do it unnecessarily (for example if we've explicitly 3 // configured the connection pool to set it already). 4 if (con.getautocommit()) &#123; 5 txobject.setmustrestoreautocommit(true); 6 if (logger.isdebugenabled()) &#123; 7 logger.debug("switching jdbc connection [" + con + "] to manual commit"); 8 &#125; 9 con.setautocommit(false);10 &#125; 有些数据连接池提供了关闭事务自动提交的设置，最好在设置连接池时就将其关闭。但C3P0没有提供这一特性，只能依靠spring来设置。因为JDBC规范规定，当连接对象建立时应该处于自动提交模式，这是跨DBMS的缺省值，如果需要,必须显式的关闭自动提交。C3P0遵守这一规范，让客户代码来显式的设置需要的提交模式。 连接关闭时的是否自动提交 当一个连接关闭时，如果有未提交的事务应该如何处理？JDBC规范没有提及，C3P0默认的策略是回滚任何未提交的事务。这是一个正确的策略，但JDBC驱动提供商之间对此问题并没有达成一致。C3P0的autoCommitOnClose属性默认是false,没有十分必要不要动它。或者可以显式的设置此属性为false，这样会更明确。 基于注解的声明式事务管理配置XML配置文件 12345671 &lt;!-- transaction support--&gt;2 &lt;!-- PlatformTransactionMnager --&gt;3 &lt;bean id="txManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt;4 &lt;property name="dataSource" ref="dataSource" /&gt;5 &lt;/bean&gt;6 &lt;!-- enable transaction annotation support --&gt;7 &lt;tx:annotation-driven transaction-manager="txManager" /&gt; 还要在xml配置文件中添加tx名字空间 123456789101112 1 ... 2 xmlns:tx="http://www.springframework.org/schema/tx" 3 xmlns:aop="http://www.springframework.org/schema/aop" 4 xsi:schemaLocation=" 5 ... 6 7 http://www.springframework.org/schema/tx 8 9 10 http://www.springframework.org/schema/tx/spring-tx.xsd11 12 ... MyBatis自动参与到spring事务管理中，无需额外配置，只要org.mybatis.spring.SqlSessionFactoryBean引用的数据源与DataSourceTransactionManager引用的数据源一致即可，否则事务管理会不起作用。 spring事务特性spring所有的事务管理策略类都继承自org.springframework.transaction.PlatformTransactionManager接口。 1234567891 public interface PlatformTransactionManager &#123;2 3 TransactionStatus getTransaction(TransactionDefinition definition)4 throws TransactionException;5 6 void commit(TransactionStatus status) throws TransactionException;7 8 void rollback(TransactionStatus status) throws TransactionException;9 &#125; 其中TransactionDefinition接口定义以下特性： 事务隔离级别 隔离级别是指若干个并发的事务之间的隔离程度。TransactionDefinition 接口中定义了五个表示隔离级别的常量，关于隔离级别的介绍可参考这篇博客： TransactionDefinition.ISOLATION_DEFAULT：这是默认值，表示使用底层数据库的默认隔离级别。对大部分数据库而言，通常这值就是TransactionDefinition.ISOLATION_READ_COMMITTED。 TransactionDefinition.ISOLATION_READ_UNCOMMITTED：该隔离级别表示一个事务可以读取另一个事务修改但还没有提交的数据。该级别不能防止脏读，不可重复读和幻读，因此很少使用该隔离级别。比如PostgreSQL实际上并没有此级别。 TransactionDefinition.ISOLATION_READ_COMMITTED：该隔离级别表示一个事务只能读取另一个事务已经提交的数据。该级别可以防止脏读，这也是大多数情况下的推荐值。 TransactionDefinition.ISOLATION_REPEATABLE_READ：该隔离级别表示一个事务在整个过程中可以多次重复执行某个查询，并且每次返回的记录都相同。该级别可以防止脏读和不可重复读。 TransactionDefinition.ISOLATION_SERIALIZABLE：所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。 大多数的数据库默认隔离级别为 Read Commited，比如 SqlServer、Oracle 少数数据库默认隔离级别为：Repeatable Read 比如： MySQL、 InnoDB 事务传播行为所谓事务的传播行为是指，如果在开始当前事务之前，一个事务上下文已经存在，此时有若干选项可以指定一个事务性方法的执行行为。在TransactionDefinition定义中包括了如下几个表示传播行为的常量： TransactionDefinition.PROPAGATION_REQUIRED：如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。这是默认值。 TransactionDefinition.PROPAGATION_REQUIRES_NEW：创建一个新的事务，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_SUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED：以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER：以非事务方式运行，如果当前存在事务，则抛出异常。 TransactionDefinition.PROPAGATION_MANDATORY：如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。 TransactionDefinition.PROPAGATION_NESTED：如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。 事务嵌套1234567voidmethodA() &#123;try&#123;ServiceB.methodB();&#125;catch(SomeException) &#123;// 执行其他业务, 如 ServiceC.methodC();&#125;&#125; 假设外层事务 Service A 的 Method A() 调用 内层Service B 的 Method B() PROPAGATION_REQUIRED(spring 默认)如果ServiceB.methodB() 的事务级别定义为 PROPAGATION_REQUIRED，那么执行 ServiceA.methodA() 的时候spring已经起了事务，这时调用 ServiceB.methodB()，ServiceB.methodB() 看到自己已经运行在 ServiceA.methodA() 的事务内部，就不再起新的事务。 假如 ServiceB.methodB() 运行的时候发现自己没有在事务中，他就会为自己分配一个事务。 这样，在 ServiceA.methodA() 或者在 ServiceB.methodB() 内的任何地方出现异常，事务都会被回滚。 PROPAGATION_REQUIRES_NEW比如我们设计 ServiceA.methodA() 的事务级别为 PROPAGATION_REQUIRED，ServiceB.methodB() 的事务级别为 PROPAGATION_REQUIRES_NEW。 那么当执行到 ServiceB.methodB() 的时候，ServiceA.methodA() 所在的事务就会挂起，ServiceB.methodB() 会起一个新的事务，等待 ServiceB.methodB() 的事务完成以后，它才继续执行。 他与 PROPAGATION_REQUIRED 的事务区别在于事务的回滚程度了。因为 ServiceB.methodB() 是新起一个事务，那么就是存在两个不同的事务。如果 ServiceB.methodB() 已经提交，那么 ServiceA.methodA() 失败回滚，ServiceB.methodB() 是不会回滚的。如果 ServiceB.methodB() 失败回滚，如果他抛出的异常被 ServiceA.methodA() 捕获，ServiceA.methodA() 事务仍然可能提交(主要看B抛出的异常是不是A会回滚的异常)。 PROPAGATION_SUPPORTS假设ServiceB.methodB() 的事务级别为 PROPAGATION_SUPPORTS，那么当执行到ServiceB.methodB()时，如果发现ServiceA.methodA()已经开启了一个事务，则加入当前的事务，如果发现ServiceA.methodA()没有开启事务，则自己也不开启事务。这种时候，内部方法的事务性完全依赖于最外层的事务。 PROPAGATION_NESTED现在的情况就变得比较复杂了, ServiceB.methodB() 的事务属性被配置为 PROPAGATION_NESTED, 此时两者之间又将如何协作呢? ServiceB#methodB 如果 rollback, 那么内部事务(即 ServiceB#methodB) 将回滚到它执行前的 SavePoint 而外部事务(即 ServiceA#methodA) 可以有以下两种处理方式: a、捕获异常，执行异常分支逻辑 这种方式也是嵌套事务最有价值的地方, 它起到了分支执行的效果, 如果 ServiceB.methodB 失败, 那么执行 ServiceC.methodC(), 而 ServiceB.methodB 已经回滚到它执行之前的 SavePoint, 所以不会产生脏数据(相当于此方法从未执行过), 这种特性可以用在某些特殊的业务中, 而 PROPAGATION_REQUIRED 和 PROPAGATION_REQUIRES_NEW 都没有办法做到这一点。 b、 外部事务回滚/提交 如果内部事务(ServiceB#methodB) rollback, 那么首先 ServiceB.methodB 回滚到它执行之前的 SavePoint(在任何情况下都会如此), 外部事务(即 ServiceA#methodA) 将根据具体的配置决定自己是 commit 还是 rollback。 另外三种事务传播属性基本用不到，在此不做分析。 事务超时所谓事务超时，就是指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。在 TransactionDefinition 中以 int 的值来表示超时时间，其单位是秒。 默认设置为底层事务系统的超时值，如果底层数据库事务系统没有设置超时值，那么就是none，没有超时限制。 事务只读属性只读事务用于客户代码只读但不修改数据的情形，只读事务用于特定情景下的优化，比如使用Hibernate的时候。默认为读写事务。 spring事务回滚规则指示spring事务管理器回滚一个事务的推荐方法是在当前事务的上下文内抛出异常。spring事务管理器会捕捉任何未处理的异常，然后依据规则决定是否回滚抛出异常的事务。 默认配置下，spring只有在抛出的异常为运行时unchecked异常时才回滚该事务，也就是抛出的异常为RuntimeException的子类(Errors也会导致事务回滚)，而抛出checked异常则不会导致事务回滚。可以明确的配置在抛出那些异常时回滚事务，包括checked异常。也可以明确定义那些异常抛出时不回滚事务。 还可以编程性的通过setRollbackOnly()方法来指示一个事务必须回滚，在调用完setRollbackOnly()后你所能执行的唯一操作就是回滚。 @Transactional注解@Transactional属性 属性 类型 描述 value String 可选的限定描述符，指定使用的事务管理器 propagation enum: Propagation 可选的事务传播行为设置 isolation enum: Isolation 可选的事务隔离级别设置 readOnly boolean 读写或只读事务，默认读写 timeout int (in seconds granularity) 事务超时时间设置 rollbackFor Class对象数组，必须继承自Throwable 导致事务回滚的异常类数组 rollbackForClassName 类名数组，必须继承自Throwable 导致事务回滚的异常类名字数组 noRollbackFor Class对象数组，必须继承自Throwable 不会导致事务回滚的异常类数组 noRollbackForClassName 类名数组，必须继承自Throwable 不会导致事务回滚的异常类名字数组 用法@Transactional 可以作用于接口、接口方法、类以及类方法上。当作用于类上时，该类的所有 public 方法将都具有该类型的事务属性，同时，我们也可以在方法级别使用该标注来覆盖类级别的定义。 虽然 @Transactional 注解可以作用于接口、接口方法、类以及类方法上，但是 Spring 建议不要在接口或者接口方法上使用该注解，因为这只有在使用基于接口的代理时它才会生效。另外， @Transactional 注解应该只被应用到 public 方法上，这是由 Spring AOP 的本质决定的。如果你在 protected、private 或者默认可见性的方法上使用 @Transactional 注解，这将被忽略，也不会抛出任何异常。 默认情况下，只有来自外部的方法调用才会被AOP代理捕获，也就是，类内部方法调用本类内部的其他方法并不会引起事务行为，即使被调用方法使用@Transactional注解进行修饰。 参考：https://www.cnblogs.com/yepei/p/4716112.html https://www.jianshu.com/p/d1c7b14ea9d7 http://www.codeceo.com/article/spring-transactions.html https://blog.csdn.net/baidu_37107022/article/details/75578140]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>一致性</tag>
        <tag>Spring</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式之代理模式]]></title>
    <url>%2F2018%2F10%2F27%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[什么是代理模式？ 日常生活中我们经常会碰到代理模式，例如我们找房产中介帮我们介绍房子，找婚姻中介帮我们介绍对象，找保洁帮我们打理房间，找律师帮我们进行诉讼等。我们在无形中都运用到了代理模式。 为什么要使用代理？ 运用代理可以使我们的生活更加便利，有了代理，我们不需要自己去找房子，不需要自己去找对象，不需要自己去打理房间，不需要自己去诉讼。当然，你也可以选择一切都自己来干，但是存在前提条件，一是你是否都具备这样的资源和能力来做这些事情，二是你是否愿意花费这么多精力和时间来做这些事情。总之，代理模式使我们各专其事，我们可以将时间浪费在美好的事情上，而不用天天被一些琐事所羁绊。 代理模式有哪些实现？ 代理的实现分动态代理和静态代理，静态代理的实现是对已经生成了的JAVA类进行封装。 动态代理则是在运行时生成了相关代理类，在JAVA中生成动态代理一般有三种方式。 静态代理 - 由程序员创建或特定工具自动生成源代码再对其编译。在程序运行前代理类的.class文件就已经存在了。 动态代理 - 在程序运行时运用反射机制动态创建而成。 静态代理代理接口：UserDao.java 123public interface UserDao &#123; void save();&#125; 目标对象：UserDaoImpl.java 123456public class UserDaoImpl implements UserDao &#123; @Override public void save() &#123; System.out.println("正在保存用户信息。。。"); &#125;&#125; 代理对象：TransactionHandler.java 123456789101112131415161718192021public class TransactionHandler implements UserDao &#123; private UserDaoImpl target; public TransactionHandler(UserDaoImpl target)&#123; this.target = target; &#125; @Override public void save() &#123; System.out.println("开启事务控制。。。"); target.save(); System.out.println("关闭事务控制。。。"); &#125; public static void main(String[] args) &#123; UserDaoImpl target = new UserDaoImpl(); UserDao userDao = new TransactionHandler(target); userDao.save(); &#125;&#125; 总的来说静态代理实现简单也容易理解，但是静态代理不能使一个代理类反复作用于多个目标对象，代理对象直接持有目标对象的引用，这导致代理对象和目标对象类型紧密耦合了在一起。如果UserDao接口下还有另一个实现类也需要进行事务控制，那么就要重新写一个代理类，这样就会产生许多重复的模版代码，不能达到代码复用的目的。而动态代理就可以很好的解决这样的问题。 动态代理Spring AOP的拦截功能是由java中的动态代理来实现的。说白了，就是在目标类的基础上增加切面逻辑，生成增强的目标类（该切面逻辑或者在目标类函数执行之前，或者目标类函数执行之后，或者在目标类函数抛出异常时候执行。不同的切入时机对应不同的Interceptor的种类，如BeforeAdviseInterceptor，AfterAdviseInterceptor以及ThrowsAdviseInterceptor等）。 Spring AOP的源码中用到了两种动态代理来实现拦截切入功能：jdk动态代理和cglib动态代理（核心）。两种方法同时存在，各有优劣。jdk动态代理是由java内部的反射机制来实现的，cglib动态代理底层则是借助asm来实现的。总的来说，反射机制在生成类的过程中比较高效，而asm在生成类之后的相关执行过程中比较高效（可以通过将asm生成的类进行缓存，这样解决asm生成类过程低效问题）。还有一点必须注意：jdk动态代理的应用前提，必须是目标类基于统一的接口。如果没有上述前提，jdk动态代理不能应用。由此可以看出，jdk动态代理有一定的局限性，cglib这种第三方类库实现的动态代理应用更加广泛，且在效率上更有优势。 这里仍然使用上面同样的代理接口和目标对象。 InvocationHandler方式实现动态代理（JDK方式）123456789101112131415161718192021222324252627282930313233343536373839import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;/** * InvocationHandler方式实现动态代理（JDK动态代理） * */public class TransactionHandler1 implements InvocationHandler &#123; private Object target; public TransactionHandler1(Object target) &#123; this.target = target; &#125; /** * @param proxy 参数是Object类型，是目标类的类加载器 * @param method 参数是Method类型，是目标类实现的接口集合 * @param args 参数是Object类型，是InvocationHandler实例 */ @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("开启事务控制。。。"); Object result = method.invoke(target, args); System.out.println("关闭事务控制。。。"); return result; &#125; public static void main(String[] args) &#123; Object target = new UserDaoImpl(); TransactionHandler1 handler = new TransactionHandler1(target); UserDao userDao = (UserDao) Proxy.newProxyInstance( target.getClass().getClassLoader(), target.getClass().getInterfaces(), handler); userDao.save(); &#125;&#125; 之前我们发现了静态代理会产生许多重复代码，不能很好的进行代码复用，而动态代理能够很好的解决这个问题，代理类TransactionHandler实现了InvocationHandler接口，并且它持有的目标对象类型是Object，因此事务控制代理类TransactionHandler能够代理任意的对象，为任意的对象添加事务控制的逻辑。因此动态代理才真正的将代码中横向切面的逻辑剥离了出来，起到代码复用的目的。 但是JDK动态代理也有缺点，一是它的实现比静态代理更加复杂也不好理解；二是它存在一定的限制，例如它要求目标对象必须实现了某个接口；三是它不够灵活，JDK动态代理会为接口中的声明的所有方法添加上相同的代理逻辑。 cglib动态代理实现CGLIB是一个开源项目，官方网址是：http://cglib.sourceforge.net/，可以去上面下载最新JAR包， Cglib是一个优秀的动态代理框架，它的底层使用ASM在内存中动态的生成被代理类的子类，使用CGLIB即使代理类没有实现任何接口也可以实现动态代理功能。CGLIB具有简单易用，它的运行速度要远远快于JDK的Proxy动态代理： cglib有两种可选方式，继承和引用。第一种是基于继承实现的动态代理，所以可以直接通过super调用target方法，但是这种方式在spring中是不支持的，因为这样的话，这个target对象就不能被spring所管理，所以cglib还是才用类似jdk的方式，通过持有target对象来达到拦截方法的效果。 CGLIB的核心类：​ cglib.proxy.Enhancer – 主要的增强类​ cglib.proxy.MethodInterceptor – 主要的方法拦截类，它是Callback接口的子接口，需要用户实现​ cglib.proxy.MethodProxy – JDK的java.lang.reflect.Method类的代理类，可以方便的实现对源对象方法的调用,如使用：​ Object o = methodProxy.invokeSuper(proxy, args);//虽然第一个参数是被代理对象，也不会出现死循环的问题。 cglib.proxy.MethodInterceptor接口是最通用的回调（callback）类型，它经常被基于代理的AOP用来实现拦截（intercept）方法的调用。这个接口只定义了一个方法public Object intercept(Object object, java.lang.reflect.Method method,Object[] args, MethodProxy proxy) throws Throwable; 第一个参数是代理对像，第二和第三个参数分别是拦截的方法和方法的参数。原来的方法可能通过使用java.lang.reflect.Method对象的一般反射调用，或者使用 net.sf.cglib.proxy.MethodProxy对象调用。net.sf.cglib.proxy.MethodProxy通常被首选使用，因为它更快。 本项目用的是springframework中的cglib 实现方式如下 12345678910111213141516171819202122232425262728293031import org.springframework.cglib.proxy.Enhancer;import org.springframework.cglib.proxy.MethodInterceptor;import org.springframework.cglib.proxy.MethodProxy;import java.lang.reflect.Method;/** * CGLIB方式实现动态代理（Spring使用这种方式实现） * */ public class TransactionHandler2 implements MethodInterceptor &#123; @Override public Object intercept(Object target, Method method, Object[] args, MethodProxy methodProxy)throws Throwable&#123; System.out.println("开启事务控制。。。"); Object result = methodProxy.invokeSuper(target,args); System.out.println("关闭事务控制。。。"); return result; &#125; public static void main(String[] args) &#123; TransactionHandler2 cglibProxy = new TransactionHandler2(); Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(UserDaoImpl.class); enhancer.setCallback(cglibProxy); UserDao userDao = (UserDao)enhancer.create(); userDao.save(); &#125;&#125; Cglib代理工厂实现方式如下 12345678910111213141516171819202122232425262728293031323334353637383940414243import org.springframework.cglib.proxy.Enhancer;import org.springframework.cglib.proxy.MethodInterceptor;import org.springframework.cglib.proxy.MethodProxy;import java.lang.reflect.Method;/** * CGLIB代理工厂方式实现动态代理（Spring使用这种方式实现） * */public class ProxyFactory implements MethodInterceptor &#123; private Object target; public ProxyFactory(Object target) &#123; this.target = target; &#125; public Object getProxyInstance()&#123; // 1.工具类 Enhancer en = new Enhancer(); // 2.设置父类 en.setSuperclass(target.getClass()); // 3.设置回调函数 en.setCallback(this); // 4.创建子类(代理对象) return en.create(); &#125; @Override public Object intercept(Object target, Method method, Object[] args, MethodProxy methodProxy)throws Throwable&#123; System.out.println("开启事务控制。。。"); Object result = methodProxy.invokeSuper(target,args); System.out.println("关闭事务控制。。。"); return result; &#125; public static void main(String[] args) &#123; // 目标对象 UserDao target = new UserDaoImpl(); // 代理对象 UserDao userDao = (UserDao)new ProxyFactory(target).getProxyInstance(); userDao.save(); &#125;&#125; 通过编译期提供的API实现动态代理假设我们确实需要给一个既是final，又未实现任何接口的ProductOwner类创建动态代码。除了InvocationHandler和CGLIB外，我们还有最后一招： 我直接把一个代理类的源代码用字符串拼出来，然后基于这个字符串调用JDK的Compiler（编译期）API，动态的创建一个新的.java文件，然后动态编译这个.java文件，这样也能得到一个新的代理类。 参考：http://www.cnblogs.com/cenyu/p/6289209.html https://www.cnblogs.com/liuyun1995/p/8144628.html https://blog.csdn.net/heyutao007/article/details/49738887 https://my.oschina.net/LiuLangEr/blog/1606667 http://www.cnblogs.com/springsource/archive/2012/08/30/2664050.html https://yq.aliyun.com/articles/629146]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring中的常用注解]]></title>
    <url>%2F2018%2F10%2F25%2FSpring%E4%B8%AD%E7%9A%8412%E7%A7%8D%E5%B8%B8%E7%94%A8%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[注解本身没有功能的，就和xml一样。注解和xml都是一种元数据，元数据即解释数据的数据，这就是所谓配置。 本文主要罗列Spring|SpringMVC相关注解的简介。 Spring部分声明bean的注解@Component 组件，没有明确的角色 @Service 在业务逻辑层使用（service层） @Repository 在数据访问层使用（dao层） @Controller 在展现层使用，控制器的声明（C） 注入bean的注解@Autowired：由Spring提供 @Inject：由JSR-330提供 @Resource：由JSR-250提供 都可以注解在set方法和属性上，推荐注解在属性上（一目了然，少写代码）。 java配置类相关注解@Configuration 声明当前类为配置类，相当于xml形式的Spring配置（类上） @Bean 注解在方法上，声明当前方法的返回值为一个bean，替代xml中的方式（方法上） @Configuration 声明当前类为配置类，其中内部组合了@Component注解，表明这个类是一个bean（类上） @ComponentScan 用于对Component进行扫描，相当于xml中的（类上） @WishlyConfiguration 为@Configuration与@ComponentScan的组合注解，可以替代这两个注解 切面（AOP）相关注解Spring支持AspectJ的注解式切面编程。 @Aspect 声明一个切面（类上）使用@After、@Before、@Around定义建言（advice），可直接将拦截规则（切点）作为参数。 @After 在方法执行之后执行（方法上）@Before 在方法执行之前执行（方法上）@Around 在方法执行之前与之后执行（方法上） @PointCut 声明切点 在java配置类中使用@EnableAspectJAutoProxy注解开启Spring对AspectJ代理的支持（类上） @Bean的属性支持@Scope 设置Spring容器如何新建Bean实例（方法上，得有@Bean）其设置类型包括： Singleton （单例,一个Spring容器中只有一个bean实例，默认模式）,Protetype （每次调用新建一个bean）,Request （web项目中，给每个http request新建一个bean）,Session （web项目中，给每个http session新建一个bean）,GlobalSession（给每一个 global http session新建一个Bean实例） @StepScope 在Spring Batch中还有涉及 @PostConstruct 由JSR-250提供，在构造函数执行完之后执行，等价于xml配置文件中bean的initMethod @PreDestory 由JSR-250提供，在Bean销毁之前执行，等价于xml配置文件中bean的destroyMethod @Value注解@Value 为属性注入值（属性上）支持如下方式的注入：》注入普通字符 12@Value("Michael Jackson")String name; 》注入操作系统属性 12@Value("#&#123;systemProperties['os.name']&#125;")String osName; 》注入表达式结果 12@Value("#&#123; T(java.lang.Math).random() * 100 &#125;")String randomNumber; 》注入其它bean属性 12@Value("#&#123;domeClass.name&#125;")String name; 》注入文件资源 12@Value("classpath:com/hgs/hello/test.txt")Resource file; 》注入网站资源 12@Value("http://www.cznovel.com")Resource url; 》注入配置文件 12@Value("$&#123;book.name&#125;")String bookName; 》注入配置使用方法：① 编写配置文件（test.properties） 1book.name=《三体》 ② @PropertySource 加载配置文件(类上) 1@PropertySource("classpath:com/hgs/hello/test/test.propertie") ③ 还需配置一个PropertySourcesPlaceholderConfigurer的bean。 环境切换@Profile 通过设定Environment的ActiveProfiles来设定当前context需要使用的配置环境。（类或方法上） @Conditional Spring4中可以使用此注解定义条件话的bean，通过实现Condition接口，并重写matches方法，从而决定该bean是否被实例化。（方法上） 异步相关@EnableAsync 配置类中，通过此注解开启对异步任务的支持，叙事性AsyncConfigurer接口（类上） @Async 在实际执行的bean方法使用该注解来申明其是一个异步任务（方法上或类上所有的方法都将异步，需要@EnableAsync开启异步任务） 定时任务相关@EnableScheduling 在配置类上使用，开启计划任务的支持（类上） @Scheduled 来申明这是一个任务，包括cron,fixDelay,fixRate等类型（方法上，需先开启计划任务的支持） @Enable*注解说明这些注解主要用来开启对xxx的支持。@EnableAspectJAutoProxy 开启对AspectJ自动代理的支持 @EnableAsync 开启异步方法的支持 @EnableScheduling 开启计划任务的支持 @EnableWebMvc 开启Web MVC的配置支持 @EnableConfigurationProperties 开启对@ConfigurationProperties注解配置Bean的支持 @EnableJpaRepositories 开启对SpringData JPA Repository的支持 @EnableTransactionManagement 开启注解式事务的支持 @EnableTransactionManagement 开启注解式事务的支持 @EnableCaching 开启注解式的缓存支持 测试相关注解@RunWith 运行器，Spring中通常用于对JUnit的支持 1@RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration 用来加载配置ApplicationContext，其中classes属性用来加载配置类 1@ContextConfiguration(classes=&#123;TestConfig.class&#125;) SpringMVC部分@EnableWebMvc 在配置类中开启Web MVC的配置支持，如一些ViewResolver或者MessageConverter等，若无此句，重写WebMvcConfigurerAdapter方法（用于对SpringMVC的配置）。 @Controller 声明该类为SpringMVC中的Controller @RequestMapping 用于映射Web请求，包括访问路径和参数（类或方法上） @ResponseBody 支持将返回值放在response内，而不是一个页面，通常用户返回json数据（返回值旁或方法上） @RequestBody 允许request的参数在request体中，而不是在直接连接在地址后面。（放在参数前） @PathVariable 用于接收路径参数，比如@RequestMapping(“/hello/{name}”)申明的路径，将注解放在参数中前，即可获取该值，通常作为Restful的接口实现方法。 @RestController 该注解为一个组合注解，相当于@Controller和@ResponseBody的组合，注解在类上，意味着，该Controller的所有方法都默认加上了@ResponseBody。 @ControllerAdvice 通过该注解，我们可以将对于控制器的全局配置放置在同一个位置，注解了@Controller的类的方法可使用@ExceptionHandler、@InitBinder、@ModelAttribute注解到方法上，这对所有注解了 @RequestMapping的控制器内的方法有效。 @ExceptionHandler 用于全局处理控制器里的异常 @InitBinder 用来设置WebDataBinder，WebDataBinder用来自动绑定前台请求参数到Model中。 @ModelAttribute 本来的作用是绑定键值对到Model里，在@ControllerAdvice中是让全局的@RequestMapping都能获得在此处设置的键值对。 参考：https://blog.csdn.net/IT_faquir/article/details/78025203]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[concurrentHashmap的设计艺术]]></title>
    <url>%2F2018%2F10%2F24%2FconcurrentHashmap%E7%9A%84%E8%AE%BE%E8%AE%A1%E8%89%BA%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[concurrentHashmap是Java工程师接触最多的关于并发和线程安全的类，本篇从jdk1.6, jdk1.7, jdk1.8三个版本的实现来详细分析其设计艺术。 结构原型首先简单从整体把握concurrentHashmap的结构原理。 JDK1.6和JDK1.7版本 该版本的ConcurrentHashMap结构：每一个segment都是一个HashEntry[] table， table中的每一个元素本质上都是一个HashEntry的单向队列（单向链表实现）。每一个segment都是一个HashEntry[] table， table中的每一个元素本质上都是一个HashEntry的单向队列。 JDK1.8版本 该版本的ConcurrentHashMap结构：放弃了Segment的概念，而是直接用Node数组+链表+红黑树的数据结构来实现，并发控制使用Synchronized和CAS来操作，整个看起来就像是优化过且线程安全的HashMap。其中Node数组里面的元素包括TreeBin，ForwardingNode ，Node 。因为放弃了Segment，所以具体并发级别可以认为是hash桶是数量，也就是容量，会随扩容而改变，不再是固定值。 构造函数名词解释initialCapacity：初始化容量，指的是所有Segment中的hash桶的数量和，默认16。 concurrencyLevel：最大并发级别，也是数组segments的最大长度，初始化之后就不会改变，实际扩容的是每个segment里的hash桶，默认16。 loadFactor：每个Segment的加载因子，当个数大于threshold=hash桶数量*loadFactor时候会扩容。 Unsafe：这是jdk1.7，1.8常用的机制，可以看这篇文章。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778&lt;------------------------------jdk1.7版本--------------------------------------&gt;private static final sun.misc.Unsafe UNSAFE; // Segment数组第一个元素的地址相对于该对象实例起始地址的偏移量private static final long SBASE; // Segment数组中元素element基本类型大小的移位量，比如我们要在i位置加入元素，//用unsafe.putOrderedObject(segments, (i &lt;&lt; SSHIFT) + SBASE, element);private static final int SSHIFT; //HashEntry数组第一个元素的地址相对于该对象实例起始地址的偏移量 private static final long TBASE; //HashEntry数组中元素基本类型大小的移位量private static final int TSHIFT; // ConcurrentHashMap属性hashSeed,segmentShift,segmentMask,segments的相对实例地址的偏移量private static final long HASHSEED_OFFSET; private static final long SEGSHIFT_OFFSET; private static final long SEGMASK_OFFSET; private static final long SEGMENTS_OFFSET; static &#123; int ss, ts; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class tc = HashEntry[].class; Class sc = Segment[].class; //获取数组中第一个元素的偏移量(get offset of a first element in the array) TBASE = UNSAFE.arrayBaseOffset(tc); SBASE = UNSAFE.arrayBaseOffset(sc); //获取数组中一个元素的大小(get size of an element in the array) ts = UNSAFE.arrayIndexScale(tc); ss = UNSAFE.arrayIndexScale(sc); //其中hashSeed = randomHashSeed(this)，随机一个hashSeed，用于key的hash计算 HASHSEED_OFFSET = UNSAFE.objectFieldOffset(ConcurrentHashMap.class.getDeclaredField("hashSeed")); SEGSHIFT_OFFSET = UNSAFE.objectFieldOffset(ConcurrentHashMap.class.getDeclaredField("segmentShift")); SEGMASK_OFFSET = UNSAFE.objectFieldOffset(ConcurrentHashMap.class.getDeclaredField("segmentMask")); SEGMENTS_OFFSET = UNSAFE.objectFieldOffset(ConcurrentHashMap.class.getDeclaredField("segments")); &#125; catch (Exception e) &#123; throw new Error(e); &#125; if ((ss &amp; (ss-1)) != 0 || (ts &amp; (ts-1)) != 0) throw new Error("data type scale not a power of two"); SSHIFT = 31 - Integer.numberOfLeadingZeros(ss); TSHIFT = 31 - Integer.numberOfLeadingZeros(ts); &#125; &lt;---------------------------------jdk1.8版本---------------------------------&gt;// Unsafe mechanics private static final sun.misc.Unsafe U; private static final long SIZECTL; private static final long TRANSFERINDEX; private static final long BASECOUNT; private static final long CELLSBUSY; private static final long CELLVALUE; private static final long ABASE; private static final int ASHIFT; static &#123; try &#123; U = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = ConcurrentHashMap.class; SIZECTL = U.objectFieldOffset (k.getDeclaredField("sizeCtl")); TRANSFERINDEX = U.objectFieldOffset (k.getDeclaredField("transferIndex")); BASECOUNT = U.objectFieldOffset (k.getDeclaredField("baseCount")); CELLSBUSY = U.objectFieldOffset (k.getDeclaredField("cellsBusy")); Class&lt;?&gt; ck = CounterCell.class; CELLVALUE = U.objectFieldOffset (ck.getDeclaredField("value")); Class&lt;?&gt; ak = Node[].class; ABASE = U.arrayBaseOffset(ak); int scale = U.arrayIndexScale(ak); if ((scale &amp; (scale - 1)) != 0) throw new Error("data type scale not a power of two"); ASHIFT = 31 - Integer.numberOfLeadingZeros(scale); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125; sizeCtl: java1.8版本特有的，用于数组的初始化与扩容控制；当sizeCtl = -1 时，表明table正在初始化；当sizeCtl = 0时，表明用了无参构造方法，没有指定初始容量默认值；当sizeCtl &gt; 0时，表明用了传参构造方法，指定了初始化容量值；当sizeCtl = -(1+ N)，表明在扩容， 其中N的低RESIZE_STAMP_SHIFT位表示参与扩容线程数。 JDK1.6版本12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; //默认loadFactor=DEFAULT_LOAD_FACTOR=0.75f if (!(loadFactor &gt; 0) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); //默认concurrencyLevel=DEFAULT_CONCURRENCY_LEVEL=16， //MAX_SEGMENTS =1&lt;&lt;16即数组segments的最大长度，也是最大并发级别； if (concurrencyLevel &gt; MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; //保证concurrencyLevel是2^n，默认ssize=16； int sshift = 0; int ssize = 1; while (ssize &lt; concurrencyLevel) &#123; ++sshift; ssize &lt;&lt;= 1; &#125; //定位Segment的index时hash值的移位,默认时候sshift=4,segmentShift=32-4=28； segmentShift = 32 - sshift; //用于&amp; 运算定位Segment的index，默认构造时是0x0f(十进制也就是15) segmentMask = ssize - 1; this.segments = Segment.newArray(ssize); //默认initialCapacity=DEFAULT_INITIAL_CAPACITY=16 //MAXIMUM_CAPACITY=1&lt;&lt;30 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; //确定每个segment初始化时候有多少个hash桶，并保证为2^n,默认cap = 1,但是之后每个segment构造完成后这个cap值就没用了。 int c = initialCapacity / ssize; if (c * ssize &lt; initialCapacity) ++c; int cap = 1; while (cap &lt; c) cap &lt;&lt;= 1; //确定cap后开始初始化每个segment的table for (int i = 0; i &lt; this.segments.length; ++i) this.segments[i] = new Segment&lt; K,V &gt; (cap, loadFactor);&#125;public ConcurrentHashMap(int initialCapacity, float loadFactor) &#123; this(initialCapacity, loadFactor, DEFAULT_CONCURRENCY_LEVEL);&#125;public ConcurrentHashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR, DEFAULT_CONCURRENCY_LEVEL);&#125;public ConcurrentHashMap() &#123; this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR, DEFAULT_CONCURRENCY_LEVEL);&#125;public ConcurrentHashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this(Math.max((int) (m.size() / DEFAULT_LOAD_FACTOR) + 1, DEFAULT_INITIAL_CAPACITY), DEFAULT_LOAD_FACTOR, DEFAULT_CONCURRENCY_LEVEL); putAll(m);&#125; JDK1.7版本1234567891011121314151617181920212223242526272829303132public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; if (!(loadFactor &gt; 0) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); if (concurrencyLevel &gt; MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; // Find power-of-two sizes best matching arguments int sshift = 0; int ssize = 1; while (ssize &lt; concurrencyLevel) &#123; ++sshift; ssize &lt;&lt;= 1; &#125; this.segmentShift = 32 - sshift; this.segmentMask = ssize - 1; if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; int c = initialCapacity / ssize; if (c * ssize &lt; initialCapacity) ++c; //cap = MIN_SEGMENT_TABLE_CAPACITY = 2，即每个segment容量最小初始化为2， //而jdk1.6默认cap=1 int cap = MIN_SEGMENT_TABLE_CAPACITY; while (cap &lt; c) cap &lt;&lt;= 1; //与jdk1.6不同点，这里只是构造一个segments和segments[0],懒加载 Segment&lt;K,V&gt; s0 = new Segment&lt;K,V&gt;(loadFactor, (int)(cap * loadFactor), (HashEntry&lt;K,V&gt;[])new HashEntry[cap]); Segment&lt;K,V&gt;[] ss = (Segment&lt;K,V&gt;[])new Segment[ssize]; //这里用UNSAFE给数组ss第一个元素赋值，内存非立即可见 UNSAFE.putOrderedObject(ss, SBASE, s0); this.segments = ss; &#125; // 其余的几个同上 JDK1.8版本123456789101112131415161718192021222324252627282930313233343536373839404142434445//这种初始化，什么都没有做，这时候sizeCtl还没有赋值 public ConcurrentHashMap() &#123; &#125;//保证cap是2^n，也就是求不小于initialCapacity的2^n public ConcurrentHashMap(int initialCapacity) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(); int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); //给sizeCtl赋值 this.sizeCtl = cap; &#125;private static final int tableSizeFor(int c) &#123; int n = c - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; &#125; //默认sizeCtl = 16 public ConcurrentHashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.sizeCtl = DEFAULT_CAPACITY; putAll(m); &#125; //为了兼容 public ConcurrentHashMap(int initialCapacity, float loadFactor) &#123; this(initialCapacity, loadFactor, 1); &#125; //loadFactor和concurrencyLevel失去原有的意义了，只为了兼容老版本，构造方法采用懒加载，在后面的put方法中会进行initTable()操作。假如initialCapacity = 16 ，loadFactor = 0.75 ，则size=21.333,那么sizeCtl = 2^5 = 32 public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; if (!(loadFactor &gt; 0.0f) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); if (initialCapacity &lt; concurrencyLevel) initialCapacity = concurrencyLevel; long size = (long)(1.0 + (long)initialCapacity / loadFactor); int cap = (size &gt;= (long)MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : tableSizeFor((int)size); this.sizeCtl = cap; &#125; 基本类HashEntry和Segments JDK1.6版本123456789101112131415161718static final class HashEntry&lt;K,V&gt; &#123; final K key; final int hash; volatile V value; final HashEntry&lt;K,V&gt; next; HashEntry(K key, int hash, HashEntry&lt;K,V&gt; next, V value) &#123; this.key = key; this.hash = hash; this.next = next; this.value = value; &#125; @SuppressWarnings("unchecked") static final &lt;K,V&gt; HashEntry&lt;K,V&gt;[] newArray(int i) &#123; return new HashEntry[i]; &#125; &#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123; private static final long serialVersionUID = 2249069246763182397L; //segment的table中HashEntry总数目，这里用volatile修饰，并发读的时候，避免无用读取操作 transient volatile int count; //反映segment是否被修改过 transient int modCount; // table的扩容阈值 transient int threshold; // 类似HashMap的table数组 transient volatile HashEntry&lt;K,V&gt;[] table; // 加载因子，确定扩容阈值，所有的segment都是一样的 final float loadFactor; Segment(int initialCapacity, float lf) &#123; loadFactor = lf; setTable(HashEntry.&lt;K,V&gt;newArray(initialCapacity)); &#125; @SuppressWarnings("unchecked") static final &lt;K,V&gt; Segment&lt;K,V&gt;[] newArray(int i) &#123; return new Segment[i]; &#125; // 只有持有本segment的锁或者是构造方法中才能调用这个方法 void setTable(HashEntry&lt;K,V&gt;[] newTable) &#123; //默认时候这里的newTable.length = 1 ， threshold = 0 threshold = (int)(newTable.length * loadFactor); table = newTable; &#125; // 跟indexFor算法一样 HashEntry&lt;K,V&gt; getFirst(int hash) &#123; HashEntry&lt;K,V&gt;[] tab = table; return tab[hash &amp; (tab.length - 1)]; &#125; // 加锁读取value，在直接读取value得到null时会调用，源码这里有英文注释：读到value为null，只有当某种重排序的HashEntry初始化代码让volatile变量初始化重排序到构造方法外面时才会出现，这一点旧的内存模型下是合法的，但是不知道会不会发生。 // stackoverflow有讨论:http://stackoverflow.com/questions/5002428/concurrenthashmap-reorder-instruction/5144515#5144515 //在新的内存模型下，是不会发生value= null的，jdk1.7修复了这个问题 V readValueUnderLock(HashEntry&lt;K,V&gt; e) &#123; lock(); try &#123; return e.value; &#125; finally &#123; unlock(); &#125; &#125; // 下面的方法是常见操作，get, put, remove, contains等等 // 读操作之get,直接读value是不用加锁的，碰到读到value == null，才加锁再读一次 V get(Object key, int hash) &#123; if (count != 0) &#123; // read-volatile HashEntry&lt;K,V&gt; e = getFirst(hash); while (e != null) &#123; if (e.hash == hash &amp;&amp; key.equals(e.key)) &#123; V v = e.value; if (v != null) return v; return readValueUnderLock(e); // recheck &#125; e = e.next; &#125; &#125; return null; &#125; boolean containsKey(Object key, int hash) &#123; if (count != 0) &#123; // read-volatile HashEntry&lt;K,V&gt; e = getFirst(hash); while (e != null) &#123; if (e.hash == hash &amp;&amp; key.equals(e.key)) return true; e = e.next; &#125; &#125; return false; &#125; //类似get boolean containsValue(Object value) &#123; if (count != 0) &#123; // read-volatile HashEntry&lt;K,V&gt;[] tab = table; int len = tab.length; for (int i = 0 ; i &lt; len; i++) &#123; for (HashEntry&lt;K,V&gt; e = tab[i]; e != null; e = e.next) &#123; V v = e.value; if (v == null) // recheck v = readValueUnderLock(e); if (value.equals(v)) return true; &#125; &#125; &#125; return false; &#125; //写操作之替换 boolean replace(K key, int hash, V oldValue, V newValue) &#123; lock(); try &#123; HashEntry&lt;K,V&gt; e = getFirst(hash); while (e != null &amp;&amp; (e.hash != hash || !key.equals(e.key))) e = e.next; boolean replaced = false; if (e != null &amp;&amp; oldValue.equals(e.value)) &#123; // replace不会修改modCount，这降低了containValue方法的准确性，jdk1.7修复了这一点 replaced = true; e.value = newValue; &#125; return replaced; &#125; finally &#123; unlock(); &#125; &#125; //写操作之替换 V replace(K key, int hash, V newValue) &#123; lock(); try &#123; HashEntry&lt;K,V&gt; e = getFirst(hash); while (e != null &amp;&amp; (e.hash != hash || !key.equals(e.key))) e = e.next; V oldValue = null; if (e != null) &#123; // replace不会修改modCount，这降低了containValue方法的准确性，jdk1.7修复了这一点 oldValue = e.value; e.value = newValue; &#125; return oldValue; &#125; finally &#123; unlock(); &#125; &#125; // 写操作之put V put(K key, int hash, V value, boolean onlyIfAbsent) &#123; lock(); try &#123; int c = count; //先执行扩容，再添加节点，1.6的HashMap是先添加节点，再扩容,由于用的是c++ &gt; threshold,如果 threshold = 12，那么在这个Segment上执行第13次put时，判断语句为 12++ &gt; 12是为false，并未扩容，在第14次 13++&gt;12才扩容的，但是会保证默认情况第二次才扩容：每个Segment的table.length都为1，threthold = 0的，第一次put时候0++ &gt;0是为false,并未扩容，第二次1++&gt;0才会扩容。 if (c++ &gt; threshold) // ensure capacity rehash(); HashEntry&lt;K,V&gt;[] tab = table; int index = hash &amp; (tab.length - 1); // 定位方式和1.6的HashMap一样 HashEntry&lt;K,V&gt; first = tab[index]; HashEntry&lt;K,V&gt; e = first; while (e != null &amp;&amp; (e.hash != hash || !key.equals(e.key))) e = e.next; V oldValue; if (e != null) &#123; oldValue = e.value; if (!onlyIfAbsent) // put相同的key（相当于replace）不会修改modCount，这降低了containsValue方法的准确性，jdk1.7修复了这一点 e.value = value; &#125; else &#123; oldValue = null; ++modCount; // 也是添加在HashEntry链的头部，前面说了，这里的HashEntry的next指针是final的，new后就不能变 tab[index] = new HashEntry&lt;K,V&gt;(key, hash, first, value); count = c; &#125; return oldValue; &#125; finally &#123; unlock(); &#125; &#125; // 扩容时为了不影响正在进行的读线程，最好的方式是全部节点复制一次并重新添加 // 这里根据扩容时节点迁移的性质，最大可能的重用一部分节点，这个性质跟1.8的HashMap中的高低位是一个道理，必须要求hash值是final的 void rehash() &#123; HashEntry&lt;K,V&gt;[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity &gt;= MAXIMUM_CAPACITY) return; // 扩容前在一个hash桶中的节点，扩容后只会有两个去向，这里是用e.hash &amp; sizeMask； // 根据这个去向，找到最末尾的去向都一样的连续的一部分，这部分可以重用，不需要复制； // HashEntry的next是final的，resize/rehash时需要重新new，这里的特殊之处就是最大程度重用HashEntry链尾部的一部分，尽量减少重新new的次数； // 作者说从统计角度看，默认设置下有大约1/6的节点需要被重新复制一次，所以通常情况还是能节省不少时间的； HashEntry&lt;K,V&gt;[] newTable = HashEntry.newArray(oldCapacity&lt;&lt;1); threshold = (int)(newTable.length * loadFactor); // 跟1.6的HashMap有一样的小问题，可能会过早变为Integer.MAX_VALUE从而导致后续永远不能扩容 int sizeMask = newTable.length - 1; for (int i = 0; i &lt; oldCapacity ; i++) &#123; // We need to guarantee that any existing reads of old Map can proceed. So we cannot yet null out each bin. 为了保证其他线程能够继续执行读操作，不执行手动将原来table赋值为null，只是再最后修改一次table的引用 HashEntry&lt;K,V&gt; e = oldTable[i]; if (e != null) &#123; HashEntry&lt;K,V&gt; next = e.next; // int idx = e.hash &amp; sizeMask; // Single node on list if (next == null) newTable[idx] = e; else &#123; // Reuse trailing consecutive sequence at same slot HashEntry&lt;K,V&gt; lastRun = e; int lastIdx = idx; // 这个循环是寻找HashEntry链最大的可重用的尾部 // 看过1.8的HashMap就知道，如果hash值是final的，那么每次扩容，扩容前在一条链表上的节点，扩容后只会有两个去向 // 这里重用部分中，所有节点的去向相同，它们可以不用被复制 for (HashEntry&lt;K,V&gt; last = next; last != null; last = last.next) &#123; int k = last.hash &amp; sizeMask; if (k != lastIdx) &#123; lastIdx = k; lastRun = last; &#125; &#125; newTable[lastIdx] = lastRun; // 把重用部分整体放在扩容后的hash桶中 // 复制不能重用的部分，并把它们插入到rehash后的所在HashEntry链的头部 for (HashEntry&lt;K,V&gt; p = e; p != lastRun; p = p.next) &#123; int k = p.hash &amp; sizeMask; HashEntry&lt;K,V&gt; n = newTable[k]; newTable[k] = new HashEntry&lt;K,V&gt;(p.key, p.hash, n, p.value); &#125; // 这里也可以看出，重用部分rehash后相对顺序不变，并且还是在rehash后的链表的尾部 // 不能重用那些节点在rehash后，再一个个重头添加到链表的头部，如果还在一条链表上面，那么不能重用节点的相对顺序会颠倒 // 所以ConcurrentHashMap的迭代顺序会变化，本身它也不保证迭代顺序不变 &#125; &#125; &#125; table = newTable; &#125; // 因为1.6的HashEntry的next指针是final的，所以比普通的链表remove要复杂些，只有被删除节点的后面可以被重用，前面的都要再重新insert一次 V remove(Object key, int hash, Object value) &#123; lock(); try &#123; int c = count - 1; HashEntry&lt;K,V&gt;[] tab = table; int index = hash &amp; (tab.length - 1); HashEntry&lt;K,V&gt; first = tab[index]; HashEntry&lt;K,V&gt; e = first; //找到被删除的e while (e != null &amp;&amp; (e.hash != hash || !key.equals(e.key))) e = e.next; V oldValue = null; if (e != null) &#123; V v = e.value; if (value == null || value.equals(v)) &#123; oldValue = v; // All entries following removed node can stay in list, but all preceding ones need to be cloned.因为next指针是final的，所以删除不能用简单的链表删除，需要把前面的节点都重新复制再插入一次，后面的节点可以重用；删除后，后面的可以重用的那部分顺序不变且还是放在最后，前面的被复制的那部分顺序颠倒地放在前面 ++modCount; //被删除的e的next标记为newFirst，不断对e之前的元素复制+头插法方式完成新的链表 HashEntry&lt;K,V&gt; newFirst = e.next; for (HashEntry&lt;K,V&gt; p = first; p != e; p = p.next) newFirst = new HashEntry&lt;K,V&gt;(p.key, p.hash, newFirst, p.value); tab[index] = newFirst; count = c; // write-volatile &#125; &#125; return oldValue; &#125; finally &#123; unlock(); &#125; &#125; //清除所有的元素 void clear() &#123; if (count != 0) &#123; lock(); try &#123; HashEntry&lt;K,V&gt;[] tab = table; for (int i = 0; i &lt; tab.length ; i++) tab[i] = null; ++modCount; count = 0; // write-volatile &#125; finally &#123; unlock(); &#125; &#125; &#125; &#125; jdk1.6的扩容图。我们假设原先某个segment的table大小为2，扩容后的newtable大小为4，table[0]对应一个大小为7的HashEntry，经过e.hash &amp; sizeMask之后，10-14-18是落到newtable[3]，6也是落到newtable[3]，,2-8-4是落到newtable[0]。这个扩容的过程是这样的：首先rehash过程会找到可重用部分，即图上的10-14-18，通过newTable[lastIdx] = lastRun整体的将它们放到newtable[3]的位置去，各个元素的顺序相对原来并没有变化的；对于不可重用的部分，只有遍历和复制的方式，通过头插法，放到newtable中，这时我们发现6被头插到newtable[3]中，所以newtable[3]中的链表元素顺序为6-10-14-18，2-8-4被头插到newtable[0]中，顺序变为倒序了。这些操作完成之后才table = newTable，这是为了保证其他线程能够继续执行读操作，不执行手动将原来table赋值为null，只是再最后修改一次table的引用；对于原先的table，会被gc回收掉。 jdk1.6的删除图。假设我们从segment[0].table[2]中删除98的节点。 JDK1.7版本12345678910111213141516171819202122232425262728293031323334static final class HashEntry&lt;K,V&gt; &#123; final int hash; // hash是final的，1.7的HashMap中不是final的，用final对扩容比较友好 final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next; // jdk1.7中next指针不再是final的，改为volatile，使用 setNext 方法（内部用Unsafe的提供的方法）更新 HashEntry(int hash, K key, V value, HashEntry&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; // putOrderedObject，这个方法只有作用于volatile才有效，它能保证写操作的之间的顺序性，但是不保证能立马被其他线程读取到最新结果，是一种lazySet，效率比volatile高，但是只有volatile的“一半”的效果； // 普通的volatile保证写操作的结果能立马被其他线程看到，不论其他线程是读操作还写操作； // putOrderedObject能保证其他线程在写操作时一定能看到这个方法对变量的改变，但是其他线程只是进行读操作时，不一定能看到这个方法对变量的改变； final void setNext(HashEntry&lt;K,V&gt; n) &#123; UNSAFE.putOrderedObject(this, nextOffset, n); &#125; // 初始化执行Unsafe有关的操作 static final sun.misc.Unsafe UNSAFE; static final long nextOffset; static &#123; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class k = HashEntry.class; nextOffset = UNSAFE.objectFieldOffset (k.getDeclaredField("next")); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125; &#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123; private static final long serialVersionUID = 2249069246763182397L; static final int MAX_SCAN_RETRIES = Runtime.getRuntime().availableProcessors() &gt; 1 ? 64 : 1; // 尝试次数 transient volatile HashEntry&lt;K,V&gt;[] table; transient int count; transient int modCount; transient int threshold; final float loadFactor; Segment(float lf, int threshold, HashEntry&lt;K,V&gt;[] tab) &#123; this.loadFactor = lf; this.threshold = threshold; this.table = tab; &#125; // jdk1.6中的 newArray、setTable 这两个方法因为实现太简单了，直接不用了; // jdk1.6中的 get、containsKey、containsValue 这几个方法，因为都是读操作，实现基本类似，用 Unsafe 提供的一些底层操作代替了; // jdk1.6中的 getFirst 虽然实现也很简单，但还是用 Unsafe 提供的一些底层方法强化了这个操作; // 保证了对数组元素的volatile读取，1.6的只保证对整个数组的读取是volatile; // jdk1.6中的 readValueUnderLock 在1.7中彻底去掉了; final V put(K key, int hash, V value, boolean onlyIfAbsent) &#123; HashEntry&lt;K,V&gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value); // 看scanAndLockForPut方法的注释 V oldValue; try &#123; HashEntry&lt;K,V&gt;[] tab = table; int index = (tab.length - 1) &amp; hash; HashEntry&lt;K,V&gt; first = entryAt(tab, index); for (HashEntry&lt;K,V&gt; e = first;;) &#123; if (e != null) &#123; K k; if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) &#123; oldValue = e.value; if (!onlyIfAbsent) &#123; e.value = value; ++modCount; // 1.7的put相同的key（这时候相当于replace）时也会修改modCount了，1.6是不会的，能够更大地保证containValue这个方法的准确性 &#125; break; &#125; e = e.next; &#125; else &#123; if (node != null) node.setNext(first); // 尝试添加在链表头部 else node = new HashEntry&lt;K,V&gt;(hash, key, value, first); int c = count + 1; // 先加1 if (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY) // 超过最大容量的情况，在put这里一并处理了 rehash(node); else setEntryAt(tab, index, node); // 不扩容时，直接让新节点成为头节点 ++modCount; count = c; oldValue = null; break; &#125; &#125; &#125; finally &#123; unlock(); &#125; return oldValue; &#125; // 1.7的rehash方法带有参数了，这个参数node就是要新put进去的node，新的rehash方法带有部分put的功能 // 节点迁移的基本思路还是和1.6的一样 @SuppressWarnings("unchecked") private void rehash(HashEntry&lt;K,V&gt; node) &#123; HashEntry&lt;K,V&gt;[] oldTable = table; int oldCapacity = oldTable.length; int newCapacity = oldCapacity &lt;&lt; 1; threshold = (int)(newCapacity * loadFactor); HashEntry&lt;K,V&gt;[] newTable = (HashEntry&lt;K,V&gt;[]) new HashEntry[newCapacity]; int sizeMask = newCapacity - 1; for (int i = 0; i &lt; oldCapacity ; i++) &#123; HashEntry&lt;K,V&gt; e = oldTable[i]; if (e != null) &#123; HashEntry&lt;K,V&gt; next = e.next; int idx = e.hash &amp; sizeMask; if (next == null) // Single node on list 只有一个节点，简单处理 newTable[idx] = e; else &#123; // Reuse consecutive sequence at same slot 最大地重用链表尾部的一段连续的节点（这些节点扩容后在新数组中的同一个hash桶中），并标记位置 HashEntry&lt;K,V&gt; lastRun = e; int lastIdx = idx; for (HashEntry&lt;K,V&gt; last = next; last != null; last = last.next) &#123; int k = last.hash &amp; sizeMask; if (k != lastIdx) &#123; lastIdx = k; lastRun = last; &#125; &#125; newTable[lastIdx] = lastRun; // Clone remaining nodes 对标记之前的不能重用的节点进行复制，再重新添加到新数组对应的hash桶中去 for (HashEntry&lt;K,V&gt; p = e; p != lastRun; p = p.next) &#123; V v = p.value; int h = p.hash; int k = h &amp; sizeMask; HashEntry&lt;K,V&gt; n = newTable[k]; newTable[k] = new HashEntry&lt;K,V&gt;(h, p.key, v, n); &#125; &#125; &#125; &#125; int nodeIndex = node.hash &amp; sizeMask; // add the new node 部分的put功能，把新节点添加到链表的最前面 node.setNext(newTable[nodeIndex]); newTable[nodeIndex] = node; table = newTable; &#125; // 为put方法而编写的，在尝试获取锁的同时时进行一些准备工作的方法 // 获取不到锁时，会尝试一定次数的准备工作，这个准备工作指的是“遍历并预先创建要被添加的新节点，同时监测链表是否改变” // 这样有可能在获取到锁时新的要被put的节点已经创建了，可以在put时少做一些工作 // 准备工作中也会不断地尝试获取锁，超过最大准备工作尝试次数就直接阻塞等待地获取锁 private HashEntry&lt;K,V&gt; scanAndLockForPut(K key, int hash, V value) &#123; HashEntry&lt;K,V&gt; first = entryForHash(this, hash); HashEntry&lt;K,V&gt; e = first; HashEntry&lt;K,V&gt; node = null; int retries = -1; // negative while locating node while (!tryLock()) &#123; HashEntry&lt;K,V&gt; f; // to recheck first below if (retries &lt; 0) &#123; if (e == null) &#123; // 遍历完了，发现这条链表上没有“相等”的节点 if (node == null) // speculatively create node 预先创建要被添加的新节点 node = new HashEntry&lt;K,V&gt;(hash, key, value, null); retries = 0; // 遍历完都没碰见“相等”，不再遍历了，改为 尝试直接获取锁，没获取到锁时尝试监测链表是否改变 &#125; else if (key.equals(e.key)) // 碰见“相等”，不再遍历了，改为 尝试直接获取锁，没获取到锁时尝试监测链表是否改变 retries = 0; else // 遍历链表 e = e.next; &#125; else if (++retries &gt; MAX_SCAN_RETRIES) &#123; // 超过最大的准备工作尝试次数，放弃准备工作尝试，直接阻塞等待地获取锁 lock(); break; &#125; else if ((retries &amp; 1) == 0 &amp;&amp; (f = entryForHash(this, hash)) != first) &#123; // 间隔一次判断是否有新节点添加进去 e = first = f; // re-traverse if entry changed 如果链表改变，就重新遍历一次链表 retries = -1; // 重置次数 &#125; &#125; return node; &#125; // 基本同scanAndLockForPut，但是更简单些，只用遍历链表并监测改变，不用创建新节点 private void scanAndLock(Object key, int hash) &#123; HashEntry&lt;K,V&gt; first = entryForHash(this, hash); HashEntry&lt;K,V&gt; e = first; int retries = -1; while (!tryLock()) &#123; HashEntry&lt;K,V&gt; f; if (retries &lt; 0) &#123; if (e == null || key.equals(e.key)) retries = 0; else e = e.next; &#125; else if (++retries &gt; MAX_SCAN_RETRIES) &#123; lock(); break; &#125; else if ((retries &amp; 1) == 0 &amp;&amp; (f = entryForHash(this, hash)) != first) &#123; e = first = f; retries = -1; &#125; &#125; &#125; // 因为1.7的HashEntry.next是volatile的，可以修改，因此remove操作简单了很多，就是基本的链表删除操作。 final V remove(Object key, int hash, Object value) &#123; if (!tryLock()) scanAndLock(key, hash); V oldValue = null; try &#123; HashEntry&lt;K,V&gt;[] tab = table; int index = (tab.length - 1) &amp; hash; HashEntry&lt;K,V&gt; e = entryAt(tab, index); HashEntry&lt;K,V&gt; pred = null; while (e != null) &#123; K k; HashEntry&lt;K,V&gt; next = e.next; if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) &#123; V v = e.value; if (value == null || value == v || value.equals(v)) &#123; if (pred == null) setEntryAt(tab, index, next); // remove的是第一个节点 else pred.setNext(next); // 直接链表操作，前面说了1.7的HashEntry.next是volatile的，可以修改，不再跟1.6一样是final的！！！ ++modCount; --count; oldValue = v; &#125; break; &#125; pred = e; e = next; &#125; &#125; finally &#123; unlock(); &#125; return oldValue; &#125; // 1.7相对1.6的两点改动： // 1、多了个scanAndLock操作；2、会修改modCount final boolean replace(K key, int hash, V oldValue, V newValue) &#123; if (!tryLock()) scanAndLock(key, hash); boolean replaced = false; try &#123; HashEntry&lt;K,V&gt; e; for (e = entryForHash(this, hash); e != null; e = e.next) &#123; K k; if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) &#123; if (oldValue.equals(e.value)) &#123; e.value = newValue; ++modCount; // 1.7的replace方法也会修改modCount了，1.6是不会的，能够更大地保证containValue这个方法 replaced = true; &#125; break; &#125; &#125; &#125; finally &#123; unlock(); &#125; return replaced; &#125; // 基本同replace(K key, int hash, V oldValue, V newValue) final V replace(K key, int hash, V value) &#123; if (!tryLock()) scanAndLock(key, hash); V oldValue = null; try &#123; HashEntry&lt;K,V&gt; e; for (e = entryForHash(this, hash); e != null; e = e.next) &#123; K k; if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) &#123; oldValue = e.value; e.value = value; ++modCount; // 1.7的replace方法也会修改modCount了，1.6是不会的，能够更大地保证containValue这个方法 break; &#125; &#125; &#125; finally &#123; unlock(); &#125; return oldValue; &#125; // 基本没改变 final void clear() &#123; lock(); try &#123; HashEntry&lt;K,V&gt;[] tab = table; for (int i = 0; i &lt; tab.length ; i++) setEntryAt(tab, i, null); ++modCount; count = 0; &#125; finally &#123; unlock(); &#125; &#125; &#125; JDK1.8版本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663//类似于jdk1.8的HashMap.node和jdk1.7的Concurrent.HashEntrystatic class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next;//跟jdk1.7版本一样 Node(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.val = val; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return val; &#125; public final int hashCode() &#123; return key.hashCode() ^ val.hashCode(); &#125; public final String toString()&#123; return key + "=" + val; &#125; //不可以直接设置 public final V setValue(V value) &#123; throw new UnsupportedOperationException(); &#125; public final boolean equals(Object o) &#123; Object k, v, u; Map.Entry&lt;?,?&gt; e; return ((o instanceof Map.Entry) &amp;&amp; (k = (e = (Map.Entry&lt;?,?&gt;)o).getKey()) != null &amp;&amp; (v = e.getValue()) != null &amp;&amp; (k == key || k.equals(key)) &amp;&amp; (v == (u = val) || v.equals(u))); &#125; /** * Virtualized support for map.get(); overridden in subclasses. */ Node&lt;K,V&gt; find(int h, Object k) &#123; Node&lt;K,V&gt; e = this; if (k != null) &#123; do &#123; K ek; if (e.hash == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; &#125; while ((e = e.next) != null); &#125; return null; &#125; &#125;//在红黑树后才存在的节点，但是由TreeBin来代理执行static final class TreeNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next, TreeNode&lt;K,V&gt; parent) &#123; super(hash, key, val, next); this.parent = parent; &#125; Node&lt;K,V&gt; find(int h, Object k) &#123; return findTreeNode(h, k, null); &#125; /** * Returns the TreeNode (or null if not found) for the given key * starting at given root. */ final TreeNode&lt;K,V&gt; findTreeNode(int h, Object k, Class&lt;?&gt; kc) &#123; if (k != null) &#123; TreeNode&lt;K,V&gt; p = this; do &#123; int ph, dir; K pk; TreeNode&lt;K,V&gt; q; TreeNode&lt;K,V&gt; pl = p.left, pr = p.right; if ((ph = p.hash) &gt; h) p = pl; else if (ph &lt; h) p = pr; else if ((pk = p.key) == k || (pk != null &amp;&amp; k.equals(pk))) return p; else if (pl == null) p = pr; else if (pr == null) p = pl; else if ((kc != null || (kc = comparableClassFor(k)) != null) &amp;&amp; (dir = compareComparables(kc, k, pk)) != 0) p = (dir &lt; 0) ? pl : pr; else if ((q = pr.findTreeNode(h, k, kc)) != null) return q; else p = pl; &#125; while (p != null); &#125; return null; &#125; &#125;//代理操作TreeNode的特殊节点，持有存储实际数据的红黑树的根节点static final class TreeBin&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; root;//红黑树结构的根节点 volatile TreeNode&lt;K,V&gt; first; volatile Thread waiter; //最近一个设置为WAITER标识位的线程 volatile int lockState; // values for lockState//红黑树的读锁状态和写锁状态表现为：当有线程持有红黑树的读锁时，写线程可能会阻塞，由于查找很快，//写线程阻塞时间也短; 当有线程持有红黑树的写锁时，读线程不会以红黑树的方式读取，而是简单的链表读取，//这样读写可以并发进行,参考Node&lt;K,V&gt; find(int h, Object k)方法； //001 static final int WRITER = 1; // set while holding write lock //010 static final int WAITER = 2; // set when waiting for write lock //100 static final int READER = 4; // increment value for setting read lock //判断大小 static int tieBreakOrder(Object a, Object b) &#123; int d; if (a == null || b == null || (d = a.getClass().getName(). compareTo(b.getClass().getName())) == 0) d = (System.identityHashCode(a) &lt;= System.identityHashCode(b) ? -1 : 1); return d; &#125; //构造函数，构造b为头结点的红黑树 TreeBin(TreeNode&lt;K,V&gt; b) &#123; super(TREEBIN, null, null, null); this.first = b; TreeNode&lt;K,V&gt; r = null; for (TreeNode&lt;K,V&gt; x = b, next; x != null; x = next) &#123; next = (TreeNode&lt;K,V&gt;)x.next; x.left = x.right = null; if (r == null) &#123; x.parent = null; x.red = false; r = x; &#125; else &#123; K k = x.key; int h = x.hash; Class&lt;?&gt; kc = null; for (TreeNode&lt;K,V&gt; p = r;;) &#123; int dir, ph; K pk = p.key; if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); TreeNode&lt;K,V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; x.parent = xp; if (dir &lt;= 0) xp.left = x; else xp.right = x; r = balanceInsertion(r, x); break; &#125; &#125; &#125; &#125; this.root = r; assert checkInvariants(root); &#125; //对根节点加写锁 private final void lockRoot() &#123; //WRITER = 1，当没有获取写锁时候，调用contendedLock if (!U.compareAndSwapInt(this, LOCKSTATE, 0, WRITER)) contendedLock(); // offload to separate method &#125; //释放写锁 private final void unlockRoot() &#123; lockState = 0; &#125; //在对根节点加写锁时候，调用的方式，这个方法的目的是直到取得写锁才会返回。 private final void contendedLock() &#123; boolean waiting = false; for (int s;;) &#123; //~WAITER是对WAITER进行二进制取反，当此时没有线程持有 读锁（不会有线程持有 写锁）时，这个if为真 if (((s = lockState) &amp; ~WAITER) == 0) &#123; if (U.compareAndSwapInt(this, LOCKSTATE, s, WRITER)) &#123; if (waiting) waiter = null; return; &#125; &#125; // 有线程持有 读锁（不会有线程持有 写锁），并且当前线程不是 WAITER 状态时，这个else if为真 else if ((s &amp; WAITER) == 0) &#123; //尝试占据 WAITER 状态标识位 if (U.compareAndSwapInt(this, LOCKSTATE, s, s | WAITER)) &#123; waiting = true; waiter = Thread.currentThread(); &#125; &#125; else if (waiting) //unsafe类阻塞自己 LockSupport.park(this); &#125; &#125; //查找 final Node&lt;K,V&gt; find(int h, Object k) &#123; if (k != null) &#123; for (Node&lt;K,V&gt; e = first; e != null; ) &#123; int s; K ek; //这里当lockState = WRITER = 1（有线程正持有写锁）或者 lockState = WAITER = 2 （有线程等待获取写锁）会以链表方式查找，如果只有持有读锁线程时候，会通过CAS竞争去读，同时lockState +=READER； if (((s = lockState) &amp; (WAITER|WRITER)) != 0) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; e = e.next; &#125; else if (U.compareAndSwapInt(this, LOCKSTATE, s, s + READER)) &#123; TreeNode&lt;K,V&gt; r, p; try &#123; p = ((r = root) == null ? null : r.findTreeNode(h, k, null)); &#125; finally &#123; Thread w;// U.getAndAddInt(this, LOCKSTATE, -READER)这个操作是在更新之后返回lockstate的旧值， //不是返回新值，相当于先判断==，再执行减法//前面的判断是LOCKSTATE是否正好包含一个读线程和一个写线程，然后减法后只剩下写线程了，也就是说当时最后一个读线程，同时有写线程因为 读锁 而阻塞，那么要通知它，告诉它可以尝试获取写锁了。 if (U.getAndAddInt(this, LOCKSTATE, -READER) == (READER|WAITER) &amp;&amp; (w = waiter) != null) LockSupport.unpark(w); &#125; return p; &#125; &#125; &#125; return null; &#125; //存在红黑树时候，添加元素 final TreeNode&lt;K,V&gt; putTreeVal(int h, K k, V v) &#123; Class&lt;?&gt; kc = null; boolean searched = false; for (TreeNode&lt;K,V&gt; p = root;;) &#123; int dir, ph; K pk; if (p == null) &#123; first = root = new TreeNode&lt;K,V&gt;(h, k, v, null, null); break; &#125; else if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; else if ((pk = p.key) == k || (pk != null &amp;&amp; k.equals(pk))) return p; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) &#123; if (!searched) &#123; TreeNode&lt;K,V&gt; q, ch; searched = true; if (((ch = p.left) != null &amp;&amp; (q = ch.findTreeNode(h, k, kc)) != null) || ((ch = p.right) != null &amp;&amp; (q = ch.findTreeNode(h, k, kc)) != null)) return q; &#125; dir = tieBreakOrder(k, pk); &#125; TreeNode&lt;K,V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; TreeNode&lt;K,V&gt; x, f = first; first = x = new TreeNode&lt;K,V&gt;(h, k, v, f, xp); if (f != null) f.prev = x; if (dir &lt;= 0) xp.left = x; else xp.right = x; if (!xp.red) x.red = true; else &#123; lockRoot(); try &#123; root = balanceInsertion(root, x); &#125; finally &#123; unlockRoot(); &#125; &#125; break; &#125; &#125; assert checkInvariants(root); return null; &#125; //移除元素 final boolean removeTreeNode(TreeNode&lt;K,V&gt; p) &#123; TreeNode&lt;K,V&gt; next = (TreeNode&lt;K,V&gt;)p.next; TreeNode&lt;K,V&gt; pred = p.prev; // unlink traversal pointers TreeNode&lt;K,V&gt; r, rl; if (pred == null) first = next; else pred.next = next; if (next != null) next.prev = pred; if (first == null) &#123; root = null; return true; &#125; if ((r = root) == null || r.right == null || // too small (rl = r.left) == null || rl.left == null) return true; //红黑树规模够，就从红黑树再删除，需要写锁 lockRoot(); try &#123; TreeNode&lt;K,V&gt; replacement; TreeNode&lt;K,V&gt; pl = p.left; TreeNode&lt;K,V&gt; pr = p.right; if (pl != null &amp;&amp; pr != null) &#123; TreeNode&lt;K,V&gt; s = pr, sl; while ((sl = s.left) != null) // find successor s = sl; boolean c = s.red; s.red = p.red; p.red = c; // swap colors TreeNode&lt;K,V&gt; sr = s.right; TreeNode&lt;K,V&gt; pp = p.parent; if (s == pr) &#123; // p was s's direct parent p.parent = s; s.right = p; &#125; else &#123; TreeNode&lt;K,V&gt; sp = s.parent; if ((p.parent = sp) != null) &#123; if (s == sp.left) sp.left = p; else sp.right = p; &#125; if ((s.right = pr) != null) pr.parent = s; &#125; p.left = null; if ((p.right = sr) != null) sr.parent = p; if ((s.left = pl) != null) pl.parent = s; if ((s.parent = pp) == null) r = s; else if (p == pp.left) pp.left = s; else pp.right = s; if (sr != null) replacement = sr; else replacement = p; &#125; else if (pl != null) replacement = pl; else if (pr != null) replacement = pr; else replacement = p; if (replacement != p) &#123; TreeNode&lt;K,V&gt; pp = replacement.parent = p.parent; if (pp == null) r = replacement; else if (p == pp.left) pp.left = replacement; else pp.right = replacement; p.left = p.right = p.parent = null; &#125; root = (p.red) ? r : balanceDeletion(r, replacement); if (p == replacement) &#123; // detach pointers TreeNode&lt;K,V&gt; pp; if ((pp = p.parent) != null) &#123; if (p == pp.left) pp.left = null; else if (p == pp.right) pp.right = null; p.parent = null; &#125; &#125; &#125; finally &#123; unlockRoot(); &#125; assert checkInvariants(root); return false; &#125; /* ------------------------------------------------------------ */ // Red-black tree methods, all adapted from CLR static &lt;K,V&gt; TreeNode&lt;K,V&gt; rotateLeft(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; p) &#123; TreeNode&lt;K,V&gt; r, pp, rl; if (p != null &amp;&amp; (r = p.right) != null) &#123; if ((rl = p.right = r.left) != null) rl.parent = p; if ((pp = r.parent = p.parent) == null) (root = r).red = false; else if (pp.left == p) pp.left = r; else pp.right = r; r.left = p; p.parent = r; &#125; return root; &#125; static &lt;K,V&gt; TreeNode&lt;K,V&gt; rotateRight(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; p) &#123; TreeNode&lt;K,V&gt; l, pp, lr; if (p != null &amp;&amp; (l = p.left) != null) &#123; if ((lr = p.left = l.right) != null) lr.parent = p; if ((pp = l.parent = p.parent) == null) (root = l).red = false; else if (pp.right == p) pp.right = l; else pp.left = l; l.right = p; p.parent = l; &#125; return root; &#125; static &lt;K,V&gt; TreeNode&lt;K,V&gt; balanceInsertion(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; x) &#123; x.red = true; for (TreeNode&lt;K,V&gt; xp, xpp, xppl, xppr;;) &#123; if ((xp = x.parent) == null) &#123; x.red = false; return x; &#125; else if (!xp.red || (xpp = xp.parent) == null) return root; if (xp == (xppl = xpp.left)) &#123; if ((xppr = xpp.right) != null &amp;&amp; xppr.red) &#123; xppr.red = false; xp.red = false; xpp.red = true; x = xpp; &#125; else &#123; if (x == xp.right) &#123; root = rotateLeft(root, x = xp); xpp = (xp = x.parent) == null ? null : xp.parent; &#125; if (xp != null) &#123; xp.red = false; if (xpp != null) &#123; xpp.red = true; root = rotateRight(root, xpp); &#125; &#125; &#125; &#125; else &#123; if (xppl != null &amp;&amp; xppl.red) &#123; xppl.red = false; xp.red = false; xpp.red = true; x = xpp; &#125; else &#123; if (x == xp.left) &#123; root = rotateRight(root, x = xp); xpp = (xp = x.parent) == null ? null : xp.parent; &#125; if (xp != null) &#123; xp.red = false; if (xpp != null) &#123; xpp.red = true; root = rotateLeft(root, xpp); &#125; &#125; &#125; &#125; &#125; &#125; static &lt;K,V&gt; TreeNode&lt;K,V&gt; balanceDeletion(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; x) &#123; for (TreeNode&lt;K,V&gt; xp, xpl, xpr;;) &#123; if (x == null || x == root) return root; else if ((xp = x.parent) == null) &#123; x.red = false; return x; &#125; else if (x.red) &#123; x.red = false; return root; &#125; else if ((xpl = xp.left) == x) &#123; if ((xpr = xp.right) != null &amp;&amp; xpr.red) &#123; xpr.red = false; xp.red = true; root = rotateLeft(root, xp); xpr = (xp = x.parent) == null ? null : xp.right; &#125; if (xpr == null) x = xp; else &#123; TreeNode&lt;K,V&gt; sl = xpr.left, sr = xpr.right; if ((sr == null || !sr.red) &amp;&amp; (sl == null || !sl.red)) &#123; xpr.red = true; x = xp; &#125; else &#123; if (sr == null || !sr.red) &#123; if (sl != null) sl.red = false; xpr.red = true; root = rotateRight(root, xpr); xpr = (xp = x.parent) == null ? null : xp.right; &#125; if (xpr != null) &#123; xpr.red = (xp == null) ? false : xp.red; if ((sr = xpr.right) != null) sr.red = false; &#125; if (xp != null) &#123; xp.red = false; root = rotateLeft(root, xp); &#125; x = root; &#125; &#125; &#125; else &#123; // symmetric if (xpl != null &amp;&amp; xpl.red) &#123; xpl.red = false; xp.red = true; root = rotateRight(root, xp); xpl = (xp = x.parent) == null ? null : xp.left; &#125; if (xpl == null) x = xp; else &#123; TreeNode&lt;K,V&gt; sl = xpl.left, sr = xpl.right; if ((sl == null || !sl.red) &amp;&amp; (sr == null || !sr.red)) &#123; xpl.red = true; x = xp; &#125; else &#123; if (sl == null || !sl.red) &#123; if (sr != null) sr.red = false; xpl.red = true; root = rotateLeft(root, xpl); xpl = (xp = x.parent) == null ? null : xp.left; &#125; if (xpl != null) &#123; xpl.red = (xp == null) ? false : xp.red; if ((sl = xpl.left) != null) sl.red = false; &#125; if (xp != null) &#123; xp.red = false; root = rotateRight(root, xp); &#125; x = root; &#125; &#125; &#125; &#125; &#125; /** * Recursive invariant check */ static &lt;K,V&gt; boolean checkInvariants(TreeNode&lt;K,V&gt; t) &#123; TreeNode&lt;K,V&gt; tp = t.parent, tl = t.left, tr = t.right, tb = t.prev, tn = (TreeNode&lt;K,V&gt;)t.next; if (tb != null &amp;&amp; tb.next != t) return false; if (tn != null &amp;&amp; tn.prev != t) return false; if (tp != null &amp;&amp; t != tp.left &amp;&amp; t != tp.right) return false; if (tl != null &amp;&amp; (tl.parent != t || tl.hash &gt; t.hash)) return false; if (tr != null &amp;&amp; (tr.parent != t || tr.hash &lt; t.hash)) return false; if (t.red &amp;&amp; tl != null &amp;&amp; tl.red &amp;&amp; tr != null &amp;&amp; tr.red) return false; if (tl != null &amp;&amp; !checkInvariants(tl)) return false; if (tr != null &amp;&amp; !checkInvariants(tr)) return false; return true; &#125; private static final sun.misc.Unsafe U; private static final long LOCKSTATE; static &#123; try &#123; U = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = TreeBin.class; LOCKSTATE = U.objectFieldOffset (k.getDeclaredField("lockState")); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125; &#125;//转发节点，是一种临时节点，如果旧数组的一个hash桶中全部的节点都迁移到新数组中，旧数组就在这个hash桶中放置一个ForwardingNode。读操作或者迭代读时碰到ForwardingNode时，将操作转发到扩容后的新的table数组上去执行，写操作碰见它时，则尝试帮助扩容。 static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; super(MOVED, null, null, null); this.nextTable = tab; &#125; Node&lt;K,V&gt; find(int h, Object k) &#123; // loop to avoid arbitrarily deep recursion on forwarding nodes outer: for (Node&lt;K,V&gt;[] tab = nextTable;;) &#123; Node&lt;K,V&gt; e; int n; if (k == null || tab == null || (n = tab.length) == 0 || (e = tabAt(tab, (n - 1) &amp; h)) == null) return null; for (;;) &#123; int eh; K ek; if ((eh = e.hash) == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; if (eh &lt; 0) &#123; if (e instanceof ForwardingNode) &#123; tab = ((ForwardingNode&lt;K,V&gt;)e).nextTable; continue outer; &#125; else return e.find(h, k); &#125; if ((e = e.next) == null) return null; &#125; &#125; &#125; &#125;//空节点，computeIfAbsent和compute这两个函数式api中才会使用 static final class ReservationNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; ReservationNode() &#123; super(RESERVED, null, null, null); &#125; Node&lt;K,V&gt; find(int h, Object k) &#123; return null; &#125; &#125; 常用方法外面封装一层其实还是操作segment。 JDK1.6版本读方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138public V get(Object key) &#123; int hash = hash(key.hashCode()); return segmentFor(hash).get(key, hash); &#125;public boolean containsKey(Object key) &#123; int hash = hash(key.hashCode()); return segmentFor(hash).containsKey(key, hash); &#125; private static int hash(int h) &#123; // Spread bits to regularize both segment and index locations, // using variant of single-word Wang/Jenkins hash. h += (h &lt;&lt; 15) ^ 0xffffcd7d; h ^= (h &gt;&gt;&gt; 10); h += (h &lt;&lt; 3); h ^= (h &gt;&gt;&gt; 6); h += (h &lt;&lt; 2) + (h &lt;&lt; 14); return h ^ (h &gt;&gt;&gt; 16); &#125; //定位Segment的index时hash值的移位,默认时候sshift=4,segmentShift=32-4=28；//segmentShift = 32 - sshift; //segmentMask = ssize - 1 = 15final Segment&lt;K,V&gt; segmentFor(int hash) &#123; return segments[(hash &gt;&gt;&gt; segmentShift) &amp; segmentMask]; &#125; // 不是Map接口的方法，为了兼容Hashtable，等价于containsValue public boolean contains(Object value) &#123; return containsValue( value); &#125; public boolean containsValue(Object value) &#123; if (value == null) throw new NullPointerException(); final Segment&lt;K,V&gt;[] segments = this.segments; int[] mc = new int[segments.length]; // 先不加锁执行RETRIES_BEFORE_LOCK = 2次 for (int k = 0; k &lt; RETRIES_BEFORE_LOCK; ++k) &#123; int sum = 0; int mcsum = 0; for (int i = 0; i &lt; segments.length; ++i) &#123; int c = segments[i].count; // 这个c没哪里用，意义不明 mcsum += mc[i] = segments[i].modCount; // 就是 mc[i] = segments[i].count; mssum += mc[i]，临时保存一份modCount if (segments[i].containsValue(value)) // 碰见contains直接return return true; &#125; boolean cleanSweep = true; // mcsum是modCount的和，为0可以认为遍历开始时没有任何put完成过任何HashEntry，即方法开始执行时不包含任何HashEntry，可以认为（近似认为，几率比较大）此时也不包含 if (mcsum != 0) &#123; for (int i = 0; i &lt; segments.length; ++i) &#123; int c = segments[i].count; if (mc[i] != segments[i].modCount) &#123; // modCount改变，说明有其他线程修改了Segment的结构，退出循环。会有replace的问题，前面说了 cleanSweep = false; break; &#125; &#125; &#125; if (cleanSweep) return false; &#125; // 如果连续两次都碰见modCount改变的情况，这时候一次性对全部Segment加锁，最大程度保证遍历时的一致性 // 因为是全部加锁后再遍历，遍历开始后没有线程可以修改任何Segment的结构，可以保证当前线程得到的是准确值 for (int i = 0; i &lt; segments.length; ++i) segments[i].lock(); boolean found = false; try &#123; for (int i = 0; i &lt; segments.length; ++i) &#123; if (segments[i].containsValue(value)) &#123; found = true; break; &#125; &#125; &#125; finally &#123; for (int i = 0; i &lt; segments.length; ++i) segments[i].unlock(); &#125; return found; &#125; //不加锁执行一次public boolean isEmpty() &#123; final Segment&lt;K,V&gt;[] segments = this.segments; int[] mc = new int[segments.length]; int mcsum = 0; for (int i = 0; i &lt; segments.length; ++i) &#123; if (segments[i].count != 0) return false; else mcsum += mc[i] = segments[i].modCount; &#125; if (mcsum != 0) &#123; for (int i = 0; i &lt; segments.length; ++i) &#123; if (segments[i].count != 0 || mc[i] != segments[i].modCount) return false; &#125; &#125; return true; &#125; public int size() &#123; final Segment&lt;K,V&gt;[] segments = this.segments; long sum = 0; long check = 0; int[] mc = new int[segments.length]; // 不加锁执行两次，如果两次数据不一样，或者碰到modCount++被修改了，就全部加锁在执行一次 for (int k = 0; k &lt; RETRIES_BEFORE_LOCK; ++k) &#123; check = 0; sum = 0; int mcsum = 0; for (int i = 0; i &lt; segments.length; ++i) &#123; sum += segments[i].count; mcsum += mc[i] = segments[i].modCount; &#125; if (mcsum != 0) &#123; for (int i = 0; i &lt; segments.length; ++i) &#123; check += segments[i].count; if (mc[i] != segments[i].modCount) &#123; check = -1; // force retry break; &#125; &#125; &#125; if (check == sum) break; &#125; if (check != sum) &#123; sum = 0; for (int i = 0; i &lt; segments.length; ++i) segments[i].lock(); for (int i = 0; i &lt; segments.length; ++i) sum += segments[i].count; for (int i = 0; i &lt; segments.length; ++i) segments[i].unlock(); &#125; if (sum &gt; Integer.MAX_VALUE) // int溢出处理，因此返回值可能会是错误的。 // 并且因为兼容性的原因，这个还无法解决，只能新增一个方法，1.8的ConcurrentHashMap就是新增了一个返回long型的方法。 return Integer.MAX_VALUE; else return (int)sum; &#125; 写方法 1234567891011121314151617181920212223public V put(K key, V value) &#123; if (value == null) throw new NullPointerException(); int hash = hash(key.hashCode()); // 这里 null key 会抛出NPE return segmentFor(hash).put(key, hash, value, false); &#125; // 下面几个基本思路同put，都是代理给相应的Segment的对应方法进行操作 public V putIfAbsent(K key, V value); public V remove(Object key); public boolean remove(Object key, Object value); public boolean replace(K key, V oldValue, V newValue); public V replace(K key, V value); // putAll和clear都是循环操作，没有全局加锁，在执行期间还是可以执行完成其他的写操作的，事务性比较差的方法，设计成不用全局锁是为了提高效率 public void putAll(Map&lt;? extends K, ? extends V&gt; m) &#123; for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) put(e.getKey(), e.getValue()); &#125; public void clear() &#123; for (int i = 0; i &lt; segments.length; ++i) segments[i].clear(); &#125; JDK1.7版本读方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163// get方法整体实现思路跟1.6基本一样 // 1.7的使用了Unsafe.getObjectVolatile，它能为普通对象提供volatile读取功能，能够强化这里的get方法 // get方法的操作都比较简单，就都把操作集中在这里，省略了Segment.get，减少方法调用带来的开销，抽象性层次性也没有变差 public V get(Object key) &#123; Segment&lt;K,V&gt; s; // manually integrate access methods to reduce overhead 集中在这里手动访问减少方法调用开销 HashEntry&lt;K,V&gt;[] tab; int h = hash(key); long u = (((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE; if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(segments, u)) != null &amp;&amp;(tab = s.table) != null) &#123; for (HashEntry&lt;K,V&gt; e = (HashEntry&lt;K,V&gt;) UNSAFE.getObjectVolatile(tab, ((long)(((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE); e != null; e = e.next) &#123; K k; if ((k = e.key) == key || (e.hash == h &amp;&amp; key.equals(k))) return e.value; &#125; &#125; return null; &#125; private int hash(Object k) &#123; int h = hashSeed; if ((0 != h) &amp;&amp; (k instanceof String)) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h ^= k.hashCode(); // Spread bits to regularize both segment and index locations, // using variant of single-word Wang/Jenkins hash. h += (h &lt;&lt; 15) ^ 0xffffcd7d; h ^= (h &gt;&gt;&gt; 10); h += (h &lt;&lt; 3); h ^= (h &gt;&gt;&gt; 6); h += (h &lt;&lt; 2) + (h &lt;&lt; 14); return h ^ (h &gt;&gt;&gt; 16); &#125; public boolean contains(Object value) &#123; return containsValue( value); &#125; public boolean containsValue(Object value) &#123; // Same idea as size() if (value == null) throw new NullPointerException(); final Segment&lt;K,V&gt;[] segments = this.segments; boolean found = false; long last = 0; int retries = -1; try &#123; outer: for (;;) &#123; if (retries++ == RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) ensureSegment(j).lock(); // force creation &#125; long hashSum = 0L; int sum = 0; for (int j = 0; j &lt; segments.length; ++j) &#123; HashEntry&lt;K,V&gt;[] tab; //segmentAt如下解释 Segment&lt;K,V&gt; seg = segmentAt(segments, j); if (seg != null &amp;&amp; (tab = seg.table) != null) &#123; for (int i = 0 ; i &lt; tab.length; i++) &#123; HashEntry&lt;K,V&gt; e; for (e = entryAt(tab, i); e != null; e = e.next) &#123; V v = e.value; if (v != null &amp;&amp; value.equals(v)) &#123; found = true; break outer; // 这里就算是contains也会再执行一次，1.6如果第一次contains就直接return，不会执行第二次 &#125; &#125; &#125; sum += seg.modCount; &#125; &#125; if (retries &gt; 0 &amp;&amp; sum == last) break; last = sum; &#125; &#125; finally &#123; if (retries &gt; RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) segmentAt(segments, j).unlock(); &#125; &#125; return found; &#125; // 使用Unsafe提供的volatile读取功能，通过下标求segments[j] // segments是用final修饰的，构造方法保证它会在ConcurrentHashMap的实例被引用前初始化成正确的值，null的情况只在反序列化时才会出现 static final &lt;K,V&gt; Segment&lt;K,V&gt; segmentAt(Segment&lt;K,V&gt;[] ss, int j) &#123; long u = (j &lt;&lt; SSHIFT) + SBASE; // 计算实际的字节偏移量 return ss == null ? null : (Segment&lt;K,V&gt;) UNSAFE.getObjectVolatile(ss, u); &#125; // 通过下标定位到Segment中下标为 i 的hash桶的第一个节点，也就是链表的头结点，用 Unsafe 提供对数组元素的 // volatile 读取，还要处理下Segment为null的情况static final &lt;K,V&gt; HashEntry&lt;K,V&gt; entryAt(HashEntry&lt;K,V&gt;[] tab, int i) &#123; return (tab == null) ? null : (HashEntry&lt;K,V&gt;) UNSAFE.getObjectVolatile(tab, ((long)i &lt;&lt; TSHIFT) + TBASE); &#125;// isEmpty方法，实现思路跟1.6的基本一样，利用modCount单调递增的性质偷了个懒，只进行sum(modCount)的前后比较，不用一个个单独地前后比较 public boolean isEmpty() &#123; long sum = 0L; final Segment&lt;K,V&gt;[] segments = this.segments; for (int j = 0; j &lt; segments.length; ++j) &#123; Segment&lt;K,V&gt; seg = segmentAt(segments, j); if (seg != null) &#123; if (seg.count != 0) return false; sum += seg.modCount; &#125; &#125; if (sum != 0L) &#123; // recheck unless no modifications for (int j = 0; j &lt; segments.length; ++j) &#123; Segment&lt;K,V&gt; seg = segmentAt(segments, j); if (seg != null) &#123; if (seg.count != 0) return false; sum -= seg.modCount; // 1.6这里的一个个modCount对比，1.7改为总体对比一次，因为modCount的单调递增的，不会有count可能出现的 ABA 问题 &#125; &#125; if (sum != 0L) return false; &#125; return true; &#125; // size方法，实现思路跟1.6的基本一样，也利用了modCount单调递增的性质偷了个懒 public int size() &#123; final Segment&lt;K,V&gt;[] segments = this.segments; int size; boolean overflow; // true if size overflows 32 bits long sum; // sum of modCounts long last = 0L; // previous sum int retries = -1; // first iteration isn't retry try &#123; for (;;) &#123; if (retries++ == RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) ensureSegment(j).lock(); // force creation &#125; sum = 0L; size = 0; overflow = false; for (int j = 0; j &lt; segments.length; ++j) &#123; Segment&lt;K,V&gt; seg = segmentAt(segments, j); if (seg != null) &#123; sum += seg.modCount; int c = seg.count; if (c &lt; 0 || (size += c) &lt; 0) overflow = true; &#125; &#125; if (sum == last) break; last = sum; &#125; &#125; finally &#123; if (retries &gt; RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) segmentAt(segments, j).unlock(); &#125; &#125; return overflow ? Integer.MAX_VALUE : size; &#125; 写方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// 1.7开始大部分集合类都是懒初始化，put这里处理下懒初始化，其余基本思路跟1.6的差不多，都是代理给相应的Segment的同名方法 public V put(K key, V value) &#123; Segment&lt;K,V&gt; s; if (value == null) throw new NullPointerException(); int hash = hash(key); int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObject(segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) // nonvolatile; recheck in ensureSegment 非volatile方式读取，在ensureSegment中再检查一次 s = ensureSegment(j); // 处理Segment的初始化 return s.put(key, hash, value, false); &#125; // 确保Segment被初始化 // 因为懒初始化的原因，只有segments[0]在构造方法中被初始化，其余的都是后面按需初始化，此方法就是做这个初始化的 // 使用 CAS 不加锁，同时也能保证每个Segment只被初始化一次 private Segment&lt;K,V&gt; ensureSegment(int k) &#123; final Segment&lt;K,V&gt;[] ss = this.segments; long u = (k &lt;&lt; SSHIFT) + SBASE; // raw offset 实际的字节偏移量 Segment&lt;K,V&gt; seg; if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; Segment&lt;K,V&gt; proto = ss[0]; // use segment 0 as prototype int cap = proto.table.length; float lf = proto.loadFactor; int threshold = (int)(cap * lf); HashEntry&lt;K,V&gt;[] tab = (HashEntry&lt;K,V&gt;[])new HashEntry[cap]; if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; // recheck 再检查一次是否已经被初始化 Segment&lt;K,V&gt; s = new Segment&lt;K,V&gt;(lf, threshold, tab); while ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; if (UNSAFE.compareAndSwapObject(ss, u, null, seg = s)) // 使用 CAS 确保只被初始化一次 break; &#125; &#125; &#125; return seg; &#125; // 同put public V putIfAbsent(K key, V value); // 跟1.6一样，都是循环put，不用全局锁，其他线程还是可以在这个方法执行期间成功进行写操作 public void putAll(Map&lt;? extends K, ? extends V&gt; m); public void clear(); // 额外处理下Segment为null的情况，其余基本同1.6，代理给相应的Segement的同名方法 public V remove(Object key) &#123; int hash = hash(key); Segment&lt;K,V&gt; s = segmentForHash(hash); return s == null ? null : s.remove(key, hash, null); // 1.7使用懒初始化，会出现Segment为null的情况 &#125; // 同remove public boolean remove(Object key, Object value); // 学remove一样额外处理下Segment为null的情况，其余思路跟1.6差不多，都是代理给相应的Segement的同名方法 public boolean replace(K key, V oldValue, V newValue); public V replace(K key, V value); JDK1.8版本读方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public int size() &#123; long n = sumCount(); return ((n &lt; 0L) ? 0 : (n &gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); &#125; final long sumCount() &#123; CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value; &#125; &#125; return sum; &#125;//public boolean isEmpty() &#123; return sumCount() &lt;= 0L; &#125; public boolean containsKey(Object key) &#123; return get(key) != null; &#125;//获取元素public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; if ((eh = e.hash) == h) &#123; // 特殊判断第一个节点 if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; else if (eh &lt; 0) //约定hash值小于0为特殊节点 // ForwardingNode会把find转发到nextTable上再去执行一次； // TreeBin则根据自身读写锁情况，判断是用红黑树方式查找，还是用链表方式查找； // ReservationNode本身只是为了synchronized有加锁对象而创建的空的占位节点，因此本身hash桶是没节点的，一定找不到，直接返回null） return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) &#123; // 是普通节点，使用链表方式查找 if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null; &#125; // hash扰动函数，跟1.8的HashMap的基本一样，&amp; HASH_BITS用于把hash值转化为正数，负数hash是有特别的作用的 static final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS; &#125;// public boolean containsValue(Object value) &#123; if (value == null) throw new NullPointerException(); Node&lt;K,V&gt;[] t; if ((t = table) != null) &#123; Traverser&lt;K,V&gt; it = new Traverser&lt;K,V&gt;(t, t.length, 0, t.length); for (Node&lt;K,V&gt; p; (p = it.advance()) != null; ) &#123; V v; if ((v = p.val) == value || (v != null &amp;&amp; value.equals(v))) return true; &#125; &#125; return false; &#125; 写方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202public V put(K key, V value) &#123; return putVal(key, value, false); &#125;final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) //处理初始化 tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; else if ((fh = f.hash) == MOVED) //发现转发节点，表明此时正在进行扩容，去帮助扩容 tab = helpTransfer(tab, f); else &#123; V oldVal = null; synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; addCount(1L, binCount); return null; &#125; private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; if ((sc = sizeCtl) &lt; 0) //当在初始化时候，其他线程来了之后需要让出cpu Thread.yield(); // lost initialization race; just spin else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if ((tab = table) == null || tab.length == 0) &#123; int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings("unchecked") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; sizeCtl = sc; &#125; break; &#125; &#125; return tab; &#125; public void putAll(Map&lt;? extends K, ? extends V&gt; m) &#123; tryPresize(m.size()); for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) putVal(e.getKey(), e.getValue(), false); &#125; public V remove(Object key) &#123; return replaceNode(key, null, null); &#125; final V replaceNode(Object key, V value, Object cv) &#123; int hash = spread(key.hashCode()); for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0 || (f = tabAt(tab, i = (n - 1) &amp; hash)) == null) break; else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else &#123; V oldVal = null; boolean validated = false; synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; validated = true; for (Node&lt;K,V&gt; e = f, pred = null;;) &#123; K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; V ev = e.val; if (cv == null || cv == ev || (ev != null &amp;&amp; cv.equals(ev))) &#123; oldVal = ev; if (value != null) e.val = value; else if (pred != null) pred.next = e.next; else setTabAt(tab, i, e.next); &#125; break; &#125; pred = e; if ((e = e.next) == null) break; &#125; &#125; else if (f instanceof TreeBin) &#123; validated = true; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; r, p; if ((r = t.root) != null &amp;&amp; (p = r.findTreeNode(hash, key, null)) != null) &#123; V pv = p.val; if (cv == null || cv == pv || (pv != null &amp;&amp; cv.equals(pv))) &#123; oldVal = pv; if (value != null) p.val = value; else if (t.removeTreeNode(p)) setTabAt(tab, i, untreeify(t.first)); &#125; &#125; &#125; &#125; &#125; if (validated) &#123; if (oldVal != null) &#123; if (value == null) addCount(-1L, -1); return oldVal; &#125; break; &#125; &#125; &#125; return null; &#125; public void clear() &#123; long delta = 0L; // negative number of deletions int i = 0; Node&lt;K,V&gt;[] tab = table; while (tab != null &amp;&amp; i &lt; tab.length) &#123; int fh; Node&lt;K,V&gt; f = tabAt(tab, i); if (f == null) ++i; else if ((fh = f.hash) == MOVED) &#123; tab = helpTransfer(tab, f); i = 0; // restart &#125; else &#123; synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; p = (fh &gt;= 0 ? f : (f instanceof TreeBin) ? ((TreeBin&lt;K,V&gt;)f).first : null); while (p != null) &#123; --delta; p = p.next; &#125; setTabAt(tab, i++, null); &#125; &#125; &#125; &#125; if (delta != 0L) addCount(delta, -1); &#125; 参考：https://blog.csdn.net/fenglibing/article/details/17119959 https://blog.csdn.net/u011392897/article/details/60466665 https://blog.csdn.net/u011392897/article/details/60469263 https://blog.csdn.net/u011392897/article/details/60479937 https://juejin.im/post/5a815743f265da4e7e10b8e8 https://blog.csdn.net/hitxueliang/article/details/24734861]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中Unsafe类的使用]]></title>
    <url>%2F2018%2F10%2F23%2FJava%E4%B8%ADUnsafe%E7%B1%BB%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Unsafe类存在sun.misc.Unsafe中可以直接读写内存、获得地址偏移值、锁定或释放线程。concurrentHashmap的1.7版本大量使用该类来提高提升Java运行效率。下面介绍一下其使用。 基本类信息12345678910111213141516171819202122public class Dog &#123; private String name; private int age; public Dog()&#123; this.name = null; this.age = 0; &#125; public Dog(String name, int age)&#123; this.age = age; this.name = name; &#125; @Override public String toString() &#123; return "&#123;\"Dog\":&#123;" + " \"name\":\"" + name + "\"" + ", \"age\":\"" + age + "\"" + "&#125;&#125;"; &#125;&#125; 操作成员变量12345678910111213141516171819202122232425public class Test &#123; public Unsafe getUnsafe() &#123; try &#123; final Field unsafeField = Unsafe.class.getDeclaredField("theUnsafe"); unsafeField.setAccessible(true); // the unsafe instance return (Unsafe) unsafeField.get(null); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125; public static void main(String[] args)throws Exception &#123; Test test = new Test(); Unsafe unsafe = test.getUnsafe(); Dog dog = new Dog(); // 获取name属性偏移量 long nameOffset = unsafe.objectFieldOffset(Dog.class.getDeclaredField("name")); unsafe.putObject(dog, nameOffset, "旺财"); //获取age属性偏移量 long ageOffset = unsafe.objectFieldOffset(Dog.class.getDeclaredField("age")); unsafe.putInt(dog, ageOffset,3); System.out.println(dog.toString()); &#125;&#125; 操作数组123456789101112131415161718192021222324252627282930public class Test1 &#123; public Unsafe getUnsafe() &#123; try &#123; final Field unsafeField = Unsafe.class.getDeclaredField("theUnsafe"); unsafeField.setAccessible(true); // the unsafe instance return (Unsafe) unsafeField.get(null); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125; public static void main(String[] args)throws Exception &#123; Test1 test = new Test1(); Unsafe unsafe = test.getUnsafe(); //帮助理解concurrentHashmap Dog[] dogs = new Dog[3]; //获取数组中第一个元素的偏移量 long dogBase = unsafe.arrayBaseOffset(Dog[].class); //获取数组中一个元素的大小 int ts = unsafe.arrayIndexScale(Dog[].class); int dogShift = 31 - Integer.numberOfLeadingZeros(ts); unsafe.putOrderedObject(dogs, dogBase, new Dog("旺财",3)); unsafe.putOrderedObject(dogs, (long)(1 &lt;&lt; dogShift) + dogBase, new Dog("小强",4)); unsafe.putOrderedObject(dogs, ts*2 + dogBase, new Dog("富贵", 5)); for(int i = 0 ; i &lt; dogs.length ; i++)&#123; System.out.println(dogs[i].toString()); &#125; &#125;&#125;]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BIO、NIO和AIO]]></title>
    <url>%2F2018%2F10%2F20%2FBIO%E3%80%81NIO%E5%92%8CAIO%2F</url>
    <content type="text"><![CDATA[IO的方式通常分为几种，同步阻塞的BIO、同步非阻塞的NIO、异步非阻塞的AIO。 同步与异步同步和异步关注的是消息通信机制 (synchronous communication/ asynchronous communication)所谓同步，就是在发出一个”调用“时，在没有得到结果之前，该”调用“就不返回。但是一旦调用返回，就得到返回值了。换句话说，就是由”调用者“主动等待这个”调用“的结果。 而异步则是相反，调用者在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，”调用者“不会立刻得到结果。而是在”调用“发出后，”被调用者“通过状态、通知来通知”调用者“，或通过回调函数处理这个调用。 典型的异步编程模型比如Node.js 举个通俗的例子：你打电话问书店老板有没有《分布式系统》这本书，如果是同步通信机制，书店老板会说，你稍等，”我查一下”，然后开始查啊查，等查好了（可能是5秒，也可能是一天）告诉你结果（返回结果）。而异步通信机制，书店老板直接告诉你我查一下啊，查好了打电话给你，然后直接挂电话了（不返回结果）。然后查好了，他会主动打电话给你。在这里老板通过“回电”这种方式来回调。 阻塞与非阻塞阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态. 阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。 还是上面的例子，你打电话问书店老板有没有《分布式系统》这本书，你如果是阻塞式调用，你会一直把自己“挂起”，直到得到这本书有没有的结果，如果是非阻塞式调用，你不管老板有没有告诉你，你自己先一边去玩了， 当然你也要偶尔过几分钟check一下老板有没有返回结果。在这里阻塞与非阻塞与是否同步异步无关。跟老板通过什么方式回答你结果无关。 BIO​ 在JDK1.4出来之前，我们建立网络连接的时候采用BIO模式，需要先在服务端启动一个ServerSocket，然后在客户端启动Socket来对服务端进行通信，默认情况下服务端需要对每个请求建立一堆线程等待请求，而客户端发送请求后，先咨询服务端是否有线程相应，如果没有则会一直等待或者遭到拒绝请求，如果有的话，客户端会线程会等待请求结束后才继续执行。 NIO​ NIO本身是基于事件驱动思想来完成的，其主要想解决的是BIO的大并发问题： 在使用同步I/O的网络应用中，如果要同时处理多个客户端请求，或是在客户端要同时和多个服务器进行通讯，就必须使用多线程来处理。也就是说，将每一个客户端请求分配给一个线程来单独处理。这样做虽然可以达到我们的要求，但同时又会带来另外一个问题。由于每创建一个线程，就要为这个线程分配一定的内存空间（也叫工作存储器），而且操作系统本身也对线程的总数有一定的限制。如果客户端的请求过多，服务端程序可能会因为不堪重负而拒绝客户端的请求，甚至服务器可能会因此而瘫痪。 ​ NIO基于Reactor，当socket有流可读或可写入socket时，操作系统会相应的通知引用程序进行处理，应用再将流读取到缓冲区或写入操作系统。 也就是说，这个时候，已经不是一个连接就要对应一个处理线程了，而是有效的请求，对应一个线程，当连接没有数据时，是没有工作线程来处理的。 BIO与NIO一个比较重要的不同，是我们使用BIO的时候往往会引入多线程，每个连接一个单独的线程；而NIO则是使用单线程或者只使用少量的多线程，每个连接共用一个线程。 ​ NIO的最重要的地方是当一个连接创建后，不需要对应一个线程，这个连接会被注册到多路复用器上面，所以所有的连接只需要一个线程就可以搞定，当这个线程中的多路复用器进行轮询的时候，发现连接上有请求的话，才开启一个线程进行处理，也就是一个请求一个线程模式。 ​ 在NIO的处理方式中，当一个请求来的话，开启线程进行处理，可能会等待后端应用的资源(JDBC连接等)，其实这个线程就被阻塞了，当并发上来的话，还是会有BIO一样的问题。 HTTP/1.1出现后，有了Http长连接，这样除了超时和指明特定关闭的http header外，这个链接是一直打开的状态的，这样在NIO处理中可以进一步的进化，在后端资源中可以实现资源池或者队列，当请求来的话，开启的线程把请求和请求数据传送给后端资源池或者队列里面就返回，并且在全局的地方保持住这个现场(哪个连接的哪个请求等)，这样前面的线程还是可以去接受其他的请求，而后端的应用的处理只需要执行队列里面的就可以了，这样请求处理和后端应用是异步的.当后端处理完，到全局地方得到现场，产生响应，这个就实现了异步处理。 AIO​ 与NIO不同，当进行读写操作时，只须直接调用API的read或write方法即可。这两种方法均为异步的，对于读操作而言，当有流可读取时，操作系统会将可读的流传入read方法的缓冲区，并通知应用程序；对于写操作而言，当操作系统将write方法传递的流写入完毕时，操作系统主动通知应用程序。 即可以理解为，read/write方法都是异步的，完成后会主动调用回调函数。 在JDK1.7中，这部分内容被称作NIO.2，主要在java.nio.channels包下增加了下面四个异步通道： AsynchronousSocketChannel AsynchronousServerSocketChannel AsynchronousFileChannel AsynchronousDatagramChannel 其中的read/write方法，会返回一个带回调函数的对象，当执行完读取/写入操作后，直接调用回调函数。 BIO是一个连接一个线程。 NIO是一个请求一个线程。 AIO是一个有效请求一个线程。 先来个例子理解一下概念，以银行取款为例： 同步 ： 自己亲自出马持银行卡到银行取钱（使用同步IO时，Java自己处理IO读写）； 异步 ： 委托一小弟拿银行卡到银行取钱，然后给你（使用异步IO时，Java将IO读写委托给OS处理，需要将数据缓冲区地址和大小传给OS(银行卡和密码)，OS需要支持异步IO操作API）； 阻塞 ： ATM排队取款，你只能等待（使用阻塞IO时，Java调用会一直阻塞到读写完成才返回）； 非阻塞 ： 柜台取款，取个号，然后坐在椅子上做其它事，等号广播会通知你办理，没到号你就不能去，你可以不断问大堂经理排到了没有，大堂经理如果说还没到你就不能去（使用非阻塞IO时，如果不能读写Java调用会马上返回，当IO事件分发器会通知可读写时再继续进行读写，不断循环直到读写完成） Java对BIO、NIO、AIO的支持 Java BIO ： 同步并阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。 Java NIO ： 同步非阻塞，服务器实现模式为一个请求一个线程，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。 Java AIO(NIO.2) ： 异步非阻塞，服务器实现模式为一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理。 BIO、NIO、AIO适用场景分析: BIO方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4以前的唯一选择，但程序直观简单易理解。 NIO方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。 AIO方式使用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持。 另外，I/O属于底层操作，需要操作系统支持，并发也需要操作系统的支持，所以性能方面不同操作系统差异会比较明显。 在高性能的I/O设计中，有两个比较著名的模式Reactor和Proactor模式，其中Reactor模式用于同步I/O，而Proactor运用于异步I/O操作。 ​ 在比较这两个模式之前，我们首先的搞明白几个概念，什么是阻塞和非阻塞，什么是同步和异步,同步和异步是针对应用程序和内核的交互而言的，同步指的是用户进程触发IO操作并等待或者轮询的去查看IO操作是否就绪，而异步是指用户进程触发IO操作以后便开始做自己的事情，而当IO操作已经完成的时候会得到IO完成的通知。而阻塞和非阻塞是针对于进程在访问数据的时候，根据IO操作的就绪状态来采取的不同方式，说白了是一种读取或者写入操作函数的实现方式，阻塞方式下读取或者写入函数将一直等待，而非阻塞方式下，读取或者写入函数会立即返回一个状态值。 一般来说I/O模型可以分为：同步阻塞，同步非阻塞，异步阻塞，异步非阻塞IO 同步阻塞IO：在此种方式下，用户进程在发起一个IO操作以后，必须等待IO操作的完成，只有当真正完成了IO操作以后，用户进程才能运行。JAVA传统的IO模型属于此种方式！ 同步非阻塞IO:在此种方式下，用户进程发起一个IO操作以后边可返回做其它事情，但是用户进程需要时不时的询问IO操作是否就绪，这就要求用户进程不停的去询问，从而引入不必要的CPU资源浪费。其中目前JAVA的NIO就属于同步非阻塞IO。 异步阻塞IO：此种方式下是指应用发起一个IO操作以后，不等待内核IO操作的完成，等内核完成IO操作以后会通知应用程序，这其实就是同步和异步最关键的区别，同步必须等待或者主动的去询问IO是否完成，那么为什么说是阻塞的呢？因为此时是通过select系统调用来完成的，而select函数本身的实现方式是阻塞的，而采用select函数有个好处就是它可以同时监听多个文件句柄，从而提高系统的并发性！ 异步非阻塞IO：在此种模式下，用户进程只需要发起一个IO操作然后立即返回，等IO操作真正的完成以后，应用程序会得到IO操作完成的通知，此时用户进程只需要对数据进行处理就好了，不需要进行实际的IO读写操作，因为真正的IO读取或者写入操作已经由内核完成了。目前Java中还没有支持此种IO模型。 1、烧水示例 老张爱喝茶，废话不说，煮开水。出场人物：老张，水壶两把（普通水壶，简称水壶；会响的水壶，简称响水壶）。1 老张把水壶放到火上，立等水开。（同步阻塞）老张觉得自己有点傻2 老张把水壶放到火上，去客厅看电视，时不时去厨房看看水开没有。（同步非阻塞）老张还是觉得自己有点傻，于是变高端了，买了把会响笛的那种水壶。水开之后，能大声发出嘀~~~~的噪音。3 老张把响水壶放到火上，立等水开。（异步阻塞）老张觉得这样傻等意义不大4 老张把响水壶放到火上，去客厅看电视，水壶响之前不再去看它了，响了再去拿壶。（异步非阻塞）老张觉得自己聪明了。 所谓同步异步，只是对于水壶而言。普通水壶，同步；响水壶，异步。虽然都能干活，但响水壶可以在自己完工之后，提示老张水开了。这是普通水壶所不能及的。同步只能让调用者去轮询自己（情况2中），造成老张效率的低下。 所谓阻塞非阻塞，仅仅对于老张而言。立等的老张，阻塞；看电视的老张，非阻塞。情况1和情况3中老张就是阻塞的，媳妇喊他都不知道。虽然3中响水壶是异步的，可对于立等的老张没有太大的意义。所以一般异步是配合非阻塞使用的，这样才能发挥异步的效用。 2、在处理 IO 的时候，阻塞和非阻塞都是同步 IO。只有使用了特殊的 API 才是异步 IO。 3个层次的同步、异步、阻塞、非阻塞 CPU层次； 线程层次； 程序员感知层次。 这几个概念之所以容易混淆，是因为没有分清楚是在哪个层次进行讨论。 CPU层次在CPU层次，或者说操作系统进行IO和任务调度的层次，现代操作系统通常使用异步非阻塞方式进行IO（有少部分IO可能会使用同步非阻塞轮询），即发出IO请求之后，并不等待IO操作完成，而是继续执行下面的指令（非阻塞），IO操作和CPU指令互不干扰（异步），最后通过中断的方式来通知IO操作完成结果。 线程层次在线程层次，或者说操作系统调度单元的层次，操作系统为了减轻程序员的思考负担，将底层的异步非阻塞的IO方式进行封装，把相关系统调用（如read，write等）以同步的方式展现出来。然而，同步阻塞的IO会使线程挂起，同步非阻塞的IO会消耗CPU资源在轮询上。为了解决这一问题，就有3种思路： 多线程（同步阻塞）； IO多路复用（select，poll，epoll）（同步非阻塞，严格地来讲，是把阻塞点改变了位置）； 直接暴露出异步的IO接口，如kernel-aio和IOCP（异步非阻塞）。 程序员感知层次在Linux中，上面提到的第2种思路用得比较广泛，也是比较理想的解决方案。然而，直接使用select之类的接口，依然比较复杂，所以各种库和框架百花齐放，都试图对IO多路复用进行封装。此时，库和框架提供的API又可以选择是以同步的方式还是异步的方式来展现。如python的asyncio库中，就通过协程，提供了同步阻塞式的API；如node.js中，就通过回调函数，提供了异步非阻塞式的API。 总结因此，我们在讨论同步、异步、阻塞、非阻塞时，必须先明确是在哪个层次进行讨论。比如node.js，我们可以说她在程序员感知层次提供了异步非阻塞的API，也可以说在Linux下，她在线程层次以同步非阻塞的epoll来实现。 参考https://www.zhihu.com/question/19732473 http://www.cnblogs.com/moonandstar08/p/5068339.html https://www.jianshu.com/p/ef418ccf2f7d https://www.jianshu.com/p/ef418ccf2f7d]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式事务提交协议——2PC、3PC]]></title>
    <url>%2F2018%2F10%2F12%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%8F%90%E4%BA%A4%E5%8D%8F%E8%AE%AE%E2%80%94%E2%80%942PC%E3%80%813PC%2F</url>
    <content type="text"><![CDATA[随着大型网站的各种高并发访问、海量数据处理等场景越来越多，如何实现网站的高可用、易伸缩、可扩展、安全等目标就显得越来越重要。为了解决这样一系列问题，大型网站的架构也在不断发展。提高大型网站的高可用架构，不得不提的就是分布式。本文将简单介绍如何有效的解决分布式的一致性问题,其中包括什么是分布式事务，二阶段提交和三阶段提交。 分布式一致性在分布式系统中，为了保证数据的高可用，通常将数据保留多个副本(replica)，这些副本会放置在不同的物理的机器上。为了对用户提供正确的增\删\改\差等语义，我们需要保证这些放置在不同物理机器上的副本是一致的。 为了解决这种分布式一致性问题，前人在性能和数据一致性的反反复复权衡过程中总结了许多典型的协议和算法。其中比较著名的有二阶提交协议（Two Phase Commitment Protocol）、三阶提交协议（Three Phase Commitment Protocol）和Paxos算法。 分布式事务 分布式事务是指会涉及到操作多个数据库的事务。其实就是将对同一库事务的概念扩大到了对多个库的事务。目的是为了保证分布式系统中的数据一致性。分布式事务处理的关键是必须有一种方法可以知道事务在任何地方所做的所有动作，提交或回滚事务的决定必须产生统一的结果（全部提交或全部回滚） 在分布式系统中，各个节点之间在物理上相互独立，通过网络进行沟通和协调。由于存在事务机制，可以保证每个独立节点上的数据操作可以满足ACID。但是，相互独立的节点之间无法准确的知道其他节点中的事务执行情况。所以从理论上讲，两台机器理论上无法达到一致的状态。如果想让分布式部署的多台机器中的数据保持一致性，那么就要保证在所有节点的数据写操作，要不全部都执行，要么全部的都不执行。但是，一台机器在执行本地事务的时候无法知道其他机器中的本地事务的执行结果。所以他也就不知道本次事务到底应该commit还是 roolback。所以，常规的解决办法就是引入一个“协调者”的组件来统一调度所有分布式节点的执行。 XA规范X/Open 组织（即现在的 Open Group ）定义了分布式事务处理模型。 X/Open DTP 模型（ 1994 ）包括应用程序（ AP ）、事务管理器（ TM ）、资源管理器（ RM ）、通信资源管理器（ CRM ）四部分。一般，常见的事务管理器（ TM ）是交易中间件，常见的资源管理器（ RM ）是数据库，常见的通信资源管理器（ CRM ）是消息中间件。 通常把一个数据库内部的事务处理，如对多个表的操作，作为本地事务看待。数据库的事务处理对象是本地事务，而分布式事务处理的对象是全局事务。 所谓全局事务，是指分布式事务处理环境中，多个数据库可能需要共同完成一个工作，这个工作即是一个全局事务，例如，一个事务中可能更新几个不同的数据库。对数据库的操作发生在系统的各处但必须全部被提交或回滚。此时一个数据库对自己内部所做操作的提交不仅依赖本身操作是否成功，还要依赖与全局事务相关的其它数据库的操作是否成功，如果任一数据库的任一操作失败，则参与此事务的所有数据库所做的所有操作都必须回滚。 一般情况下，某一数据库无法知道其它数据库在做什么，因此，在一个 DTP 环境中，交易中间件是必需的，由它通知和协调相关数据库的提交或回滚。而一个数据库只将其自己所做的操作（可恢复）影射到全局事务中。 XA 就是 X/Open DTP 定义的交易中间件与数据库之间的接口规范（即接口函数），交易中间件用它来通知数据库事务的开始、结束以及提交、回滚等。 XA 接口函数由数据库厂商提供。 二阶提交协议和三阶提交协议就是根据这一思想衍生出来的。可以说二阶段提交其实就是实现XA分布式事务的关键(确切地说：两阶段提交主要保证了分布式事务的原子性：即所有结点要么全做要么全不做) 2PC 二阶段提交(Two-phaseCommit)是指，在计算机网络以及数据库领域内，为了使基于分布式系统架构下的所有节点在进行事务提交时保持一致性而设计的一种算法(Algorithm)。通常，二阶段提交也被称为是一种协议(Protocol))。在分布式系统中，每个节点虽然可以知晓自己的操作时成功或者失败，却无法知道其他节点的操作的成功或失败。当一个事务跨越多个节点时，为了保持事务的ACID特性，需要引入一个作为协调者的组件来统一掌控所有节点(称作参与者)的操作结果并最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)。因此，二阶段提交的算法思路可以概括为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。 所谓的两个阶段是指：第一阶段：准备阶段(投票阶段)和第二阶段：提交阶段（执行阶段）。 准备阶段事务协调者(事务管理器)给每个参与者(资源管理器)发送Prepare消息，每个参与者要么直接返回失败(如权限验证失败)，要么在本地执行事务，写本地的redo和undo日志，但不提交，到达一种“万事俱备，只欠东风”的状态。 可以进一步将准备阶段分为以下三个步骤： 1）协调者节点向所有参与者节点询问是否可以执行提交操作(vote)，并开始等待各参与者节点的响应。 2）参与者节点执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入日志。（注意：若成功这里其实每个参与者已经执行了事务操作） 3）各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。 提交阶段如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚(Rollback)消息；否则，发送提交(Commit)消息；参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源。(注意:必须在最后阶段释放锁资源) 接下来分两种情况分别讨论提交阶段的过程。 当协调者节点从所有参与者节点获得的相应消息都为”同意”时: 1）协调者节点向所有参与者节点发出”正式提交(commit)”的请求。 2）参与者节点正式完成操作，并释放在整个事务期间内占用的资源。 3）参与者节点向协调者节点发送”完成”消息。 4）协调者节点受到所有参与者节点反馈的”完成”消息后，完成事务。 如果任一参与者节点在第一阶段返回的响应消息为”中止”，或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时： 1）协调者节点向所有参与者节点发出”回滚操作(rollback)”的请求。 2）参与者节点利用之前写入的Undo信息执行回滚，并释放在整个事务期间内占用的资源。 3）参与者节点向协调者节点发送”回滚完成”消息。 4）协调者节点受到所有参与者节点反馈的”回滚完成”消息后，取消事务。 不管最后结果如何，第二阶段都会结束当前事务。 二阶段提交看起来确实能够提供原子性的操作，但是不幸的事，二阶段提交还是有几个缺点的。 2PC的缺点 1、同步阻塞问题。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。 2、单点故障。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题） 3、数据不一致。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。 4、二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。 由于二阶段提交存在着诸如同步阻塞、单点问题、脑裂等缺陷，所以，研究者们在二阶段提交的基础上做了改进，提出了三阶段提交。 3PC 三阶段提交（Three-phase commit），也叫三阶段提交协议（Three-phase commit protocol），是二阶段提交（2PC）的改进版本。 与两阶段提交不同的是，三阶段提交有两个改动点。 121、引入超时机制。同时在协调者和参与者中都引入超时机制。2、在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。 也就是说，除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。 CanCommit阶段3PC的CanCommit阶段其实和2PC的准备阶段很像。协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。 1.事务询问 协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。 2.响应反馈 参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态。否则反馈No PreCommit阶段协调者根据参与者的反应情况来决定是否可以记性事务的PreCommit操作。根据响应情况，有以下两种可能。 假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行。 1.发送预提交请求 协调者向参与者发送PreCommit请求，并进入Prepared阶段。 2.事务预提交 参与者接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。 3.响应反馈 如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。 假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。 1.发送中断请求 协调者向所有参与者发送abort请求。 2.中断事务 参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。 doCommit阶段该阶段进行真正的事务提交，也可以分为以下两种情况。 执行提交 1.发送提交请求 协调接收到参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送doCommit请求。 2.事务提交 参与者接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。 3.响应反馈 事务提交完之后，向协调者发送Ack响应。 4.完成事务 协调者接收到所有参与者的ack响应之后，完成事务。 中断事务协调者没有接收到参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。 1.发送中断请求 协调者向所有参与者发送abort请求 2.事务回滚 参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。 3.反馈结果 参与者完成事务回滚之后，向协调者发送ACK消息 4.中断事务 协调者接收到参与者反馈的ACK消息之后，执行事务的中断。 1在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交。（其实这个应该是基于概率来决定的，当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求，那么协调者产生PreCommit请求的前提条件是他在第二阶段开始之前，收到所有参与者的CanCommit响应都是Yes。（一旦参与者收到了PreCommit，意味他知道大家其实都同意修改了）所以，一句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到commit或者abort响应，但是他有理由相信：成功提交的几率很大。 ） 生活中类似三阶段提交的例子： 班长要组织全班同学聚餐，由于大家毕业多年，所以要逐个打电话敲定时间，时间初定10.1日。然后开始逐个打电话。 班长：小A，我们想定在10.1号聚会，你有时间嘛？有时间你就说YES，没有你就说NO，然后我还会再去问其他人，具体时间地点我会再通知你，这段时间你可先去干你自己的事儿，不用一直等着我。（协调者询问事务是否可以执行，这一步不会锁定资源） 小A：好的，我有时间。（参与者反馈） 班长：小B，我们想定在10.1号聚会……不用一直等我。 班长收集完大家的时间情况了，一看大家都有时间，那么就再次通知大家。（协调者接收到所有YES指令） 班长：小A，我们确定了10.1号聚餐，你要把这一天的时间空出来，这一天你不能再安排其他的事儿了。然后我会逐个通知其他同学，通知完之后我会再来和你确认一下，还有啊，如果我没有特意给你打电话，你就10.1号那天来聚餐就行了。对了，你确定能来是吧？（协调者发送事务执行指令，这一步锁住资源。如果由于网络原因参与者在后面没有收到协调者的命令，他也会执行commit） 小A顺手在自己的日历上把10.1号这一天圈上了，然后跟班长说，我可以去。（参与者执行事务操作，反馈状态） 班长：小B，我们觉得了10.1号聚餐……你就10.1号那天来聚餐就行了。 班长通知完一圈之后。所有同学都跟他说：”我已经把10.1号这天空出来了”。于是，他在10.1号这一天又挨个打了一遍电话告诉他们：嘿，现在你们可以出门拉。。。。（协调者收到所有参与者的ACK响应，通知所有参与者执行事务的commit） 小A，小B：我已经出门拉。（执行commit操作，反馈状态） 3PC存在的一致性问题在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交。 所以，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。 2PC与3PC的区别相对于2PC，3PC主要解决的单点故障问题，并减少阻塞，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。 了解了2PC和3PC之后，我们可以发现，无论是二阶段提交还是三阶段提交都无法彻底解决分布式的一致性问题。Google Chubby的作者Mike Burrows说过， there is only one consensus protocol, and that’s Paxos” – all other approaches are just broken versions of Paxos. 意即世上只有一种一致性算法，那就是Paxos，所有其他一致性算法都是Paxos算法的不完整版。 参考http://www.hollischuang.com/archives/681 http://www.hollischuang.com/archives/1580]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>一致性</tag>
        <tag>数据库</tag>
        <tag>分布式</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap容量的初始化]]></title>
    <url>%2F2018%2F10%2F10%2FHashMap%E5%AE%B9%E9%87%8F%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96%2F</url>
    <content type="text"><![CDATA[默认情况下HashMap的容量是16，但是，如果用户通过构造函数指定了一个数字作为容量，那么Hash会选择大于该数字的第一个2的幂作为容量。 为什么要设置HashMap的初始化容量《阿里巴巴Java开发手册》中建议设置HashMap的初始化容量。 那么，为什么要这么建议？ 先来写一段代码在JDK 1.7 （jdk1.7.0_79）下面来分别测试下，在不指定初始化容量和指定初始化容量的情况下性能情况如何。 1234567891011121314151617181920212223242526272829303132333435public static void main(String[] args) &#123; int aHundredMillion = 10000000; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); long s1 = System.currentTimeMillis(); for (int i = 0; i &lt; aHundredMillion; i++) &#123; map.put(i, i); &#125; long s2 = System.currentTimeMillis(); System.out.println("未初始化容量，耗时 ： " + (s2 - s1)); Map&lt;Integer, Integer&gt; map1 = new HashMap&lt;&gt;(aHundredMillion / 2); long s5 = System.currentTimeMillis(); for (int i = 0; i &lt; aHundredMillion; i++) &#123; map1.put(i, i); &#125; long s6 = System.currentTimeMillis(); System.out.println("初始化容量5000000，耗时 ： " + (s6 - s5)); Map&lt;Integer, Integer&gt; map2 = new HashMap&lt;&gt;(aHundredMillion); long s3 = System.currentTimeMillis(); for (int i = 0; i &lt; aHundredMillion; i++) &#123; map2.put(i, i); &#125; long s4 = System.currentTimeMillis(); System.out.println("初始化容量为10000000，耗时 ： " + (s4 - s3));&#125; 以上代码不难理解，创建了3个HashMap，分别使用默认的容量（16）、使用元素个数的一半（5千万）作为初始容量、使用元素个数（一亿）作为初始容量进行初始化。然后分别向其中put一亿个KV。 输出结果： 123未初始化容量，耗时 ： 14419初始化容量5000000，耗时 ： 11916初始化容量为10000000，耗时 ： 7984 从结果中，可以知道，在已知HashMap中将要存放的KV个数的时候，设置一个合理的初始化容量可以有效的提高性能。 HashMap有扩容机制，就是当达到扩容条件时会进行扩容。HashMap的扩容条件就是当HashMap中的元素个数（size）超过临界值（threshold）时就会自动扩容。在HashMap中，threshold = loadFactor * capacity。 所以，如果没有设置初始容量大小，随着元素的不断增加，HashMap会发生多次扩容，而HashMap中的扩容机制决定了每次扩容都需要重建hash表，是非常影响性能的。 从上面的代码示例中，还可以发现，同样是设置初始化容量，设置的数值不同也会影响性能，那么当已知HashMap中即将存放的KV个数的时候，容量设置成多少为好呢？ HashMap中容量的初始化默认情况下，当设置HashMap的初始化容量时，实际上HashMap会采用第一个大于该数值的2的幂作为初始化容量。 1234567Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(1);map.put("hahaha", "hollischuang");Class&lt;?&gt; mapType = map.getClass();Method capacity = mapType.getDeclaredMethod("capacity");capacity.setAccessible(true);System.out.println("capacity : " + capacity.invoke(map)); 初始化容量设置成1的时候，输出结果是2。在jdk1.8中，如果传入的初始化容量为1，实际上设置的结果也为1，上面代码输出结果为2的原因是代码中map.put(“hahaha”, “hollischuang”);导致了扩容，容量从1扩容到2。 那么，话题再说回来，当通过HashMap(int initialCapacity)设置初始容量的时候，HashMap并不一定会直接采用传入的数值，而是经过计算，得到一个新值，目的是提高hash的效率。(1-&gt;1、3-&gt;4、7-&gt;8、9-&gt;16) 在Jdk 1.7和Jdk 1.8中，HashMap初始化这个容量的时机不同。jdk1.8中，在调用HashMap的构造函数定义HashMap的时候，就会进行容量的设定。而在Jdk 1.7中，要等到第一次put操作时才进行这一操作。 不管是Jdk 1.7还是Jdk 1.8，计算初始化容量的算法其实是如出一辙的，主要代码如下： 1234567int n = cap - 1;n |= n &gt;&gt;&gt; 1;n |= n &gt;&gt;&gt; 2;n |= n &gt;&gt;&gt; 4;n |= n &gt;&gt;&gt; 8;n |= n &gt;&gt;&gt; 16;return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; 上面的代码挺有意思的，一个简单的容量初始化，Java的工程师也有很多考虑在里面。 上面的算法目的挺简单，就是：根据用户传入的容量值（代码中的cap），通过计算，得到第一个比他大的2的幂并返回。 聪明的读者们，如果让你设计这个算法你准备如何计算？如果你想到二进制的话，那就很简单了。举几个例子看一下： 关注上面的几个例子中，蓝色字体部分的变化情况，可以发现些规律。5-&gt;8、9-&gt;16、19-&gt;32、37-&gt;64都是主要经过了两个阶段。 Step 1，5-&gt;7 Step 2，7-&gt;8 Step 1，9-&gt;15 Step 2，15-&gt;16 Step 1，19-&gt;31 Step 2，31-&gt;32 对应到以上代码中，Step1： 12345n |= n &gt;&gt;&gt; 1;n |= n &gt;&gt;&gt; 2;n |= n &gt;&gt;&gt; 4;n |= n &gt;&gt;&gt; 8;n |= n &gt;&gt;&gt; 16; 对应到以上代码中，Step2： 1return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; Step 2 比较简单，就是做一下极限值的判断，然后把Step 1得到的数值+1。 Step 1 怎么理解呢？其实是对一个二进制数依次向右移位，然后与原值取或。其目的对于一个数字的二进制，从第一个不为0的位开始，把后面的所有位都设置成1。 随便拿一个二进制数，套一遍上面的公式就发现其目的了： 1234561100 1100 1100 &gt;&gt;&gt;1 = 0110 0110 01101100 1100 1100 | 0110 0110 0110 = 1110 1110 11101110 1110 1110 &gt;&gt;&gt;2 = 0011 1011 10111110 1110 1110 | 0011 1011 1011 = 1111 1111 11111111 1111 1111 &gt;&gt;&gt;4 = 1111 1111 11111111 1111 1111 | 1111 1111 1111 = 1111 1111 1111 通过几次无符号右移和按位或运算，把1100 1100 1100转换成了1111 1111 1111 ，再把1111 1111 1111加1，就得到了1 0000 0000 0000，这就是大于1100 1100 1100的第一个2的幂。 好了，现在解释清楚了Step 1和Step 2的代码。就是可以把一个数转化成第一个比他自身大的2的幂。（可以开始佩服Java的工程师们了，使用无符号右移和按位或运算大大提升了效率。） 但是还有一种特殊情况套用以上公式不行，这些数字就是2的幂自身。如果数字4 套用公式的话。得到的会是 8 ： 123456789Step 1: 0100 &gt;&gt;&gt;1 = 00100100 | 0010 = 01100110 &gt;&gt;&gt;1 = 00110110 | 0011 = 0111Step 2:0111 + 0001 = 1000 为了解决这个问题，JDK的工程师把所有用户传进来的数在进行计算之前先-1，就是源码中的第一行： 1int n = cap - 1; 至此，再来回过头看看这个设置初始容量的代码，目的是不是一目了然了： 1234567int n = cap - 1;n |= n &gt;&gt;&gt; 1;n |= n &gt;&gt;&gt; 2;n |= n &gt;&gt;&gt; 4;n |= n &gt;&gt;&gt; 8;n |= n &gt;&gt;&gt; 16;return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; HashMap中初始容量的合理值当使用HashMap(int initialCapacity)来初始化容量的时候，jdk会默认计算一个相对合理的值当做初始容量。那么，是不是只需要把已知的HashMap中即将存放的元素个数直接传给initialCapacity就可以了呢？ 关于这个值的设置，在《阿里巴巴Java开发手册》有以下建议： 这个值，并不是阿里巴巴的工程师原创的，在guava（21.0版本）中也使用的是这个值。 123456789101112131415161718192021public static &lt;K, V&gt; HashMap&lt;K, V&gt; newHashMapWithExpectedSize(int expectedSize) &#123; return new HashMap&lt;K, V&gt;(capacity(expectedSize));&#125;/*** Returns a capacity that is sufficient to keep the map from being resized as long as it grows no* larger than expectedSize and the load factor is ≥ its default (0.75).*/static int capacity(int expectedSize) &#123; if (expectedSize &lt; 3) &#123; checkNonnegative(expectedSize, "expectedSize"); return expectedSize + 1; &#125; if (expectedSize &lt; Ints.MAX_POWER_OF_TWO) &#123; // This is the calculation used in JDK8 to resize when a putAll // happens; it seems to be the most conservative calculation we // can make. 0.75 is the default load factor. return (int) ((float) expectedSize / 0.75F + 1.0F); &#125; return Integer.MAX_VALUE; // any large value&#125; 在return (int) ((float) expectedSize / 0.75F + 1.0F);上面有一行注释，说明了这个公式也不是guava原创，参考的是JDK8中putAll方法中的实现的。感兴趣的读者可以去看下putAll方法的实现，也是以上的这个公式。 虽然，当使用HashMap(int initialCapacity)来初始化容量的时候，jdk会默认计算一个相对合理的值当做初始容量。但是这个值并没有参考loadFactor的值。 也就是说，如果设置的默认值是7，经过Jdk处理之后，会被设置成8，但是，这个HashMap在元素个数达到 8*0.75 = 6的时候就会进行一次扩容，这明显是我们不希望见到的。 如果通过expectedSize / 0.75F + 1.0F计算，7/0.75 + 1 = 10 ,10经过Jdk处理之后，会被设置成16，这就大大的减少了扩容的几率。 当HashMap内部维护的哈希表的容量达到75%时（默认情况下），会触发rehash，而rehash的过程是比较耗费时间的。所以初始化容量要设置成expectedSize/0.75 + 1的话，可以有效的减少冲突也可以减小误差。 所以，我可以认为，当明确知道HashMap中元素的个数的时候，把默认容量设置成expectedSize / 0.75F + 1.0F 是一个在性能上相对好的选择，但是，同时也会牺牲些内存。 总结当想要在代码中创建一个HashMap的时候，如果已知这个Map中即将存放的元素个数，给HashMap设置初始容量可以在一定程度上提升效率。 但是，JDK并不会直接拿用户传进来的数字当做默认容量，而是会进行一番运算，最终得到一个2的幂，得到这个数字的算法其实是使用了使用无符号右移和按位或运算来提升效率。 但是，为了最大程度的避免扩容带来的性能消耗，建议可以把默认容量的数字设置成expectedSize / 0.75F + 1.0F 。在日常开发中，可以使用 1Map&lt;String, String&gt; map = Maps.newHashMapWithExpectedSize(10); 来创建一个HashMap，计算的过程guava会完成。 但是，以上的操作是一种用内存换性能的做法，真正使用的时候，要考虑到内存的影响。 参考：https://mp.weixin.qq.com/s/Rf0eyCTlb4raxk0iQlbHlg]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>数据结构</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大话数据结构——排序]]></title>
    <url>%2F2018%2F10%2F04%2F%E5%A4%A7%E8%AF%9D%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E2%80%94%E2%80%94%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[本文在阅读大话数据结构这本书的基础上，结合java语言的特点，来理解排序。 排序方法 平均情况 最好情况 最坏情况 辅助空间 稳定性 冒泡排序 O(n^2) O(n) O(n^2) O(1) 稳定 简单选择排序 O(n^2) O(n^2) O(n^2) O(1) 稳定 直接插入排序 O(n^2) O(n) O(n^2) O(1) 稳定 希尔排序 O(nlogn)~O(n^2) O(n^1.3) O(n^2) O(1) 不稳定 堆排序 O(nlogn) O(nlogn) O(nlogn) O(1) 不稳定 归并排序 O(nlogn) O(nlogn) O(nlogn) O(n) 稳定 快速排序 O(nlogn) O(nlogn) O(n^2) O(logn)~O(n) 不稳定 排序的基本概念 假设含有n个记录的序列为{r1,r2,……,rn}，其相应的关键字分别为{k1,k2,……,kn}，需确定1，2，……，n的一种排列p1,p2,……,pn，使其相应的关键字满足kp1≤kp2≤……≤kpn（非递减或非递增）关系，即使得序列成为一个按关键字有序的序列{rp1,rp2,……,rpn}，这样的操作就称为排序。假设ki=kj（1≤i≤n,1≤j≤n,i≠j），且在排序前的序列中ri领先于rj（即i&lt;j）。如果排序后ri仍领先于rj，则称所用的排序方法是稳定的；反之，若可能使得排序后的序列中rj领先ri，则称所用的排序方法是不稳定的。 内排序与外排序：根据在排序过程中待排序的记录是否全部被放置在内存中。内排序是在排序整个过程中，待排序的所有记录全部被放置在内存中。外排序是由于排序的记录个数太多，不能同时放置在内存，整个排序过程需要在内外存之间多次交换数据才能进行。 对于内排序来说，排序算法的性能主要是受三个方面影响：时间性能；辅助空间；算法的复杂性。 排序的结构图如下： 交换排序冒泡排序包含传统冒泡和优化冒泡两种方式实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class Sort &#123; public void swap(int[] array,int i,int j)&#123; if(i&lt;array.length&amp;&amp;j&lt;array.length)&#123; int data=array[i]; array[i]=array[j]; array[j]=data; &#125; &#125;&#125;/** * 冒泡排序 */public class BubbleSort extends Sort&#123; /** * 冒泡排序 */ public void bubbleSort(int[] array)&#123; if(array.length&lt;2)&#123; return; &#125; int i,j; for(i=0;i&lt;array.length-1;i++)&#123; for(j=array.length-1;j&gt;i;j--)&#123; if(array[j]&lt;array[j-1])&#123; swap(array,j,j-1); &#125; &#125; &#125; &#125; /** * 优化的冒泡排序 */ public void bubbleSort2(int[] array)&#123; if(array.length&lt;2)&#123; return; &#125; int i,j; boolean flag=true; for(i=0 ; i&lt;array.length-1 &amp;&amp; flag ; i++)&#123; flag=false; for(j=array.length-1;j&gt;i;j--)&#123; if(array[j]&lt;array[j-1])&#123; swap(array,j,j-1); &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; int[] array=&#123;9,1,5,8,3,7,4,6,2&#125;; System.out.println(Arrays.toString(array)); BubbleSort bubbleSort=new BubbleSort(); bubbleSort.bubbleSort(array); System.out.println(Arrays.toString(array)); int[] array2=&#123;2,1,3,4,5,6,7,8,9&#125;; System.out.println(Arrays.toString(array2)); bubbleSort.bubbleSort2(array2); System.out.println(Arrays.toString(array2)); &#125;&#125; 快速排序包含递归和尾递归优化实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374/** * 快速排序 */public class QuickSort &#123; /** * 对数组进行快速排序 */ public void quickSort(int[] array)&#123; if(array.length&gt;1)&#123; qSort(array,0,array.length-1); &#125; &#125; /** * 利用递归对数组子序列进行快速排序 */ private void qSort(int[] array,int left,int right)&#123; if(left&lt;right)&#123; int pivot; pivot=partition(array,left,right); qSort(array,left,pivot); qSort(array,pivot+1,right); &#125; &#125; /** * 递归优化对数组进行快速排序 */ public void quickSort2(int[] array)&#123; if(array.length&gt;1)&#123; qSort2(array,0,array.length-1); &#125; &#125; /** * 尾递归优化对数组子序列进行快速排序 */ private void qSort2(int[] array,int left,int right)&#123; while(left&lt;right)&#123; int pivot; pivot=partition(array,left,right); qSort2(array,left,pivot); left=pivot+1; &#125; &#125; /** * 分区（将较小的数分至左侧，较大的数分至右侧） */ private int partition(int[] array,int left,int right)&#123; // TODO（可优化） 将枢轴（支点）放至数组第一个元素 int data=array[left]; while(left&lt;right)&#123; while(left&lt;right &amp;&amp; array[right]&gt;=data)&#123; right--; &#125; array[left]=array[right];//将右侧较小的数替换至左侧 while(left&lt;right &amp;&amp; array[left]&lt;=data)&#123; left++; &#125; array[right]=array[left];//将左侧较大的数替换至右侧 &#125; array[left]=data;//将枢轴替换至left位置 return left; &#125; public static void main(String[] args) &#123; int[] array=&#123;9,1,5,8,3,7,4,6,2&#125;; System.out.println(Arrays.toString(array)); QuickSort quickSort=new QuickSort(); quickSort.quickSort(array); System.out.println(Arrays.toString(array)); int[] array2=&#123;9,1,5,8,3,7,4,6,2&#125;; System.out.println(Arrays.toString(array2)); quickSort.quickSort2(array2); System.out.println(Arrays.toString(array2)); &#125;&#125; 插入排序直接插入排序12345678910111213141516171819202122232425262728/** * 直接插入排序 */public class StraightInsertionSort &#123; public void insertSort(int[] array)&#123; if(array.length&gt;1)&#123; int i,j; for(i=1;i&lt;array.length;i++)&#123; if(array[i-1]&gt;array[i])&#123; int data=array[i]; for(j=i;j&gt;0 &amp;&amp; array[j-1]&gt;data;j--)&#123; array[j]=array[j-1]; &#125; array[j]=data; &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; int[] array=&#123;9,1,5,8,3,7,4,6,2&#125;; System.out.println(Arrays.toString(array)); StraightInsertionSort straightInsertionSort=new StraightInsertionSort(); straightInsertionSort.insertSort(array); System.out.println(Arrays.toString(array)); &#125;&#125; 希尔排序12345678910111213141516171819202122232425262728/** * 希尔排序 */public class ShellSort extends Sort&#123; public void shellSort(int[] array)&#123; if(array.length&gt;1)&#123; int increment=array.length/2; while(increment&gt;=1)&#123; for(int i=0;i&lt;array.length;i++)&#123; for(int j=i;j&lt;array.length-increment;j +=increment)&#123; if(array[j]&gt;array[j+increment])&#123; swap(array,j,j+increment); &#125; &#125; &#125; increment=increment/2; &#125; &#125; &#125; public static void main(String[] args) &#123; int[] array=&#123;9,1,5,8,3,7,4,6,2&#125;; System.out.println(Arrays.toString(array)); ShellSort shellSort=new ShellSort(); shellSort.shellSort(array); System.out.println(Arrays.toString(array)); &#125;&#125; 选择排序简单选择排序1234567891011121314151617181920212223242526272829/** * 简单选择排序 */public class SimpleSelectionSort extends Sort&#123; public void selectSort(int[] array)&#123; if(array.length&gt;1)&#123; int i,j,min; for(i=0;i&lt;array.length-1;i++)&#123; min=i; for(j=min+1;j&lt;array.length;j++)&#123; if(array[min]&gt;array[j])&#123; min=j; &#125; &#125; if(min!=i)&#123; swap(array,i,min); &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; int[] array=&#123;9,1,5,8,3,7,4,6,2&#125;; System.out.println(Arrays.toString(array)); SimpleSelectionSort simpleSelectionSort=new SimpleSelectionSort(); simpleSelectionSort.selectSort(array); System.out.println(Arrays.toString(array)); &#125;&#125; 堆排序123456789101112131415161718192021222324252627282930313233343536373839/** * 堆排序 */public class HeapSort extends Sort &#123; public void heapSort(int[] array)&#123; if(array.length&gt;1)&#123; for(int i=array.length/2;i&gt;=0;i--)&#123; heapAdjust(array,i,array.length); &#125; for(int i=array.length-1;i&gt;=1;i--)&#123; swap(array,0,i); heapAdjust(array,0,i); &#125; &#125; &#125; public void heapAdjust(int[] array,int i,int length)&#123; int data=array[i]; for(int j=2*i+1;j&lt;=length-1;j=2*j+1)&#123; if(j&lt;length-1 &amp;&amp; array[j]&lt;array[j+1])&#123; j++; &#125; if(data&gt;=array[j])&#123; break; &#125; array[i]=array[j]; i=j; &#125; array[i]=data; &#125; public static void main(String[] args) &#123; int[] array=&#123;9,1,5,8,3,7,4,6,2&#125;; System.out.println(Arrays.toString(array)); HeapSort heapSort=new HeapSort(); heapSort.heapSort(array); System.out.println(Arrays.toString(array)); &#125;&#125; 归并排序包含递归和非递归实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283/** * 归并排序 */public class MergingSort &#123; /** * 递归实现归并排序 */ public void mergeSort(int[] array)&#123; if(array.length&gt;1)&#123; mSort(array,0,array.length-1); &#125; &#125; /** * 数组拆分 */ private void mSort(int[] array, int left , int right)&#123; if(left&lt;right)&#123; int mid=(left+right)/2; mSort(array,left,mid); mSort(array,mid+1,right); merge(array,left,mid,right); &#125; &#125; /** * 数组归并 */ private void merge(int[] array , int left , int mid , int right)&#123; int[] num=new int[right-left+1]; int i = left; int j = mid+1; int count=0; while(i&lt;=mid &amp;&amp; j&lt;=right)&#123; if(array[i]&gt;array[j])&#123; num[count++]=array[j++]; &#125;else&#123; num[count++]=array[i++]; &#125; &#125; while(i&lt;=mid)&#123; num[count++]=array[i++]; &#125; while(j&lt;=right)&#123; num[count++]=array[j++]; &#125; count=0; while(left&lt;=right)&#123; array[left++]=num[count++]; &#125; &#125; /** * 非递归实现归并排序 */ public void mergeSort2(int[] array)&#123; if(array.length&gt;1)&#123; int length=1; while(length&lt;array.length)&#123; for(int i=0;i&lt;array.length;i+=2*length)&#123; if(i+2*length&lt;array.length)&#123; merge(array,i,i+length-1,i+2*length-1); &#125;else if(i+length&lt;array.length)&#123; merge(array,i,i+length-1,array.length-1); &#125;else&#123; merge(array,i,array.length-1,array.length-1); &#125; &#125; length *=2; &#125; &#125; &#125; public static void main(String[] args) &#123; int[] array=&#123;9,1,5,8,3,7,4,6,2&#125;; System.out.println(Arrays.toString(array)); MergingSort mergingSort=new MergingSort(); mergingSort.mergeSort(array); System.out.println(Arrays.toString(array)); int[] array2=&#123;9,1,5,8,3,7,4,6,2&#125;; System.out.println(Arrays.toString(array2)); mergingSort.mergeSort2(array2); System.out.println(Arrays.toString(array2)); &#125;&#125; 基数排序基于两种不同的排序顺序，我们将基数排序分为LSD（Least significant digital）或MSD（Most significant digital），LSD的排序方式由数值的最右边（低位）开始，而MSD则相反，由数值的最左边（高位）开始。注意一点：LSD的基数排序适用于位数少的数列，如果位数多的话，使用MSD的效率会比较好。 http://www.cnblogs.com/Braveliu/archive/2013/01/21/2870201.html 代码实现（包含LSD和MSD两种实现方式） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495/** * 基数排序 */public class RadixSort &#123; /** * LSD实现基数排序 */ public void radixSortLSD(int [] array,int radix,int digit)&#123; if(array.length&gt;1)&#123; int num[]=new int[array.length]; int buckets[]=new int[radix];//定义桶的数量 int rate=1; for(int i=0;i&lt;=digit;i++)&#123; Arrays.fill(buckets,0); //将数组元素复制到临时数组num中 System.arraycopy(array,0,num,0,array.length); /** * 计算各个桶的容量 */ for(int j=0;j&lt;array.length;j++)&#123; int subKey=(array[j]/rate)%radix; buckets[subKey]++; &#125; /** * 计算桶内元素在数组中应该排序的位置 */ for(int j=1;j&lt;radix;j++)&#123; buckets[j]=buckets[j]+buckets[j-1]; &#125; /** * 对数组元素按照余数进行排序 */ for(int k=array.length-1;k&gt;=0;k--)&#123; int subKey =(num[k]/rate)%radix; array[--buckets[subKey]]=num[k]; &#125; rate*=radix; &#125; &#125; &#125; /** * MSD实现基数排序 */ public void radixSortMSD(int [] array,int left,int right,int radix,int digit)&#123; if(right&gt;left)&#123; int num[]=new int[right-left+1]; int buckets[]=new int[radix];//定义桶的数量 int rate=(int)Math.pow(radix,digit-1); Arrays.fill(buckets,0); //将数组元素复制到临时数组num中 System.arraycopy(array,left,num,0,right-left+1); /** * 计算各个桶的容量 */ for(int j=left;j&lt;=right;j++)&#123; int subKey=(array[j]/rate)%radix; buckets[subKey]++; &#125; /** * 计算桶内元素在数组中应该排序的位置 */ for(int j=1;j&lt;radix;j++)&#123; buckets[j]=buckets[j]+buckets[j-1]; &#125; /** * 对数组元素按照商的余数进行排序 */ for(int k=num.length-1;k&gt;=0;k--)&#123; int subKey =(num[k]/rate)%radix; array[left+(--buckets[subKey])]=num[k]; &#125; if(digit&gt;0)&#123; int subKey; radixSortMSD(array,left,left+buckets[0]-1,radix,digit-1); for(subKey=0;subKey&lt;radix-1;subKey++)&#123; radixSortMSD(array,left+buckets[subKey],left+buckets[subKey+1]-1,radix,digit-1); &#125; radixSortMSD(array,left+buckets[subKey],right,radix,digit-1); &#125; &#125; &#125; public static void main(String[] args) &#123; int[] array=&#123;20, 80, 90, 589, 998, 965, 852, 123, 456, 789&#125;; System.out.println(Arrays.toString(array)); RadixSort radixSort=new RadixSort(); radixSort.radixSortLSD(array,10,3); System.out.println(Arrays.toString(array)); int[] array2=&#123;200, 1, 3, 42, 9, 64, 7, 81, 5, 10, 52, 61&#125;; System.out.println(Arrays.toString(array2)); radixSort.radixSortMSD(array2,0,array2.length-1,10,3); System.out.println(Arrays.toString(array2)); &#125;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大话数据结构——查找]]></title>
    <url>%2F2018%2F10%2F01%2F%E5%A4%A7%E8%AF%9D%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E2%80%94%E2%80%94%E6%9F%A5%E6%89%BE%2F</url>
    <content type="text"><![CDATA[本文在阅读大话数据结构这本书的基础上，结合java语言的特点，来理解查找。 查找（Searching）是根据给定的某个值，在查找表中确定一个其关键字等于给定值的数据元素（或记录）。 顺序表查找 顺序查找又叫线性查找，是最基本的查找技术，它的查找过程是：从表中第一个（或最后一个）记录开始，逐个记性记录的关键字和给定值比较，若某个记录的关键字和给定值相等，则查找成功，找到所查的记录；如果直到最后一个（或第一个）记录，其关键字和给定值比较都不等时，则表中没有所查的记录，查找不成功。 123456789101112131415161718192021/** * 顺序查找 */public class SequentialSearch &#123; public int sequentialSearch(int[] array, int key)&#123; if(array.length&gt;0)&#123; for(int i=0;i&lt;array.length;i++)&#123; if(array[i]==key) return i+1; &#125; &#125; return -1; &#125; public static void main(String[] args) &#123; int[] array=new int[]&#123;1,3,5,4,6,8,7&#125;; SequentialSearch sequentialSearch=new SequentialSearch(); int n=sequentialSearch.sequentialSearch(array,8); System.out.println("数8在数组中的第"+n+"个位置"); &#125;&#125; 有序表查找折半查找 折半查找又称二分查找。它的前提是线性表中的记录必须是关键码有序（通常是从小到大有序），线性表必须采用顺序存储。​ 折半查找的基本思想是：在有序表中，取中间记录作为比较对象，若给定值与中间记录的关键字相等，则查找成功；若给定值小于中间记录的关键字，则在中间记录的左半区继续查找；若给定值大于中间记录的关键字，则在中间记录的右半区继续查找。不断重复上述过程，直到查找成功，或所有查找区域无记录，查找失败为止。 时间复杂度为O(logn） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * 折半查找||二分查找 */public class BinarySearch &#123; /** * 非递归实现 */ public int binarySearch(int[] array, int key)&#123; if(array.length&gt;0)&#123; int low=0; int high=array.length-1; int mid; while(low&lt;=high)&#123; mid=(low+high)/2; if(key&lt;array[mid])&#123; high=mid-1; //最高下标调整到中位下标小1位 &#125;else if(key&gt;array[mid])&#123; low=mid+1; //最低下标调整到中位下标大1位 &#125; else&#123; return mid+1; &#125; &#125; &#125; return -1; &#125; /** * 递归实现 */ public int binarySearch2(int[] array,int key,int low,int high)&#123; if(low&lt;=high)&#123; int mid=(low+high)/2; if(key&lt;array[mid])&#123; return binarySearch2(array,key,low,mid-1); &#125;else if(key&gt;array[mid])&#123; return binarySearch2(array,key,mid+1,high); &#125;else&#123; return mid+1; &#125; &#125; return -1; &#125; public static void main(String[] args) &#123; int[] array=new int[]&#123;1,3,5,9,11,15&#125;; BinarySearch binarySearch=new BinarySearch(); System.out.println("---------非递归实现二分查找----------"); int n=binarySearch.binarySearch(array,11); System.out.println("数11在数组中的第"+n+"个位置"); System.out.println("---------递归实现二分查找----------"); int m=binarySearch.binarySearch2(array,11,0,array.length-1); System.out.println("数11在数组中的第"+m+"个位置"); &#125;&#125; 插值查找 算法思想：插值查找的关键是根据要查找的关键字key与查找表中最大最小记录的关键字比较后的查找方法，其核心就在于插值的计算公式mid=low+(key-a[low])*(high-low)/(a[high]-a[low])。 时间复杂度：从时间复杂度上看，它也是O(logn) 优缺点：对于表长比较大，而关键字分布又比较均匀的查找表来说，插值查找算法的平均性能比这般查找要好很多。 斐波那契查找算法思想：依然是对查找点的优化，采用Fibonacci数组，找到对于当前长度的有序表的黄金分割点，作为每次的中间值。 时间复杂度：时间复杂度和其他两种有序表查找相同，都是O(logn) 优缺点：对于平均性能，斐波那契查找要优于折半查找，但如果是最坏情况，查找效率低于折半查找。 小结：有序表查找是一种针对查找优化的表结构，查找的时间复杂度是O(logn)。但有序表的插入和删除性能是比较差的，插入和删除不能破坏有序表的特性。 线性索引查找 索引是把一个关键字与它对应的记录相关联的过程。一个索引由若干个索引构成，每个索引项至少应包含关键字和其对应的记录在存储器中的位置等信息。​ 线性索引是将索引项集合组织为线性结构，称为索引表。 以下重点介绍三种线性索引：稠密索引、分块索引、倒排索引。 稠密索引稠密索引是指在线性索引中，将数据集中的每个记录对应一个索引项。 算法思想：稠密索引要应对的可能是成千上万的数据，因此对于稠密索引这个索引表来说，索引项一定是按照关键码有序的排列。因此可以对索引使用折半、插值、斐波那契等有序表查找算法，大大提高了效率。 时间复杂度：因为对于索引的查找使用的也是有序表的查找算法，时间复杂度是O(logn)。 优缺点：和有序表类似的是，稠密索引必须要维护索引的有序性。另外如果数据量很大，也要同时维护一个同样规模的索引，可能就需要反复访问磁盘，降低了查找性能。 分块索引算法思想：如果对索引进行一次分级呢？对于一级索引下，可能会有多个记录，称之为一个块，块内的记录再获得一个二级的索引。这些块有一个条件，就是块内无序，块间有序。块内无序就是在一级索引内部的记录可以是无序的，只要落在索引的范围内就可以；块间有序就是下一个块所有的关键字都要大于上一个块的最大关键字。因此对于一个块结构来讲，它具有最大关键码，块中的记录个数和指向块首数据的指针。 时间复杂度：分块索引在查找时，先找到查找记录所在的块，在查找在块内的为孩子。设n个记录，平均分成m个块，每个块有t个记录，这样平均查找次数就是(m+1)/2 + (t+1)/2 = (n/t + t)/2 + 1 &gt;= logn + 1。所以分块索引的时间复杂度介于O(n)和O(logn)之间。 分块索引兼顾了有序和无序的需求，平衡了插入，删除和查找的性能，普遍用于数据库查找技术等。 倒排索引算法思想：倒排索引主要应用于搜索引擎。基本思想就是将得到的key-value关系进行一个反映射，得到某个value有多少个key指向它。比如查找某个单词出现在哪些文章中，可以先访问文章中的所有单词，建立一个单词的索引，将出现该单词的文章记录到索引中。这样在搜索时直接输入单词，就能得到文章列表。 优缺点：倒排索引的优点是速度快，缺点就是记录不等长，维护比较困难，插入和删除都要做相应的处理。比如删除某个文章，就可能要对所有的单词都进行考察。 二叉排序树算法思想：有序表的问题就是如果插入一个较小的记录，就要把比它大的记录依次移动，腾出插入的位置。如果用二叉树来实现呢，只需要让这个较小的记录成为某个结点的左孩子就可以了。为什么是左孩子呢，和二叉排序数的定义有关，简单来说，二叉排序树的中序遍历就是一个有序表。这样插入任何一个记录都不需要改变已经建好的树。 查找：查找某个记录时，从根结点开始，如果查找记录大于该结点的值，就走右子树；如果小于该结点的值，就走左子树。不断向下查找，直到找到该记录，或者到叶子结点的值和查找记录不同，未找到该记录。 插入：插入和查找类似，向下找到最接近它的结点，然后把该记录作为它的左孩子或者右孩子。 删除：删除相对查找和插入来讲复杂一点，主要复杂在如果处理它的子树。下面的算法是这么处理的：首先获取要删除的节点的parent节点，如果找不到直接返回；找到parent之后，判断删除节点是parent节点的左节点还是右节点，并保存在一个临时的tmp节点；接下来要做删除操作，首先判断的是删除节点是否具有右节点：如果没有右节点的话，且删除节点是parent左节点，就直接让parent左节点指向删除节点的左节点，如果删除节点是parent右节点，同样直接让parent右节点指向删除节点的左节点；没有右节点是最好处理的情况，最复杂的是删除节点有右节点：首先将删除节点记录为father ,删除节点的右节点为tmp；如果tmp没有左节点，只有右节点最好处理了，直接移除father，将tmp补在father；如果tmp有左节点，我们不断向下直到找到最底下的左节点，将其替换删除节点即可。 时间复杂度：如果二叉排序树是平衡的，那么查找的时间复杂度是O(logn)；如果是不平衡，比如最极端的斜树，那么时间复杂度是O(n)。 优缺点：二叉排序树保留了有序表查找高效的特点，最理想的情况能达到O(logn)的时间复杂度，并且解决了插入和删除记录的问题，能够保证树的整体结构不受影响。缺点就是可能在插入的过程中，二叉排序树不能保持平衡，出现了某一边的树远远大于另一边，降低了查找的效率。后面提到的平衡二叉树解决了这个问题。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159/** * 二叉排序树 */public class BinarySearchTree &#123; BiTreeNode root; // 非递归方式插入新的节点数据 public boolean insert(int data)&#123; if(!find(data))&#123; BiTreeNode newNode=new BiTreeNode(data); if(root==null)&#123; root=newNode; &#125; else &#123; BiTreeNode current=root; BiTreeNode parent; while(true)&#123; parent=current; if(data&lt;current.data)&#123; current=current.left; if(current==null)&#123; parent.left=newNode; return true; &#125; &#125;else&#123; current=current.right; if(current==null)&#123; parent.right=newNode; return true; &#125; &#125; &#125; &#125; &#125; return false; &#125; //非递归方式实现查询 public boolean find(int data)&#123; BiTreeNode current=root; while(current!=null)&#123; if(current.data==data)&#123; return true; &#125;else if(data&lt;current.data)&#123; current = current.left; &#125;else&#123; current = current.right; &#125; &#125; return false; &#125; //删除值为data的节点 public boolean remove(int data)&#123; BiTreeNode dummy=new BiTreeNode(-1); dummy.left=root; BiTreeNode node; BiTreeNode parent=findParent(dummy,root,data); if(parent.left!=null &amp;&amp; parent.left.data==data)&#123; node=parent.left; &#125;else if(parent.right!=null &amp;&amp; parent.right.data==data)&#123; node=parent.right; &#125;else&#123; return false; &#125; delete(parent,node); return true; &#125; //查找父节点 private BiTreeNode findParent(BiTreeNode parent,BiTreeNode current,int data)&#123; if(current==null)&#123; return parent; &#125;else if(current.data==data)&#123; return parent; &#125;else if(current.data&gt;data)&#123; return findParent(current,current.left,data); &#125;else&#123; return findParent(current,current.right,data); &#125; &#125; //删除节点 private void delete(BiTreeNode parent,BiTreeNode current)&#123; if(current.left==null)&#123; if(parent.left==current)&#123; parent.left=current.right; &#125;else&#123; parent.right=current.right; &#125; &#125;else if(current.right==null)&#123; if(parent.left==current)&#123; parent.left=current.left; &#125;else&#123; parent.right=current.left; &#125; &#125;else&#123; BiTreeNode father =current; BiTreeNode node =current.right; while(node.left!=null)&#123; //转左，然后向右到尽头（找待删结点的后继） father=node; node=node.left; &#125; if(father.left==node)&#123; //将待删结点的后继接上 father.left=node.right; &#125;else&#123; father.right=node.right; &#125; if(parent.left==current)&#123; parent.left=node; &#125;else&#123; parent.right=node; &#125; node.left=current.left; node.right=current.right; if(root==current)&#123; //如果根结点，要将根节点重置 root=node; &#125; &#125; &#125; public static void main(String[] args) &#123; BinarySearchTree binarySearchTree=new BinarySearchTree(); binarySearchTree.insert(8); binarySearchTree.insert(3); System.out.println(binarySearchTree.find(8)); System.out.println(binarySearchTree.find(3)); System.out.println(binarySearchTree.root.left.data); System.out.println(binarySearchTree.insert(3)); binarySearchTree.insert(10); System.out.println(binarySearchTree.root.right.data); binarySearchTree.insert(2); binarySearchTree.insert(7); System.out.println(binarySearchTree.root.left.right.data); binarySearchTree.insert(11); binarySearchTree.insert(9); System.out.println(binarySearchTree.remove(8)); System.out.println("is find 8 ?"+binarySearchTree.find(8)); System.out.println("root is "+binarySearchTree.root.data); System.out.println(binarySearchTree.remove(3)); System.out.println("is find 3 ?"+binarySearchTree.find(3)); System.out.println("root is "+binarySearchTree.root.left.data); &#125;&#125;/** * 二叉树结点 */public class BiTreeNode &#123; int data; BiTreeNode left; BiTreeNode right; public BiTreeNode(int data)&#123; this.data=data; this.left=null; this.right=null; &#125;&#125; 平衡二叉树（AVL树） 平衡二叉树是一种二叉排序树，其中每一个节点的的左子树和右子树的高度差之多等于1。我们将二叉树上结点的左子树深度减去右子树深度的值称为平衡因子BF(Balance Factor)。 最小不平衡子树：距离插入节点最近的，且平衡因子的绝对值大于1的节点为根的子树，我们称为最小不平衡子树。 判断是否为平衡二叉树： 1234567891011121314151617181920212223242526272829303132333435public class BalancedBinaryTree extends BinarySearchTree &#123; /** * 判断结点是否平衡 */ public boolean isBalanced (BiTreeNode node)&#123; return maxDepth(node)!=-1; &#125; /** * 求结点的最大深度 * 若该结点不平衡，则深度为-1 */ private int maxDepth(BiTreeNode node)&#123; if(node==null)&#123; return 0; &#125; int left=maxDepth(node.left); int right=maxDepth(node.right); //若左子树或右子树不平衡，或其高度差大于1,则该结点不平衡 if(left==-1||right==-1||Math.abs(left-right)&gt;1)&#123; return -1; &#125; return Math.max(left,right)+1; &#125; public static void main(String[] args) &#123; BalancedBinaryTree tree=new BalancedBinaryTree(); tree.insert(8); tree.insert(3); tree.insert(10); tree.insert(12); System.out.println(tree.isBalanced(tree.root)); tree.insert(15); System.out.println(tree.isBalanced(tree.root)); &#125;&#125; 多路查找树多路查找树(mutil-way search tree)，其每一个节点的孩子树可以多于两个，且每个节点处可存储多个元素。 散列表查找散列技术是在记录的存储位置和它的关键字之间建立一个确定的对应关系f，使得每个关键字key对应一个存储位置f(key)。 散列技术最适合的求解问题是查找与给定值相等的记录。我们把上述的对应关系f称为散列函数，又称为哈希函数。采用散列技术将记录存储在一块连续的存储空间中，这块连续存储空间称为散列表或者哈希表。 散列函数的构造方法： 直接定址法：取关键字的某个线性函数，f(key)=a*key+b；优点：简单、均匀、不会产生冲突，适合事先知道关键字的分布，查找表较小且连续，不常用。​ 数字分析法：抽取，使用关键字的一部分来计算存储位置，适合事先知道关键字的分布且关键字若干位的分布较均匀，关键字位数较多。​ 平方取中法：先（关键字^2）再抽取中间位，适合不知道关键字分布，关键字位数较少。​ 除留余数法：散列表长m，f(key)=key MOD p，(p≦m)，其中可以对关键字取模，也可以在折叠、平方取中后再取模，缺点：p值取的不好，很容易有冲突、出现同义词，取的好也不容易避免冲突，最常用。​ 随机数法：f(key)=random(key)，适合关键字长度不等。 处理散列冲突的方法： 开放定址探测法：单向寻找，线性探测法。改进，di=1^2,-1^2,2^2,-2^2,…,q^2,-q^2,(q≦m/2)，双向寻找，二次探测法，伪随机数，随机种子，di=random(di)，随机探测法。​ 再散列函数法：一旦发生冲突就换一个散列函数，优点：使得关键字不会产生聚集。缺点：增加了计算时间。​ 链地址法：有冲突的关键字存储在一个单链表中，同义词子表。优点：冲突较多时，不会找不到空地址缺点：查找时可能需要遍历单链表。​ 公共溢出区法：有冲突的关键字都放在公共溢出表，散列表=基本表+溢出表。查找：先通过散列函数得到散列地址在基本表中找，如果没有，再到溢出表中顺序找。适合冲突较少的情况。散列表的查找：散列表的查找性能取决于：1）关键字的分布，2）散列函数的选择，3）处理冲突的方法，4）装填因子α装填因子=记录个数/散列表长度，α=n/m，通常将散列表的空间设置的比查找表/集合大，空间换时间。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大话数据结构——栈和队列]]></title>
    <url>%2F2018%2F09%2F17%2F%E5%A4%A7%E8%AF%9D%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E2%80%94%E2%80%94%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[本文在阅读大话数据结构这本书的基础上，结合java语言的特点，来理解栈和队列。 栈与队列的定义栈是限定仅在表尾进行插入和删除操作的线性表。（类似弹夹中的子弹）队列是只允许在一端进行插入操作、而在另一端进行删除操作的线性表。（类似等待客服电话排队） 栈栈本身也是一种线性表，除了删除和添加改名为pop(弹)和push(压)，抽象功能没有特别的地方。需要注意一点是java的sdk继承vector实现的stack，相关操作使用synchronized，具有安全性。 栈的顺序存储结构(顺序栈)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111/** * 基于数组实现的顺序栈 * @param &lt;E&gt; */public class Stack&lt;E&gt; &#123; private Object[] data = null; private Object e; private int maxSize=0; private int top=-1; /** * 构造函数：根据给定的size初始化栈 */ Stack()&#123; this(10); &#125; public Stack(int initSize)&#123; if(initSize&gt;=0)&#123; this.maxSize=initSize; data = new Object[initSize]; top=-1; &#125;else&#123; throw new RuntimeException("初始化大小不能小于0"+initSize); &#125; &#125; //判断栈是否为空 public boolean isEmpty()&#123; return top == -1 ? true : false; &#125; //元素进栈 public boolean push(E e)&#123; if (top == maxSize-1)&#123; throw new RuntimeException("栈已满，元素无法入栈！"); &#125;else&#123; data[++top]=e; return true; &#125; &#125; //查看栈顶元素 public Object peek()&#123; if(top==-1)&#123; throw new RuntimeException("栈为空！"); &#125;else&#123; return data[top]; &#125; &#125; //元素出栈 public Object pop()&#123; if(top==-1)&#123; throw new RuntimeException("栈为空！"); &#125;else&#123; Object elem=data[top]; data[top--]=null; return elem; &#125; &#125; //返回对象在堆栈中的位置，以1为基数 public int search(E e)&#123; int i=top; while(top!=-1)&#123; if(peek()!=e)&#123; top--; &#125;else&#123; break; &#125; &#125; int result = top+1; top=i; return result; &#125; //查看栈的大小 public int getSize()&#123; return top+1; &#125; //清空栈 public void clear()&#123; Arrays.fill(data,null); top=-1; &#125; public static void main(String[] args) &#123; Stack&lt;String&gt; stack=new Stack&lt;&gt;(); System.out.println("stack.maxSize="+stack.maxSize); System.out.println("stack.size="+stack.getSize()); System.out.println("stack is empty?"+stack.isEmpty()); stack.push("a"); stack.push("b"); stack.push("c"); stack.push("d"); stack.push("e"); System.out.println("stack.size="+stack.getSize()); System.out.println("stack is empty?"+stack.isEmpty()); System.out.println(stack.pop()); System.out.println(stack.peek()); System.out.println(stack.pop()); System.out.println("stack.size="+stack.getSize()); System.out.println("stack is empty?"+stack.isEmpty()); stack.clear(); System.out.println("---------------after clear---------------"); System.out.println("queue is empty?"+stack.isEmpty()); System.out.println("queue.size="+stack.getSize()); &#125;&#125; 栈的链式存储结构（链栈）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293/** * 基于链表实现的栈 * @param &lt;E&gt; */public class LinkStack&lt;E&gt; &#123; private class Node&#123; E e; Node next; public Node(E e,Node next)&#123; this.e=e; this.next=next; &#125; &#125; private Node top; private int size; public LinkStack()&#123; top=null; &#125; //当前栈大小 public int getSize()&#123; return size; &#125; //判断是否为空 public boolean isEmpty()&#123; return size==0; &#125; //入栈 public boolean push(E e)&#123; top = new Node(e,top); size++; return true; &#125; //查看栈顶元素但不删除 public E peek()&#123; if(isEmpty())&#123; throw new RuntimeException("栈为空！"); &#125;else&#123; return top.e; &#125; &#125; //出栈 public E pop()&#123; if (isEmpty()) &#123; throw new RuntimeException("栈为空！"); &#125;else&#123; Node elem=top; top =top.next; elem.next=null; size--; return elem.e; &#125; &#125; //清空栈 public void clear()&#123; for(Node node=top;node!=null;)&#123; Node next=node.next; node.e=null; node.next=null; node=next; &#125; size=0; &#125; public static void main(String[] args) &#123; LinkStack&lt;String&gt; linkStack=new LinkStack&lt;&gt;(); System.out.println("linkStack.size = "+linkStack.getSize()); System.out.println("stack is empty?"+linkStack.isEmpty()); linkStack.push("a"); linkStack.push("b"); linkStack.push("c"); linkStack.push("d"); linkStack.push("e"); System.out.println("stack.size="+linkStack.getSize()); System.out.println("stack is empty?"+linkStack.isEmpty()); System.out.println(linkStack.pop()); System.out.println(linkStack.peek()); System.out.println(linkStack.pop()); System.out.println("stack.size="+linkStack.getSize()); System.out.println("stack is empty?"+linkStack.isEmpty()); linkStack.clear(); System.out.println("------------after clear-------------"); System.out.println("stack.size="+linkStack.getSize()); System.out.println("stack is empty?"+linkStack.isEmpty()); &#125; &#125; 队列队列本身也是一种线性表，不同的是插入只能是队尾，删除只能是队首。 队列的顺序存储结构顺序队列12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394/** * 基于数组实现的队列 * @param &lt;E&gt; */public class Queue&lt;E&gt; &#123; private Object[] data=null; private int maxSize; private int front; private int rear; //构造函数 public Queue()&#123; this(10); &#125; public Queue(int initSize)&#123; if(initSize&gt;=0)&#123; this.maxSize=initSize; data=new Object[initSize]; front=rear=0; &#125;else&#123; throw new RuntimeException("初始化大小不能小于："+initSize); &#125; &#125; //判断是否为空 public boolean isEmpty()&#123; return rear==front?true:false; &#125; //插入队列 public boolean offer(E e)&#123; if(rear==maxSize)&#123; throw new RuntimeException("队列已满，无法插入新的元素！"); &#125;else&#123; data[rear++]=e; return true; &#125; &#125; //返回队首元素，但不删除 public Object peek()&#123; if(isEmpty())&#123; throw new RuntimeException("队列为空！"); &#125;else&#123; return data[front]; &#125; &#125; //出队 public Object poll()&#123; if (isEmpty())&#123; throw new RuntimeException("队列为空！"); &#125;else&#123; Object elem= data[front]; data[front++]=null; return elem; &#125; &#125; //队列长度 public int getSize()&#123; return rear-front; &#125; //清空队列 public void clear()&#123; Arrays.fill(data,null); rear=front=0; &#125; public static void main(String[] args) &#123; Queue&lt;String&gt; queue=new Queue&lt;&gt;(); System.out.println("queue is empty:"+queue.isEmpty()); System.out.println("queue.size="+queue.getSize()); queue.offer("a"); queue.offer("b"); queue.offer("c"); queue.offer("d"); queue.offer("e"); System.out.println("queue is empty:"+queue.isEmpty()); System.out.println("queue.size="+queue.getSize()); System.out.println(queue.poll()); System.out.println(queue.peek()); System.out.println(queue.poll()); System.out.println("queue is empty:"+queue.isEmpty()); System.out.println("queue.size="+queue.getSize()); queue.clear(); System.out.println("---------------after clear---------------"); System.out.println("queue is empty:"+queue.isEmpty()); System.out.println("queue.size="+queue.getSize()); &#125;&#125; 循环队列123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100/** * 基于数组实现的循环队列 * @param &lt;E&gt; */public class LoopQueue&lt;E&gt; &#123; public Object[] data=null; private int maxSize;//队列容量 private int rear;//队列尾，允许插入 private int front;//队列头，允许删除 private int size=0;//队列当前长度 //构造队列 public LoopQueue()&#123; this(10); &#125; public LoopQueue(int initSize)&#123; if(initSize&gt;=0)&#123; this.maxSize=initSize; data=new Object[initSize]; front=rear=0; &#125;else&#123; throw new RuntimeException("初始化大小不能小于0："+initSize); &#125; &#125; //判断是否为空 public boolean isEmpty()&#123; return size==0; &#125; //队列插入元素 public boolean offer(E e)&#123; if(size==maxSize)&#123; throw new RuntimeException("队列已满，无法插入新的元素！"); &#125;else&#123; data[rear]=e; rear=(rear+1)%maxSize;//队尾指针+1 size++; return true; &#125; &#125; //返回队首元素，但并不删除 public Object peek()&#123; if(isEmpty())&#123; throw new RuntimeException("队列为空！"); &#125;else&#123; return data[front]; &#125; &#125; //出队 public Object poll()&#123; if(isEmpty())&#123; throw new RuntimeException("队列为空！"); &#125;else&#123; Object elem=data[front]; data[front]=null; front=(front+1)%maxSize;//队首指针+1 size--; return elem; &#125; &#125; //获取队列长度 public int getSize()&#123; return size; &#125; //清空队列 public void clear()&#123; Arrays.fill(data,null); size=0; rear=front=0; &#125; public static void main(String[] args) &#123; LoopQueue&lt;String&gt; loopQueue=new LoopQueue&lt;&gt;(); System.out.println("queue is empty:"+loopQueue.isEmpty()); System.out.println("queue.size="+loopQueue.getSize()); loopQueue.offer("a"); loopQueue.offer("b"); loopQueue.offer("c"); loopQueue.offer("d"); loopQueue.offer("e"); System.out.println("queue is empty:"+loopQueue.isEmpty()); System.out.println("queue.size="+loopQueue.getSize()); System.out.println(loopQueue.poll()); System.out.println(loopQueue.peek()); System.out.println(loopQueue.poll()); System.out.println("queue is empty:"+loopQueue.isEmpty()); System.out.println("queue.size="+loopQueue.getSize()); loopQueue.clear(); System.out.println("---------------after clear---------------"); System.out.println("queue is empty:"+loopQueue.isEmpty()); System.out.println("queue.size="+loopQueue.getSize()); &#125;&#125; 队列的链式存储结构123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899public class LinkQueue&lt;E&gt; &#123; //链式队列的节点 private class Node&#123; E e; Node next; public Node()&#123; &#125; public Node(E e,Node next)&#123; this.e=e; this.next=next; &#125; &#125; private Node front;//队列头，允许插入 private Node rear;//队列尾，允许插入 private int size;//队列当前长度 //构造队列 public LinkQueue()&#123; Node node=new Node(null,null); front=node; rear=node; size=0; &#125; //判断是否为空 public boolean isEmpty()&#123; return size==0; &#125; //队列插入元素 public boolean offer(E e)&#123; Node elem=new Node(e,null); rear.next=elem; rear=elem; size++; return true; &#125; //返回队首元素，但并不删除 public E peek()&#123; if(isEmpty())&#123; throw new RuntimeException("队列为空！"); &#125;else&#123; return front.next.e; &#125; &#125; //出队 public E poll()&#123; if(isEmpty())&#123; throw new RuntimeException("队列为空！"); &#125;else&#123; Node elem=front.next; front.next=elem.next; elem.next=null; size--; return elem.e; &#125; &#125; //获取队列长度 public int getSize()&#123; return size; &#125; //清空队列 public void clear()&#123; for(Node node=front;node!=null;)&#123; Node next=node.next; node.e=null; node.next=null; node=next; &#125; size=0; &#125; public static void main(String[] args) &#123; LinkQueue&lt;String&gt; linkQueue=new LinkQueue&lt;&gt;(); System.out.println("queue is empty:"+linkQueue.isEmpty()); System.out.println("queue.size="+linkQueue.getSize()); linkQueue.offer("a"); linkQueue.offer("b"); linkQueue.offer("c"); linkQueue.offer("d"); linkQueue.offer("e"); System.out.println("queue is empty:"+linkQueue.isEmpty()); System.out.println("queue.size="+linkQueue.getSize()); System.out.println(linkQueue.poll()); System.out.println(linkQueue.peek()); System.out.println(linkQueue.poll()); System.out.println("queue is empty:"+linkQueue.isEmpty()); System.out.println("queue.size="+linkQueue.getSize()); linkQueue.clear(); System.out.println("---------------after clear---------------"); System.out.println("queue is empty:"+linkQueue.isEmpty()); System.out.println("queue.size="+linkQueue.getSize()); &#125;&#125;]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大话数据结构——串]]></title>
    <url>%2F2018%2F09%2F15%2F%E5%A4%A7%E8%AF%9D%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E2%80%94%E2%80%94%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[本文在阅读大话数据结构这本书的基础上，结合java语言的特点，来理解串。 串串（string）是由零个或多个字符组成的有限序列，又名字符串。 串的存储结构 串的顺序存储结构是用一组地址连续的存储单元来存储串中的字符序列的。按照预定义的大小，为每个定义的串变量分配一个固定长度的存储区。 用“\0”来表示串的终结，不计入串长度，但是计入数组长度。 两个长度不同的串不可能相等。 串的链式存储结构要考虑一个结点是存放一个字符（会造成很大的空间浪费）还是多个字符。除了链接串与串的操作有一定方便外，总的来说不如顺序存储量或，性能也不如顺序存储结构好。 朴素的模式匹配算法串的模式匹配：串的定位操作。时间复杂度：O(1)–最好；O(n+m)–平均；O(n-m+1)*m–最不好 1234567891011121314151617181920212223242526272829303132333435/** * 朴素的模式匹配算法 */public class Match &#123; public int match(String s , String t , int post)&#123; if(s == null || t == null)&#123; return -1; &#125; int len1 = s.length(); int len2 = t.length(); int i = post; int j = 0; while (i &lt; len1 &amp;&amp; j &lt; len2) &#123; if (s.charAt(i) == t.charAt(j)) &#123; i++; j++; &#125;else&#123; i = i - j + 1; j = 0; &#125; &#125; if (j &gt;= len2)&#123; return i - len2; &#125;else&#123; return -1; &#125; &#125; public static void main(String[] args) &#123; String s ="Star is good man"; String t =" good"; Match matching = new Match(); System.out.println(matching.match(s, t, 0)); System.out.println(matching.match(s, t, 8)); &#125;&#125; KMP模式匹配算法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * KMP模式匹配算法 */public class MatchKMP &#123; public int matchKMP(String s ,String t ,int post)&#123; if(s == null || t == null)&#123; return -1; &#125; int len1 = s.length(); int len2 = t.length(); int i = post; int j = 0; int[] next = getNext(t); while (i &lt; len1 &amp;&amp; j &lt; len2) &#123; if (j== -1 || s.charAt(i) == t.charAt(j)) &#123; i++; j++; &#125;else&#123; j = next[j]; &#125; &#125; if (j &gt;= len2)&#123; return i - len2; &#125;else&#123; return -1; &#125; &#125; private int[] getNext(String t)&#123; int [] next = new int [t.length()]; int i = 0; int j = -1; next[i] = j; while (i &lt; t.length() - 1)&#123; if (j == -1 || t.charAt(i) == t.charAt(j))&#123; i++; j++; next[i] =j; &#125;else&#123; j = next[j]; &#125; &#125; return next; &#125; public static void main(String[] args) &#123; String s ="Star is good man"; String t =" good"; MatchKMP matching = new MatchKMP(); System.out.println(matching.matchKMP(s, t, 0)); System.out.println(matching.matchKMP(s, t, 8)); &#125;&#125; 改进的KMP模式匹配算法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * 改进的KMP模式匹配算法 */public class MatchImproveKMP &#123; public int matchImproveKMP(String s ,String t ,int post)&#123; if(s == null || t == null)&#123; return -1; &#125; int len1 = s.length(); int len2 = t.length(); int i = post; int j = 0; int[] next = getNext(t); while (i &lt; len1 &amp;&amp; j &lt; len2) &#123; if (j== -1 || s.charAt(i) == t.charAt(j)) &#123; i++; j++; &#125;else&#123; j = next[j]; &#125; &#125; if (j &gt;= len2)&#123; return i - len2; &#125;else&#123; return -1; &#125; &#125; private int[] getNext(String t)&#123; int [] next = new int [t.length()]; int i = 0; int j = -1; next[i] = j; while (i &lt; t.length() - 1)&#123; if (j == -1 || t.charAt(i) == t.charAt(j))&#123; i++; j++; if(t.charAt(i) == t.charAt(j))&#123; next[i] = next[j]; &#125;else&#123; next[i] = j; &#125; &#125;else&#123; j = next[j]; &#125; &#125; return next; &#125; public static void main(String[] args) &#123; String s ="Star is good man"; String t =" good"; MatchImproveKMP matching = new MatchImproveKMP(); System.out.println(matching.matchImproveKMP(s, t, 0)); System.out.println(matching.matchImproveKMP(s, t, 8)); &#125;&#125;]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大话数据结构——树]]></title>
    <url>%2F2018%2F09%2F15%2F%E5%A4%A7%E8%AF%9D%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E2%80%94%E2%80%94%E6%A0%91%2F</url>
    <content type="text"><![CDATA[本文在阅读大话数据结构这本书的基础上，结合java语言的特点，来理解树。 树树（Tree）是n（n&gt;=0）个结点的有限集。 n=0又称为空树。在任意一课非空的树中：有且仅有一个特定的称为跟（Root）的结点；当n&gt;1时，其余结点可分为m（m&gt;0）个互不相交的有限集，其中每一个集合本身又是一棵树，并且称为根的子树（SubTree）。 树是一种一对多的数据结构。 需要注意的是：当n&gt;0时根结点是惟一的，不可能存在多个根结点。 m&gt;0时，子树的个数没有限制，但它们一定是互不相交的。如果相交，就不符合树的定义。 结点拥有的子树称为结点的度。度为0的结点称为叶结点或终端结点；度不为0的结点称为非终端结点或分支结点。除根结点外，分支结点也称为内部结点。树的度是树内各结点的度的最大值。 树中结点最大的层次称为树的深度或高度。 树的存储结构树有两种实现方式：数组和链表； 树有三种不同表示法：双亲表示法、孩子表示法、孩子兄弟表示法。 二叉树二叉树（Binary Tree）是n（n&gt;=0）个结点的有限集合，该集合或者为空集（称为空二叉树），或者由一个根结点和两棵互不相交的、分别称为根结点的左子树和右子树的二叉树组成。 二叉树的特点每个结点最多有两棵子树，所以二叉树中不存在大于2的结点。注意不是只有两棵子树，而是最多有。没有子树或者有一棵子树都是可以的。左子树和右子树是有顺序的，次序不能颠倒。就像人的左右手。即使树中某结点只有一棵子树，也要区分它是左子树还是右子树。 二叉树有5种基本形态 空二叉树。 只有一个根结点。 根结点只有左子树。 根结点只有右子树。 根结点既有左子树又有右子树。 特殊二叉树斜树：只有左子树或者只有右子树。线性表是一种特殊的树。 满二叉树：所有分支结点都存在左子树和右子树，并且所有叶子都在同一层上。 满二叉树的特点有： 叶子只能在最下一层。出现在其他层就不可能达到平衡。 非叶子结点的度一定是2。否则就是“缺胳膊少腿了”。 在同样深度的二叉树中，满二叉树的结点个数越多，叶子树越多。 完全二叉树叶子结点只能在最下两层。 最下层的叶子一定集中在左部连续的位置。 倒数第二层，若有叶子结点，一定都在右部连续位置。 如果结点度为1，则该结点只有左孩子，即不存在右子树的情况。 同样结点数从二叉树，完全二叉树的深度最小。 判断一棵树是否是完全二叉树，心中默默给每个结点按照满二叉树的结构逐层顺序编号，如果编号出现空档，就不是完全二叉树，否则就是。 满二叉树是特殊的完全二叉树。 完全二叉树必须先满足左后满足右，缺的元素只能是满二叉树最下一层的，高度差小于或等于1。 二叉树的性质12345678性质1：在二叉树的第i层上至多有2^(i-1)个结点（i≥1）。性质2：深度为k的二叉树至多有2^k-1个结点（k≥1）。性质3：对任何一棵二叉树T，如果其终端结点数为n0，度为2的结点数为n2，则n0=n2+1。性质4：具有n个结点的完全二叉树的深度为|log2n+1|（|x|表示不大于x的最大整数）。性质5：如果对一棵有n个结点的完全二叉树（其深度为）的结点按层序编号（从第1层到第层，每层从左到右），对任一结点i（1≤i≤n）有：1．如果i=1，则结点i是二叉树的根，无双亲；如果i&gt;1，则其双亲是结点。2．如果2i&gt;n，则结点i无左孩子（结点i为叶子结点）；否则其左孩子是结点2i。3．如果2i+1&gt;n，则结点i无右孩子；否则其右孩子是结点2i+1。 二叉树的存储结构二叉树有两种存储结构：顺序存储结构和二叉链表。 顺序存储结构：一般只有完全二叉树才考虑顺序存储结构，因为完全二叉树的严格性，可以充分利用顺序存储空间。其他二叉树都会造成空间的浪费，特别是右斜树。 二叉链表： 12345678910public class TreeNode&lt;E&gt; &#123; private E val; private TreeNode&lt;E&gt; left; private TreeNode&lt;E&gt; right; public TreeNode(E val)&#123; this.val=val; this.left=null; this.right=null; &#125;&#125; 遍历二叉树定义树本文以int类型为例，其他值类似 12345678910public class BinaryTree &#123; private int val; private BinaryTree left; private BinaryTree right; public BinaryTree(int val)&#123; this.val=val; this.left=null; this.right=null; &#125;&#125; 前序遍历实现一：递归方式 1234567891011121314151617/** * 前序遍历二叉树 * 方式一：递归 */public ArrayList&lt;Integer&gt; preOrderTraversal(BinaryTree root)&#123; ArrayList&lt;Integer&gt; preOrder=new ArrayList&lt;&gt;(); preTraverse(root,preOrder); return preOrder;&#125;private void preTraverse(BinaryTree root, List&lt;Integer&gt; preOrder)&#123; if(root==null)&#123; return; &#125; preOrder.add(root.val); preTraverse(root.left,preOrder); preTraverse(root.right,preOrder);&#125; 实现二：分治思想 12345678910111213141516/** * 前序遍历二叉树 * 方式二：分治思想 */public ArrayList&lt;Integer&gt; preOrderTraversal2(BinaryTree root)&#123; ArrayList&lt;Integer&gt; preOrder = new ArrayList&lt;&gt;(); if (root==null)&#123; return preOrder; &#125; ArrayList&lt;Integer&gt; left=preOrderTraversal(root.left); ArrayList&lt;Integer&gt; right=preOrderTraversal(root.right); preOrder.add(root.val); preOrder.addAll(left); preOrder.addAll(right); return preOrder;&#125; 中序遍历实现一：递归方式 1234567891011121314151617/** * 中序遍历二叉树 * 方式一：递归 */public ArrayList&lt;Integer&gt; inOrderTraversal(BinaryTree root)&#123; ArrayList&lt;Integer&gt; inOrder=new ArrayList&lt;&gt;(); inOrderTraverse(root,inOrder); return inOrder;&#125;private void inOrderTraverse(BinaryTree root,List&lt;Integer&gt; inOrder)&#123; if(root==null)&#123; return; &#125; inOrderTraverse(root.left,inOrder); inOrder.add(root.val); inOrderTraverse(root.right,inOrder);&#125; 实现二：分治思想 12345678910111213141516/** * 中序遍历二叉树 * 方式二：分治思想 */public ArrayList&lt;Integer&gt; inOrderTraverse2(BinaryTree root)&#123; ArrayList&lt;Integer&gt; inOrder=new ArrayList&lt;&gt;(); if (root==null)&#123; return inOrder; &#125; ArrayList&lt;Integer&gt; left=inOrderTraversal(root.left); ArrayList&lt;Integer&gt; right=inOrderTraversal(root.right); inOrder.addAll(left); inOrder.add(root.val); inOrder.addAll(right); return inOrder;&#125; 后序遍历实现一：递归方式 1234567891011121314151617/** * 后序遍历二叉树 * 方式一：递归 */public ArrayList&lt;Integer&gt; postOrderTraverse(BinaryTree root)&#123; ArrayList&lt;Integer&gt; postOrder=new ArrayList&lt;&gt;(); postOrderTraverse(root,postOrder); return postOrder;&#125;private void postOrderTraverse(BinaryTree root,List&lt;Integer&gt; postOrder)&#123; if(root==null)&#123; return; &#125; postOrderTraverse(root.left); postOrderTraverse(root.right); postOrder.add(root.val);&#125; 实现二：分治思想 12345678910111213141516/** * 后序遍历二叉树 * 方式二：分治思想 */public ArrayList&lt;Integer&gt; postOrderTraverse2(BinaryTree root)&#123; ArrayList&lt;Integer&gt; postOrder=new ArrayList&lt;&gt;(); if(root==null)&#123; return postOrder; &#125; ArrayList&lt;Integer&gt; left=postOrderTraverse(root.left); ArrayList&lt;Integer&gt; right=postOrderTraverse(root.right); postOrder.addAll(left); postOrder.addAll(right); postOrder.add(root.val); return postOrder;&#125; 层序遍历123456789101112131415161718192021222324/** * 层序遍历 * 使用队列思想 */public ArrayList&lt;Integer&gt; levelOrderTraversal(BinaryTree root)&#123; ArrayList&lt;Integer&gt; result =new ArrayList&lt;&gt;(); Queue&lt;BinaryTree&gt; queue = new LinkedList&lt;&gt;(); if(root!=null)&#123; result.add(root.val); queue.offer(root); while(queue.size()&gt;0)&#123; BinaryTree node=queue.poll(); if(node.left!=null)&#123; result.add(node.left.val); queue.offer(node.left); &#125; if(node.right!=null)&#123; result.add(node.right.val); queue.offer(node.right); &#125; &#125; &#125; return result;&#125; 已知前序遍历序列和中序遍历序列，可以唯一确定一棵二叉树。 已知后序遍历序列和中序遍历序列，可以唯一确定一棵二叉树。 已知前序遍历序列和后序遍历序列，不可以唯一确定一棵二叉树。 线索二叉树 为了充分利用二叉链表的空指针，把空指针指向前驱和后继，这种指向前驱和后继的指针称为线索，加上线索的二叉链表称为线索链表，相应的二叉树就称为线索二叉树。 对二叉树以某种次序遍历使其变为线索二叉树的过程称作是线索化。线索化的过程就是在遍历的过程中修改空指针的过程。 为了判别某一结点的lchild是指向左孩子还是前驱，rchild是指向右孩子还是后继，引入ltag和rtag两个标志域。 1lchild ltag data rtag rchild 其中，ltag为0时指向该结点的左孩子，为1时指向该结点的前驱；rtag为0时指向该结点的右孩子，为1时指向该结点的后继。 赫夫曼树赫夫曼树：带全路径长度WPL最小的二叉树。先取最小权值的结点作为叶子结点，逐级递增就能构造出哈夫曼树。把左结点标为0，右结点标为1，就能构造出赫夫曼编码。]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一致性哈希算法]]></title>
    <url>%2F2018%2F09%2F13%2F%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一致性Hash算法背景 一致性哈希算法在1997年由麻省理工学院的Karger等人在解决分布式Cache中提出的，设计目标是为了解决因特网中的热点(Hot spot)问题，初衷和CARP十分类似。一致性哈希修正了CARP使用的简单哈希算法带来的问题，使得DHT可以在P2P环境中真正得到应用。 但现在一致性hash算法在分布式系统中也得到了广泛应用，研究过memcached缓存数据库的人都知道，memcached服务器端本身不提供分布式cache的一致性，而是由客户端来提供，具体在计算一致性hash时采用如下步骤： 首先求出memcached服务器（节点）的哈希值，并将其配置到0～2^32的圆（continuum）上。 然后采用同样的方法求出存储数据的键的哈希值，并映射到相同的圆上。 然后从数据映射到的位置开始顺时针查找，将数据保存到找到的第一个服务器上。如果超过232仍然找不到服务器，就会保存到第一台memcached服务器上。 从上图的状态中添加一台memcached服务器。余数分布式算法由于保存键的服务器会发生巨大变化而影响缓存的命中率，但Consistent Hashing中，只有在园（continuum）上增加服务器的地点逆时针方向的第一台服务器上的键会受到影响，如下图所示： 一致性Hash性质 考虑到分布式系统每个节点都有可能失效，并且新的节点很可能动态的增加进来，如何保证当系统的节点数目发生变化时仍然能够对外提供良好的服务，这是值得考虑的，尤其实在设计分布式缓存系统时，如果某台服务器失效，对于整个系统来说如果不采用合适的算法来保证一致性，那么缓存于系统中的所有数据都可能会失效（即由于系统节点数目变少，客户端在请求某一对象时需要重新计算其hash值（通常与系统中的节点数目有关），由于hash值已经改变，所以很可能找不到保存该对象的服务器节点），因此一致性hash就显得至关重要，良好的分布式cahce系统中的一致性hash算法应该满足以下几个方面： 平衡性(Balance)平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。 单调性(Monotonicity)单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲区加入到系统中，那么哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲区中去，而不会被映射到旧的缓冲集合中的其他缓冲区。简单的哈希算法往往不能满足单调性的要求，如最简单的线性哈希：x = (ax + b) mod (P)，在上式中，P表示全部缓冲的大小。不难看出，当缓冲大小发生变化时(从P1到P2)，原来所有的哈希结果均会发生变化，从而不满足单调性的要求。哈希结果的变化意味着当缓冲空间发生变化时，所有的映射关系需要在系统内全部更新。而在P2P系统内，缓冲的变化等价于Peer加入或退出系统，这一情况在P2P系统中会频繁发生，因此会带来极大计算和传输负荷。单调性就是要求哈希算法能够应对这种情况。 分散性(Spread)在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。 负载(Load)负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。 平滑性(Smoothness)平滑性是指缓存服务器的数目平滑改变和缓存对象的平滑改变是一致的。 原理基本概念 一致性哈希算法（Consistent Hashing）最早在论文《Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web》中被提出。简单来说，一致性哈希将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0-2^32-1（即哈希值是一个32位无符号整形），整个哈希空间环如下： 整个空间按顺时针方向组织。0和2^32-1在零点中方向重合。 下一步将各个服务器使用Hash进行一个哈希，具体可以选择服务器的ip或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中四台服务器使用ip地址哈希后在环空间的位置如下： 接下来使用如下算法定位数据访问到相应服务器：将数据key使用相同的函数Hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器。 例如我们有Object A、Object B、Object C、Object D四个数据对象，经过哈希计算后，在环空间上的位置如下： 根据一致性哈希算法，数据A会被定为到Node A上，B被定为到Node B上，C被定为到Node C上，D被定为到Node D上。 下面分析一致性哈希算法的容错性和可扩展性。现假设Node C不幸宕机，可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。一般的，在一致性哈希算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。 下面考虑另外一种情况，如果在系统中增加一台服务器Node X，如下图所示： 此时对象Object A、B、D不受影响，只有对象C需要重定位到新的Node X 。一般的，在一致性哈希算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响。 综上所述，一致性哈希算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。 另外，一致性哈希算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜问题。例如系统中只有两台服务器，其环分布如下， 此时必然造成大量数据集中到Node A上，而只有极少量会定位到Node B上。为了解决这种数据倾斜问题，一致性哈希算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器ip或主机名的后面增加编号来实现。例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点： 同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到“Node A#1”、“Node A#2”、“Node A#3”三个虚拟节点的数据均定位到Node A上。这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为32甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。 参考：http://www.zsythink.net/archives/1182 https://www.cnblogs.com/lpfuture/p/5796398.html]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>一致性</tag>
        <tag>数据库</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机的类加载机制]]></title>
    <url>%2F2018%2F09%2F03%2FJava%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[Java程序的运行时需要JVM支持的，而在JVM的运行过程中，需要把所有的Java类加载到虚拟机中才能运行。本文介绍了JVM的类加载机制。 类的生命周期与加载时机类的生命周期一个类从被加载到虚拟机内存中开始，到被卸载出内存为止，整个生命周期包括了 加载、验证、准备、解析、初始化、使用和卸载7个阶段。其中 验证、准备、解析 3部分统称为链接，如下图： 整个顺序并不是完全固定的，其中解析阶段可以在初始化之后再开始，这样便可以实现Java的运行时绑定（动态绑定）机制。 类的加载时机 JVM虚拟机规范并没有对类的加载时机做出严格的要求，只规定了以下五种情况需要立刻触发类的初始化： 其余条件下，可以由JVM虚拟机自行决定何时去加载一个类。 遇到new,getstatic,putstatic和invokestatic这四个字节码指令时，如果类没有进行过初始化，则需要先触发其初始化。 使用反射机制对类进行调用的时候，如果类没有进行过初始化，则需要先触发其初始化。 当初始化一个类时，如果其父类还没有进行过初始化，则需要先触发其父类的初始化。 虚拟机启动时，用户需要指定一个要执行的主类（包含main方法），此时会先初始化这个类 使用JDK1.7的动态语言支持时，如果一个MethodHandle实例最后的解析结果包含REF_getStatic,REF_putStatic,REF_invokeStatic的方法句柄，且这个方法句柄对应的类没有初始化，则需要先对其进行初始化。 主动引用和被动引用上面五种条件也被称为对类的主动引用，除此之外其他引用类的方式都不会触发初始化，即类的被动引用，举个例子： 123456789101112public class Father &#123; static &#123; System.out.println("father init."); &#125; public static int val = 123;&#125;public class Son extends Father &#123; static &#123; System.out.println("son init."); &#125;&#125; 当我们访问Son.val时，会发现并没有输出son init. 对于静态字段，只有直接定义这个字段的类才会被初始化，因此通过子类来引用父类的静态字段，子类相当于是被动引用，也就不会被初始化了。 类的加载过程下面简单的介绍一下整个加载过程中，每个阶段JVM都执行了什么操作： 加载(Loading)加载过程是Java的一大特点，类的来源可以多种多样，压缩包、网络字节流、运行时动态计算生成(reflect)等等…这也造就了Java语言强大的动态特性。 通过一个类的完整限定名来获取定义此类的二进制字节流（注意，字节流的来源非常灵活） 将这个字节流所代表的静态储存结构转换成为方法区的运行时数据结构 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口 验证(Verification)这一过程主要是为了确保Class的字节流中包含的信息符合虚拟机标准，以免造成破坏 文件格式验证 元数据验证 字节码验证，通过数据流和控制流分析确定程序的语义是合法的 符号引用验证，确保解析动作能够正常执行 准备(Preparation)这一阶段将会为类变量分配内存并设置其初始值，注意此时进行内存分配的仅包括类变量(static修饰)，并且初始值通常情况下是数据类型的零值而不是设定值，如下例 1public static int val = 123; 在这一阶段变量val的赋值是0而不是123，因为此时尚未执行任何Java方法，而对val复制的putstatic指令在初始化阶段后才会执行。当然也有特殊情况，如下 1public static final int val = 123; 加上final关键字修饰后，Java编译时会为val生成ConstantValue属性，这时准备阶段就会根据设置将其值设置为123。 解析(Resolution)此阶段虚拟机将常量池内的符号替换为直接引用，主要包含以下动作： 类或接口的解析 字段解析 类方法解析 接口方法解析 初始化(Initialization)这时类加载过程的最后一步，这部分开始真正的执行Java代码，也就是说，这个阶段可以由程序员参与。 此阶段其实就是执行类构造器&lt;clinit&gt;()方法的过程。 类加载器类加载器(Class Loader)是Java虚拟机的一大创举，它将“获取类的二进制字节流”这个过程交给了开发人员自己去实现，只要编写不同的Class Loader，应用程序本身就可以用相应的方式来获取自己需要的类。 类与加载器的关系对于任意一个类，都需要由加载它的类加载器和这个类本身一同确立其在虚拟机中的唯一性。 通俗的讲，就是即便同一个Class文件，被不同的类加载器加载之后，得到也不是同一个“类”（equals方法返回false）。 双亲委派模型从虚拟机角度讲，只有两种类加载器，一种是启动类加载器(Bootstrap ClassLoader)，在hotpot上使用C++实现，属于虚拟机的一部分；另一种则是所有其他类的加载器，这些加载器是独立于虚拟机的，由Java语言实现的，从开发者角度看，可以分为以下两类： 扩展类加载器(Extension ClassLoader) 应用程序类加载器(Appliaction ClassLoader) 当然开发人员也可以自己编写类加载器，最终不同的类加载器之间的层次关系如下图所示： 这就是Java中著名的双亲委派模型，它要求除了顶级的BootStrap加载器之外，其他类加载器都必须有父类加载器，工作流程如下： 如果一个类加载器收到了类加载的请求，他首先不会自己去尝试加载这个类，而是将这个请求委派给父类加载器去完成，只有当父加载器反馈自己无法完成加载请求时，子加载器才会自己去尝试加载这个类。 这样做的好处是，Java类随着它的类加载器一起具备了一种带有优先级的层次关系。举个例子，比如java.lang.Object这个类，无论哪个类加载器加载时，最终都会委派给Bootstrap加载器去加载，这就保证了整个系统运行过程中的Object都是同一个类。 否则，如果用户自己编写了一个java.lang.Object类，并放在程序的classpath中，最终系统将会出现多个不同的Object类，整个Java体系就变得一团混乱了。]]></content>
      <categories>
        <category>Java虚拟机</category>
      </categories>
      <tags>
        <tag>类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么分布式一定要有Redis?]]></title>
    <url>%2F2018%2F09%2F01%2F%E4%B8%BA%E4%BB%80%E4%B9%88%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E5%AE%9A%E8%A6%81%E6%9C%89Redis%2F</url>
    <content type="text"><![CDATA[大部分写业务的程序员，在实际开发中使用 Redis 的时候，只会 Set Value 和 Get Value 两个操作，对 Redis 整体缺乏一个认知。 本文围绕以下几点进行阐述： 为什么使用 Redis 使用 Redis 有什么缺点 单线程的 Redis 为什么这么快 Redis 的数据类型，以及每种数据类型的使用场景 Redis 的过期策略以及内存淘汰机制 Redis 和数据库双写一致性问题 如何应对缓存穿透和缓存雪崩问题 如何解决 Redis 的并发竞争 Key 问题 为什么使用 Redis我觉得在项目中使用 Redis，主要是从两个角度去考虑：性能和并发。 当然，Redis 还具备可以做分布式锁等其他功能，但是如果只是为了分布式锁这些其他功能，完全还有其他中间件，如 ZooKeeper 等代替，并不是非要使用 Redis。因此，这个问题主要从性能和并发两个角度去答。点击这里查看Redis面试题汇总。 性能如下图所示，我们在碰到需要执行耗时特别久，且结果不频繁变动的 SQL，就特别适合将运行结果放入缓存。这样，后面的请求就去缓存中读取，使得请求能够迅速响应。 题外话：忽然想聊一下这个迅速响应的标准。根据交互效果的不同，这个响应时间没有固定标准。 不过曾经有人这么告诉我：“在理想状态下，我们的页面跳转需要在瞬间解决，对于页内操作则需要在刹那间解决。另外，超过一弹指的耗时操作要有进度提示，并且可以随时中止或取消，这样才能给用户最好的体验。” 那么瞬间、刹那、一弹指具体是多少时间呢？ 根据《摩诃僧祗律》记载： 一刹那者为一念，二十念为一瞬，二十瞬为一弹指，二十弹指为一罗预，二十罗预为一须臾，一日一夜有三十须臾。 那么，经过周密的计算，一瞬间为 0.36 秒、一刹那有 0.018 秒、一弹指长达 7.2 秒。 并发如下图所示，在大并发的情况下，所有的请求直接访问数据库，数据库会出现连接异常。 这个时候，就需要使用 Redis 做一个缓冲操作，让请求先访问到 Redis，而不是直接访问数据库。点击这里查看Redis面试题汇总。 使用 Redis 有什么缺点大家用 Redis 这么久，这个问题是必须要了解的，基本上使用 Redis 都会碰到一些问题，常见的也就几个。 回答主要是四个问题： 缓存和数据库双写一致性问题 缓存雪崩问题 缓存击穿问题 缓存的并发竞争问题 这四个问题，我个人觉得在项目中是常遇见的，具体解决方案，后文给出。 单线程的 Redis 为什么这么快这个问题是对 Redis 内部机制的一个考察。根据我的面试经验，很多人都不知道 Redis 是单线程工作模型。所以，这个问题还是应该要复习一下的。 回答主要是以下三点： 纯内存操作 单线程操作，避免了频繁的上下文切换 采用了非阻塞 I/O 多路复用机制 题外话：我们现在要仔细的说一说 I/O 多路复用机制，因为这个说法实在是太通俗了，通俗到一般人都不懂是什么意思。 打一个比方：小曲在 S 城开了一家快递店，负责同城快送服务。小曲因为资金限制，雇佣了一批快递员，然后小曲发现资金不够了，只够买一辆车送快递。 经营方式一客户每送来一份快递，小曲就让一个快递员盯着，然后快递员开车去送快递。 慢慢的小曲就发现了这种经营方式存在下述问题： 几十个快递员基本上时间都花在了抢车上了，大部分快递员都处在闲置状态，谁抢到了车，谁就能去送快递。 随着快递的增多，快递员也越来越多，小曲发现快递店里越来越挤，没办法雇佣新的快递员了。 快递员之间的协调很花时间。 综合上述缺点，小曲痛定思痛，提出了下面的经营方式。 经营方式二小曲只雇佣一个快递员。然后呢，客户送来的快递，小曲按送达地点标注好，然后依次放在一个地方。 最后，那个快递员依次的去取快递，一次拿一个，然后开着车去送快递，送好了就回来拿下一个快递。 上述两种经营方式对比，是不是明显觉得第二种，效率更高，更好呢？ 在上述比喻中： 每个快递员→每个线程 每个快递→每个 Socket(I/O 流) 快递的送达地点→Socket 的不同状态 客户送快递请求→来自客户端的请求 小曲的经营方式→服务端运行的代码 一辆车→CPU 的核数 于是我们有如下结论： 经营方式一就是传统的并发模型，每个 I/O 流(快递)都有一个新的线程(快递员)管理。 经营方式二就是 I/O 多路复用。只有单个线程(一个快递员)，通过跟踪每个 I/O 流的状态(每个快递的送达地点)，来管理多个 I/O 流。 下面类比到真实的 Redis 线程模型，如图所示： 简单来说，就是我们的 redis-client 在操作的时候，会产生具有不同事件类型的 Socket。 在服务端，有一段 I/O 多路复用程序，将其置入队列之中。然后，文件事件分派器，依次去队列中取，转发到不同的事件处理器中。 需要说明的是，这个 I/O 多路复用机制，Redis 还提供了 select、epoll、evport、kqueue 等多路复用函数库，大家可以自行去了解。 Redis 的数据类型，以及每种数据类型的使用场景是不是觉得这个问题很基础？我也这么觉得。然而根据面试经验发现，至少百分之八十的人答不上这个问题。 建议，在项目中用到后，再类比记忆，体会更深，不要硬记。基本上，一个合格的程序员，五种类型都会用到。 String这个没啥好说的，最常规的 set/get 操作，Value 可以是 String 也可以是数字。一般做一些复杂的计数功能的缓存。 Hash这里 Value 存放的是结构化的对象，比较方便的就是操作其中的某个字段。 我在做单点登录的时候，就是用这种数据结构存储用户信息，以 CookieId 作为 Key，设置 30 分钟为缓存过期时间，能很好的模拟出类似 Session 的效果。 List使用 List 的数据结构，可以做简单的消息队列的功能。另外还有一个就是，可以利用 lrange 命令，做基于 Redis 的分页功能，性能极佳，用户体验好。 Set因为 Set 堆放的是一堆不重复值的集合。所以可以做全局去重的功能。为什么不用 JVM 自带的 Set 进行去重？ 因为我们的系统一般都是集群部署，使用 JVM 自带的 Set，比较麻烦，难道为了一个做一个全局去重，再起一个公共服务，太麻烦了。 另外，就是利用交集、并集、差集等操作，可以计算共同喜好，全部的喜好，自己独有的喜好等功能。 Sorted SetSorted Set多了一个权重参数 Score，集合中的元素能够按 Score 进行排列。 可以做排行榜应用，取 TOP N 操作。Sorted Set 可以用来做延时任务。最后一个应用就是可以做范围查找。点击这里查看Redis面试题汇总。 Redis 的过期策略以及内存淘汰机制这个问题相当重要，到底 Redis 有没用到家，这个问题就可以看出来。 比如你 Redis 只能存 5G 数据，可是你写了 10G，那会删 5G 的数据。怎么删的，这个问题思考过么？ 还有，你的数据已经设置了过期时间，但是时间到了，内存占用率还是比较高，有思考过原因么? 回答：Redis 采用的是定期删除+惰性删除策略。 为什么不用定时删除策略定时删除，用一个定时器来负责监视 Key，过期则自动删除。虽然内存及时释放，但是十分消耗 CPU 资源。 在大并发请求下，CPU 要将时间应用在处理请求，而不是删除 Key，因此没有采用这一策略。 定期删除+惰性删除是如何工作定期删除，Redis 默认每个 100ms 检查，是否有过期的 Key，有过期 Key 则删除。 需要说明的是，Redis 不是每个 100ms 将所有的 Key 检查一次，而是随机抽取进行检查(如果每隔 100ms，全部 Key 进行检查，Redis 岂不是卡死)。 因此，如果只采用定期删除策略，会导致很多 Key 到时间没有删除。于是，惰性删除派上用场。 也就是说在你获取某个 Key 的时候，Redis 会检查一下，这个 Key 如果设置了过期时间，那么是否过期了？如果过期了此时就会删除。 采用定期删除+惰性删除就没其他问题了么? 不是的，如果定期删除没删除 Key。然后你也没即时去请求 Key，也就是说惰性删除也没生效。这样，Redis的内存会越来越高。那么就应该采用内存淘汰机制。 在 redis.conf 中有一行配置： 1# maxmemory-policy volatile-lru 该配置就是配内存淘汰策略的： noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。应该没人用吧。 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 Key。推荐使用，目前项目在用这种。 allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个 Key。应该也没人用吧，你不删最少使用 Key，去随机删。 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 Key。这种情况一般是把 Redis 既当缓存，又做持久化存储的时候才用。不推荐。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 Key。依然不推荐。 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 Key 优先移除。不推荐。 PS：如果没有设置 expire 的 Key，不满足先决条件(prerequisites)；那么 volatile-lru，volatile-random 和 volatile-ttl 策略的行为，和 noeviction(不删除) 基本上一致。点击这里查看Redis面试题汇总。 Redis 和数据库双写一致性问题一致性问题是分布式常见问题，还可以再分为最终一致性和强一致性。数据库和缓存双写，就必然会存在不一致的问题。 答这个问题，先明白一个前提。就是如果对数据有强一致性要求，不能放缓存。我们所做的一切，只能保证最终一致性。 另外，我们所做的方案从根本上来说，只能说降低不一致发生的概率，无法完全避免。因此，有强一致性要求的数据，不能放缓存。 回答：首先，采取正确更新策略，先更新数据库，再删缓存。其次，因为可能存在删除缓存失败的问题，提供一个补偿措施即可，例如利用消息队列。 如何应对缓存穿透和缓存雪崩问题这两个问题，说句实在话，一般中小型传统软件企业，很难碰到这个问题。如果有大并发的项目，流量有几百万左右。这两个问题一定要深刻考虑。 缓存穿透，即黑客故意去请求缓存中不存在的数据，导致所有的请求都怼到数据库上，从而数据库连接异常。 缓存穿透解决方案： 利用互斥锁，缓存失效的时候，先去获得锁，得到锁了，再去请求数据库。没得到锁，则休眠一段时间重试。 采用异步更新策略，无论 Key 是否取到值，都直接返回。Value 值中维护一个缓存失效时间，缓存如果过期，异步起一个线程去读数据库，更新缓存。需要做缓存预热(项目启动前，先加载缓存)操作。 提供一个能迅速判断请求是否有效的拦截机制，比如，利用布隆过滤器，内部维护一系列合法有效的 Key。迅速判断出，请求所携带的 Key 是否合法有效。如果不合法，则直接返回。 缓存雪崩，即缓存同一时间大面积的失效，这个时候又来了一波请求，结果请求都怼到数据库上，从而导致数据库连接异常。点击这里查看Redis面试题汇总。 缓存雪崩，即缓存同一时间大面积的失效，这个时候又来了一波请求，结果请求都怼到数据库上，从而导致数据库连接异常。点击这里查看Redis面试题汇总。 缓存雪崩解决方案： 给缓存的失效时间，加上一个随机值，避免集体失效。 使用互斥锁，但是该方案吞吐量明显下降了。 双缓存。我们有两个缓存，缓存 A 和缓存 B。缓存 A 的失效时间为 20 分钟，缓存 B 不设失效时间。自己做缓存预热操作。 然后细分以下几个小点：从缓存 A 读数据库，有则直接返回；A 没有数据，直接从 B 读数据，直接返回，并且异步启动一个更新线程，更新线程同时更新缓存 A 和缓存 B。 如何解决 Redis 的并发竞争 Key 问题这个问题大致就是，同时有多个子系统去 Set 一个 Key。这个时候大家思考过要注意什么呢？ 需要说明一下，我提前百度了一下，发现答案基本都是推荐用 Redis 事务机制。 我并不推荐使用 Redis 的事务机制。因为我们的生产环境，基本都是 Redis 集群环境，做了数据分片操作。 你一个事务中有涉及到多个 Key 操作的时候，这多个 Key 不一定都存储在同一个 redis-server 上。因此，Redis 的事务机制，十分鸡肋。 如果对这个 Key 操作，不要求顺序这种情况下，准备一个分布式锁，大家去抢锁，抢到锁就做 set 操作即可，比较简单。 如果对这个 Key 操作，要求顺序假设有一个 key1，系统 A 需要将 key1 设置为 valueA，系统 B 需要将 key1 设置为 valueB，系统 C 需要将 key1 设置为 valueC。 期望按照 key1 的 value 值按照 valueA &gt; valueB &gt; valueC 的顺序变化。这种时候我们在数据写入数据库的时候，需要保存一个时间戳。 假设时间戳如下： 123系统A key 1 &#123;valueA 3:00&#125;系统B key 1 &#123;valueB 3:05&#125;系统C key 1 &#123;valueC 3:10&#125; 那么，假设这会系统 B 先抢到锁，将 key1 设置为{valueB 3:05}。接下来系统 A 抢到锁，发现自己的 valueA 的时间戳早于缓存中的时间戳，那就不做 set 操作了，以此类推。 其他方法，比如利用队列，将 set 方法变成串行访问也可以。总之，灵活变通。 总结本文对 Redis 的常见问题做了一个总结。大部分是自己在工作中遇到，以及之前面试别人的时候，爱问的一些问题。另外，不推荐大家临时抱佛脚，真正碰到一些有经验的工程师，其实几下就能把你问懵。最后，希望大家有所收获。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>内存</tag>
        <tag>线程</tag>
        <tag>数据结构</tag>
        <tag>Redis</tag>
        <tag>缓存</tag>
        <tag>CPU</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[遍历集合时删除元素，到底发生了什么]]></title>
    <url>%2F2018%2F08%2F30%2F%E9%81%8D%E5%8E%86%E9%9B%86%E5%90%88%E6%97%B6%E5%88%A0%E9%99%A4%E5%85%83%E7%B4%A0%EF%BC%8C%E5%88%B0%E5%BA%95%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[开发规范里写的非常清楚，当通过 for 循环遍历集合时，一般禁止操作 (add or remove) 集合元素。 代码示例：1234567891011List&lt;String&gt; list = new ArrayList&lt;&gt;();list.add("e1");list.add("e2");for(String str : list) &#123; if("e1".equals(str)) &#123; list.remove("e1"); &#125; if("e2".equals(str)) &#123; System.out.println("element 2 fetched"); &#125;&#125; 运行结果：element 2 fetched 将不会被打印。 字节码中是如何处理的？让我们看看字节码是怎么样的，仅截图了部分字节码。 如上面截图的 #27、#34、#43，foreach 实际上是通过 Iterator 来处理的。最后通过 #87 的 goto指令进入下一次遍历，并进行 hasNext()判断。 class文件反编译后又是怎么样的？再来看看将.class文件反编译后得到的代码，实际上编译器将 foreach 转换成了用 Iterator来处理。 所以，眼见不一定为实，程序员开发时用的是高级语言，编码怎么简单高效怎么来，所以偶尔也可以看看反编译class后的代码以及字节码文件，看看编译器做了哪些优化。 12345678910111213ArrayList list = new ArrayList();list.add("e1");list.add("e2");Iterator var2 = list.iterator();while(var2.hasNext()) &#123; String str = (String)var2.next(); if("e1".equals(str)) &#123; list.remove("e1"); &#125; if("e2".equals(str)) &#123; System.out.println("element 2 fetched"); &#125;&#125; 为什么remove(e1)会导致无法获取e2？当 list.remove(“e1”)后，在 while(var2.hasNext()) 时，返回结果将为 false，因此当循环一次后Iterator将认为list已经遍历结束。 要弄清原因，需要看看ArrayList对于Iterator接口的实现，了解hasNext()、next()方法的实现。 先看看ArrayList中实现Iterator的内部类Itr。 12345private class Itr implements Iterator&lt;E&gt; &#123; int cursor;// index of next element to return int lastRet = -1;// index of last element returned; -1 if no such ...&#125; cursor表示下一个返回元素的下标，可以理解成 游标；lastRet表示上一次返回的元素下标。另ArrayList有个size属性，表示ArrayList中的元素个数。 hasNext() 的判断条件是cursor != size. 只要没遍历到最后一个元素，就返回true。 123public boolean hasNext() &#123;return cursor != size;&#125; 下面是 next() 部分代码。 1234567public E next() &#123;... int i = cursor;// cursor为当前需要返回元素的下标... cursor = i + 1;// cursor向后移动一个位置，指向下一个要返回的元素 return (E) elementData[lastRet = i];// 对lastRet赋值，然后返回当前元素&#125; 现在，看一下下面代码的运行情况： 12345678910ArrayList list = new ArrayList();list.add("e1");list.add("e2");Iterator var2 = list.iterator();while (var2.hasNext()) &#123; String str = (String)var2.next(); if("e1".equals(str)) &#123; list.remove("e1"); &#125;&#125; 第一次 调用var2.hasNext()，此时满足条件 cursor(0) != size(2)，然后执行 var2.next()，此时cursor=1 执行 list.remove(“e1”)，此时，list的size将从2变为1 当执行完第一次循环，进入第二次hasNext()判断时，cursor=1而且size=1，导致Iterator认为已经遍历结束，因此e2将被漏掉。 此时，过程已非常清楚。list本有2个元素，Iterator第一次获取元素时，程序删掉了当前元素，导致list的size变为1。Iterator第二次获取元素时，开心说到：”list一共只有一个元素，我已经遍历了一个，easy，轻松搞定!”。 矛盾点在于：hasNext() 是根据已fetch元素和被遍历对象的size动态判断的，一旦遍历过程中被遍历对象的size变化，就会出现以上问题。 用普通for循环进行处理如果在普通for循环中进行如上操作，又会发生什么呢？ 12345678List&lt;String&gt; list =new ArrayList&lt;&gt;();list.add("e1");list.add("e2");for(int i =0, length = list.size(); i &lt; length; i++) &#123; if("e1".equals(list.get(i))) &#123; list.remove("e1"); &#125;&#125; 运行后将报如下异常： 1java.lang.IndexOutOfBoundsException: Index: 1, Size: 1 原因：局部变量length为list遍历前的size，length=2；remove(“e1”)后，list的size变为1；因此，第二次进入循环执行list.get(1)时将出现上述异常。 正确的姿势将remove操作交给Iterator来处理，使用Iterator接口提供的remove操作。 123456789101112List&lt;String&gt; list = new ArrayList&lt;&gt;();list.add("e1");list.add("e2");for(Iterator&lt;String&gt; iterator = list.iterator(); iterator.hasNext(); ) &#123; String str = iterator.next(); if("e1".equals(str)) &#123; iterator.remove(); &#125; if("e2".equals(str)) &#123; System.out.println("element 2 fetched"); &#125;&#125; 运行结果：element 2 fetched 被正常打印出来。 那Iterator的remove()又是怎么做的？下面是ArrayList中迭代器的remove方法。 123456789101112public void remove() &#123; if(lastRet &lt;0 ) throw new IllegalStateException(); checkForComodification(); try &#123; ArrayList.this.remove(lastRet);// 调用ArrayList的remove移除元素,且size减1 cursor = lastRet;// 将游标回退一位 lastRet = -1 ;// 重置lastRet expectedModCount = modCount; &#125; catch (IndexOutOfBoundsException ex) &#123; throw new ConcurrentModificationException(); &#125;&#125; 因为Iterator.remove()在执行集合本身的remove后，同时对游标进行了 “校准”。 关于ConcurrentModificationException以下Demo将抛出该异常。 12345678910111213141516private static List&lt;String&gt; list = new ArrayList&lt;&gt;();private static boolean isListUpdated = false;public static void main( String[] args) throws InterruptedException &#123; list.add("e1"); list.add("e2"); new Thread(() -&gt; &#123; list.add("e3"); isListUpdated = true; &#125;).start(); for (Iterator&lt;String&gt; iterator = list.iterator(); iterator.hasNext(); ) &#123; while (!isListUpdated) &#123; Thread.sleep(1000); &#125; iterator.next(); &#125;&#125; ArrayList中对于Iterator的实现类为Itr如下： 12345private class Itr implements Iterator &lt;E&gt; &#123; int cursor;// index of next element to return int lastRet = -1;// index of last element returned; -1 if no such int expectedModCount = modCount;&#125; 其中有个重要的属性 expectedModCount，表示本次期望修改的次数，初始值为modCount. modCount 是 AbstractList 的属性，如下： 1protected transient int modCount = 0; 注意，它由transient修饰，保证了线程之间修改的可见性。对集合中对象的增加、删除操作都会对modCount加1。 在next()、remove()操作中都会进行 checkForComodification() ，用于检查迭代期间其他线程是否修改了被迭代对象。下面是checkForComodification方法： 1234final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException();&#125; 这是一种 Fail-Fast(快速失败) 策略，只要被迭代对象发生变更，将满足 modCount != expectedModCount 条件，从而抛出ConcurrentModificationException。]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA对象引用，对象赋值]]></title>
    <url>%2F2018%2F08%2F27%2FJAVA%E5%AF%B9%E8%B1%A1%E5%BC%95%E7%94%A8%EF%BC%8C%E5%AF%B9%E8%B1%A1%E8%B5%8B%E5%80%BC%2F</url>
    <content type="text"><![CDATA[关于对象与引用之间的一些基本概念。 ​ 初学Java时，在很长一段时间里，总觉得基本概念很模糊。后来才知道，在许多Java书中，把对象和对象的引用混为一谈。可是，如果我分不清对象与对象引用，那实在没法很好地理解下面的面向对象技术。把自己的一点认识写下来，或许能让初学Java的朋友们少走一点弯路。 ​ 为便于说明，我们先定义一个简单的类： 12345public class Vehicle &#123; int passengers; int fuelcap; int mpg;&#125; ​ 有了这个模板，就可以用它来创建对象： 1Vehicle veh1 = new Vehicle(); 通常把这条语句的动作称之为创建一个对象，其实，它包含了四个动作。 ​ 1）右边的“new Vehicle”，是以Vehicle类为模板，在堆空间里创建一个Vehicle类对象（也简称为Vehicle对象）。 ​ 2）末尾的()意味着，在对象创建后，立即调用Vehicle类的构造函数，对刚生成的对象进行初始化。构造函数是肯定有的。如果你没写，Java会给你补上一个默认的构造函数。 ​ 3）左边的“Vehicle veh 1”创建了一个Vehicle类引用变量。所谓Vehicle类引用，就是以后可以用来指向Vehicle对象的对象引用。 ​ 4）“=”操作符使对象引用指向刚创建的那个Vehicle对象。 我们可以把这条语句拆成两部分： 12Vehicle veh1;veh1 = new Vehicle(); 效果是一样的。这样写，就比较清楚了，有两个实体：一是对象引用变量，一是对象本身。 ​ 在堆空间里创建的实体，与在数据段以及栈空间里创建的实体不同。尽管它们也是确确实实存在的实体，但是，我们看不见，也摸不着。不仅如此，我们仔细研究一下第二句，找找刚创建的对象叫什么名字？有人说，它叫“Vehicle”。不对，“Vehicle”是类（对象的创建模板）的名字。 ​ 一个Vehicle类可以据此创建出无数个对象，这些对象不可能全叫“Vehicle”。 ​ 对象连名都没有，没法直接访问它。我们只能通过对象引用来间接访问对象。 ​ 为了形象地说明对象、引用及它们之间的关系，可以做一个或许不很妥当的比喻。对象好比是一只很大的气球，大到我们抓不住它。引用变量是一根绳， 可以用来系汽球。 ​ 如果只执行了第一条语句，还没执行第二条，此时创建的引用变量veh1还没指向任何一个对象，它的值是null。引用变量可以指向某个对象，或者为null。 ​ 它是一根绳，一根还没有系上任何一个汽球的绳。执行了第二句后，一只新汽球做出来了，并被系在veh1这根绳上。我们抓住这根绳，就等于抓住了那只汽球。 ​ 再来一句： 1Vehicle veh2; ​ 就又做了一根绳，还没系上汽球。如果再加一句： 1veh2 = veh1; 系上了。这里，发生了复制行为。但是，要说明的是，对象本身并没有被复制，被复制的只是对象引用。结果是，veh2也指向了veh1所指向的对象。两根绳系的是同一只汽球。 ​ 如果用下句再创建一个对象： 1veh2 = new Vehicle(); ​ 则引用变量veh2改指向第二个对象。 ​ 从以上叙述再推演下去，我们可以获得以下结论： （1）一个对象引用可以指向0个或1个对象（一根绳子可以不系汽球，也可以系一个汽球）； （2）一个对象可以有N个引用指向它（可以有N条绳子系住一个汽球）。 ​ 如果再来下面语句： 1veh1 = veh2; ​ 按上面的推断，veh1也指向了第二个对象。这个没问题。问题是第一个对象呢？没有一条绳子系住它，它飞了。多数书里说，它被Java的垃圾回收机制回收了。这不确切。正确地说，它已成为垃圾回收机制的处理对象。至于什么时候真正被回收，那要看垃圾回收机制的心情了。 ​ 由此看来，下面的语句应该不合法吧？至少是没用的吧？ 1new Vehicle(); ​ 不对。它是合法的，而且可用的。譬如，如果我们仅仅为了打印而生成一个对象，就不需要用引用变量来系住它。最常见的就是打印字符串： 1System.out.println(“I am Java!”); ​ 字符串对象“I am Java!”在打印后即被丢弃。有人把这种对象称之为临时对象。 ​ 对象与引用的关系将持续到对象回收。 ​ Java对象及引用是容易混淆却又必须掌握的基础知识，本章阐述Java对象和引用的概念，以及与其密切相关的参数传递。 ​ 先看下面的程序： 12StringBuffer s;s = new StringBuffer("Hello World!"); ​ 第一个语句仅为引用(reference)分配了空间，而第二个语句则通过调用类(StringBuffer)的构造函数StringBuffer(String str)为类生成了一个实例（或称为对象）。这两个操作被完成后，对象的内容则可通过s进行访问——在Java里都是通过引用来操纵对象的。 ​ Java对象和引用的关系可以说是互相关联，却又彼此独立。彼此独立主要表现在：引用是可以改变的，它可以指向别的对象，譬如上面的s，你可以给它另外的对象，如： 1s = new StringBuffer("Java"); ​ 这样一来，s就和它指向的第一个对象脱离关系。 从存储空间上来说，对象和引用也是独立的，它们存储在不同的地方，对象一般存储在堆中，而引用存储在速度更快的堆栈中。 引用可以指向不同的对象，对象也可以被多个引用操纵，如： 1StringBuffer s1 = s; ​ 这条语句使得s1和s指向同一个对象。既然两个引用指向同一个对象，那么不管使用哪个引用操纵对象，对象的内容都发生改变，并且只有一份，通过s1和s得到的内容自然也一样，(String除外，因为String始终不变，String s1=”AAAA”; String s=s1,操作s,s1由于始终不变，所以为s另外开辟了空间来存储s,)如下面的程序： 123456StringBuffer s;s = new StringBuffer("Java");StringBuffer s1 = s;s1.append(" World");System.out.println("s1=" + s1.toString());//打印结果为：s1=Java WorldSystem.out.println("s=" + s.toString());//打印结果为：s=Java World 上面的程序表明，s1和s打印出来的内容是一样的，这样的结果看起来让人非常疑惑，但是仔细想想，s1和s只是两个引用，它们只是操纵杆而已，它们指向同一个对象，操纵的也是同一个对象，通过它们得到的是同一个对象的内容。这就像汽车的刹车和油门，它们操纵的都是车速，假如汽车开始的速度是80，然后你踩了一次油门，汽车加速了，假如车速升到了120，然后你踩一下刹车，此时车速是从120开始下降的，假如下降到60，再踩一次油门，车速则从60开始上升，而不是从第一次踩油门后的120开始。也就是说车速同时受油门和刹车影响，它们的影响是累积起来的，而不是各自独立（除非刹车和油门不在一辆车上）。所以，在上面的程序中，不管使用s1还是s操纵对象，它们对对象的影响也是累积起来的（更多的引用同理）。 ​ 只有理解了对象和引用的关系，才能理解参数传递。 ​ 一般面试题中都会考Java传参的问题，并且它的标准答案是Java只有一种参数传递方式：那就是按值传递，即Java中传递任何东西都是传值。如果传入方法的是基本类型的东西，你就得到此基本类型的一份拷贝。如果是传递引用，就得到引用的拷贝。 ​ 一般来说，对于基本类型的传递，我们很容易理解，而对于对象，总让人感觉是按引用传递，看下面的程序 ： 123456789101112131415161718192021222324252627public class ObjectRef &#123; //基本类型的参数传递 public static void testBasicType(int m) &#123; System.out.println("m=" + m);//m=50 m = 100; System.out.println("m=" + m);//m=100 &#125; //参数为对象，不改变引用的值 ？？？？？？ public static void add(StringBuffer s) &#123; s.append("_add"); &#125; //参数为对象，改变引用的值 ？？？？？ public static void changeRef(StringBuffer s) &#123; s = new StringBuffer("Java"); &#125; public static void main(String[] args) &#123; int i = 50; testBasicType(i); System.out.println(i);//i=50 StringBuffer sMain = new StringBuffer("init"); System.out.println("sMain=" + sMain.toString());//sMain=init add(sMain); System.out.println("sMain=" + sMain.toString());//sMain=init_add changeRef(sMain); System.out.println("sMain=" + sMain.toString());//sMain=init_add &#125;&#125; 以上程序的允许结果显示出，testBasicType方法的参数是基本类型，尽管参数m的值发生改变，但并不影响i。 ​ add方法的参数是一个对象，当把sMain传给参数s时，s得到的是sMain的拷贝，所以s和sMain指向同一个对象，因此，使用s操作影响的其实就是sMain指向的对象，故调用add方法后，sMain指向的对象的内容发生了改变。 ​ 在changeRef方法中，参数也是对象，当把sMain传给参数s时，s得到的是sMain的拷贝，但与add方法不同的是，在方法体内改变了s指向的对象（也就是s指向了别的对象,牵着气球的绳子换气球了），给s重新赋值后，s与sMain已经毫无关联，它和sMain指向了不同的对象，所以不管对s做什么操作，都不会影响sMain指向的对象，故调用changeRef方法前后sMain指向的对象内容并未发生改变。 ​ 对于add方法的调用结果，可能很多人会有这种感觉：这不明明是按引用传递吗？对于这种问题，还是套用Bruce Eckel的话：这依赖于你如何看待引用，最终你会明白，这个争论并没那么重要。真正重要的是，你要理解，传引用使得（调用者的）对象的修改变得不可预期。 1234567891011121314151617181920212223public class Test &#123; public int i,j; public void test_m(Test a) &#123; Test b = new Test(); b.i = 1; b.j = 2; a = b; &#125; public void test_m1(Test a) &#123; a.i = 1; a.j = 2; &#125; public static void main(String argv[]) &#123; Test t= new Test(); t.i = 5; t.j = 6; System.out.println( "t.i = "+ t.i + " t.j= " + t.j); //5,6 t.test_m(t); System.out.println( "t.i = "+ t.i + " t.j= " + t.j); //5,6,a和t都指向了一个对象，而在test_m中s又指向了另一个对象，所以对象t不变！！！ t.test_m1(t); System.out.println( "t.i = "+ t.i + " t.j= " + t.j); //1,2 &#125;&#125; 答案只有一个：Java里都是按值传递参数。而实际上，我们要明白，当参数是对象时，传引用会发生什么状况（就像上面的add方法）？ 这样来记这个问题 如下表达式： 1A a1 = new A(); 它代表A是类，a1是引用，a1不是对象，new A()才是对象，a1引用指向new A()这个对象。 在JAVA里，“=”不能被看成是一个赋值语句，它不是在把一个对象赋给另外一个对象，它的执行过程实质上是将右边对象的地址传给了左边的引用，使得左边的引用指向了右边的对象。JAVA表面上看起来没有指针，但它的引用其实质就是一个指针，引用里面存放的并不是对象，而是该对象的地址，使得该引用指向了对象。在JAVA里，“=”语句不应该被翻译成赋值语句，因为它所执行的确实不是一个赋值的过程，而是一个传地址的过程，被译成赋值语句会造成很多误解，译得不准确。 ​ 再如： A a2; 它代表A是类，a2是引用，a2不是对象，a2所指向的对象为空null; 再如： a2 = a1; 它代表，a2是引用，a1也是引用，a1所指向的对象的地址传给了a2(传址），使得a2和a1指向了同一对象。 ​ 综上所述，可以简单的记为，在初始化时，“=”语句左边的是引用，右边new出来的是对象。 在后面的左右都是引用的“=”语句时，左右的引用同时指向了右边引用所指向的对象。 再所谓实例，其实就是对象的同义词。]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>对象</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA实现单链表]]></title>
    <url>%2F2018%2F08%2F26%2FJAVA%E5%AE%9E%E7%8E%B0%E5%8D%95%E9%93%BE%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[​ 刚开始学习java不久的时候以为java没有指针。。。不知道怎么弄链表，java中有基本数据类型和引用数据类型（其实就是指针）。如果对引用不够了解请访问 http://zwmf.iteye.com/blog/1738574 （写得特别好，就没必要重复了）。 实现链表的思路： 1）链表类，结点类（链表类的内部类），在main()方法创建一条链表类对象，通过方法逐步创建结点类，通过引用链接起来成为链表。 2）结点类包含数据和对下个结点的引用，以及可以对数据赋值的构造函数。 3）链表类的构造方法，只构造出不含数据的头结点。（外部类可以直接对内部类的私有成员进行访问，这样就可以直接修改引用） 主体代码：123456public class MyLink&lt;E&gt;&#123; private class Node&#123; privare Object data; //保存数据 private Node next; //对下个结点的引用 &#125; &#125; 完整版代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272package Date_Structures.LinkedList;public class MyLink &#123; public static void main(String[] args) &#123; MyLink list=new MyLink(); list.addNode(5); list.addNode(3); list.addNode(1); list.addNode(2); list.addNode(55); list.addNode(36); System.out.println("LinkLength:"+list.linkListLength(head)); list.printListReversely(head); list.traverse(head); list.deleteNode(head,4); System.out.println("after delete(4):"); list.printListReversely(head); list.traverse(head); list.insertNode(head,4,7); System.out.println("after insert(4,7):"); list.traverse(head); &#125; static Node head=null; public class Node&#123; //数据域 public int data; //指针域，指向下一个节点 public Node next=null; public Node() &#123; &#125; public Node(int data) &#123; this.data = data; &#125; &#125; /** * 向链表添加数据 * * @param value 要添加的数据 */ public void addNode(int value) &#123; //初始化要加入的节点 Node newNode = new Node(value); if(head==null)&#123; head=new Node(); &#125; //临时节点 Node temp = head; // 找到尾节点 while (temp.next != null) &#123; temp = temp.next; &#125; // 已经包括了头节点.next为null的情况了～ temp.next = newNode; &#125; /** * 遍历链表 * * @param head 头节点 */ public void traverse(Node head) &#123; //临时节点，从首节点开始 Node temp = head.next; while (temp != null) &#123; System.out.println("当前节点：" + temp.data); //继续下一个 temp = temp.next; &#125; &#125; /** * 插入节点 * * @param head 头指针 * @param index 要插入的位置 * @param value 要插入的值 */ public void insertNode(Node head, int index, int value) &#123; //首先需要判断指定位置是否合法， if (index &lt; 1 || index &gt; linkListLength(head) + 1) &#123; System.out.println("插入位置不合法。"); return; &#125; //临时节点，从头节点开始 Node temp = head; //记录遍历的当前位置 int currentPos = 0; //初始化要插入的节点 Node insertNode = new Node(value); while (temp.next != null) &#123; //找到上一个节点的位置了 if ((index - 1) == currentPos) &#123; //temp表示的是上一个节点 //将原本由上一个节点的指向交由插入的节点来指向 insertNode.next = temp.next; //将上一个节点的指针域指向要插入的节点 temp.next = insertNode; return; &#125; currentPos++; temp = temp.next; &#125; &#125; /** * 获取链表的长度 * @param head 头指针 */ public static int linkListLength(Node head) &#123; int length = 0; //临时节点，从首节点开始 Node temp = head.next; // 找到尾节点 while (temp != null) &#123; length++; temp = temp.next; &#125; return length; &#125; /** * 根据位置删除节点 * * @param head 头指针 * @param index 要删除的位置 */ public void deleteNode(Node head, int index) &#123; //首先需要判断指定位置是否合法， if (index &lt; 1 || index &gt; linkListLength(head) + 1) &#123; System.out.println("删除位置不合法。"); return; &#125; //临时节点，从头节点开始 Node temp = head; //记录遍历的当前位置 int currentPos = 0; while (temp.next != null) &#123; //找到上一个节点的位置了 if ((index - 1) == currentPos) &#123; //temp表示的是上一个节点 //temp.next表示的是想要删除的节点 //将想要删除的节点存储一下 Node deleteNode = temp.next; //想要删除节点的下一个节点交由上一个节点来控制 temp.next = deleteNode.next; //Java会回收它，设置不设置为null应该没多大意义了(个人觉得,如果不对请指出哦～) deleteNode = null; return; &#125; currentPos++; temp = temp.next; &#125; &#125; /** * 对链表进行排序 * * @param head * */ public void sortLinkList(Node head) &#123; Node currentNode; Node nextNode; for (currentNode = head.next; currentNode.next != null; currentNode = currentNode.next) &#123; for (nextNode = head.next; nextNode.next != null; nextNode = nextNode.next) &#123; if (nextNode.data &gt; nextNode.next.data) &#123; int temp = nextNode.data; nextNode.data = nextNode.next.data; nextNode.next.data = temp; &#125; &#125; &#125; &#125; /** * 找到链表中倒数第k个节点(设置两个指针p1、p2，让p2比p1快k个节点，同时向后遍历，当p2为空，则p1为倒数第k个节点 * * @param head * @param k 倒数第k个节点 */ public Node findKNode(Node head, int k) &#123; if (k &lt; 1 || k &gt; linkListLength(head)) return null; Node p1 = head; Node p2 = head; // p2比怕p1快k个节点 for (int i = 0; i &lt; k - 1; i++) p2 = p2.next; // 只要p2为null，那么p1就是倒数第k个节点了 while (p2.next != null) &#123; p2 = p2.next; p1 = p1.next; &#125; return p1; &#125; /** * 删除链表重复数据(跟冒泡差不多，等于删除就是了) * * @param head 头节点 */ public void deleteDuplecate(Node head) &#123; //临时节点，(从首节点开始--&gt;真正有数据的节点) Node temp = head.next; //当前节点(首节点)的下一个节点 Node nextNode = temp.next; while (temp.next != null) &#123; while (nextNode.next != null) &#123; if (nextNode.next.data == nextNode.data) &#123; //将下一个节点删除(当前节点指向下下个节点) nextNode.next = nextNode.next.next; &#125; else &#123; //继续下一个 nextNode = nextNode.next; &#125; &#125; //下一轮比较 temp = temp.next; &#125; &#125; /** * 查询单链表的中间节点 */ public static Node searchMid(Node head) &#123; Node p1 = head; Node p2 = head; // 一个走一步，一个走两步，直到为null，走一步的到达的就是中间节点 while (p2 != null &amp;&amp; p2.next != null &amp;&amp; p2.next.next != null) &#123; p1 = p1.next; p2 = p2.next.next; &#125; return p1; &#125; /** * 通过递归从尾到头输出单链表 * * @param head 头节点 */ public void printListReversely(Node head) &#123; if (head != null) &#123; printListReversely(head.next); System.out.println(head.data); &#125; &#125; /** * 实现链表的反转 * * @param node 链表的头节点 */ public Node reverseLinkList(Node node) &#123; Node prev ; if (node == null || node.next == null) &#123; prev = node; &#125; else &#123; Node tmp = reverseLinkList(node.next); node.next.next = node; node.next = null; prev = tmp; &#125; return prev; &#125;&#125;]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[详细的Nginx简易教程]]></title>
    <url>%2F2018%2F07%2F16%2F%E8%AF%A6%E7%BB%86%E7%9A%84Nginx%E7%AE%80%E6%98%93%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[什么是Nginx?Nginx (engine x) 是一款轻量级的Web 服务器 、反向代理服务器及电子邮件（IMAP/POP3）代理服务器。 什么是反向代理？反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。 安装与使用安装nginx官网下载地址：http://nginx.org ，发布版本分为 Linux 和 windows 版本。 也可以下载源码，编译后运行。 从源代码编译 Nginx把源码解压缩之后，在终端里运行如下命令： 123$ ./configure$ make$ sudo make install 默认情况下，Nginx 会被安装在 /usr/local/nginx。通过设定编译选项，你可以改变这个设定。 Windows 安装为了安装 Nginx / Win32，需先下载它。然后解压之，然后运行即可。下面以 C 盘根目录为例说明下： 12cd C:cd C: ginx-0.8.54 start nginx Nginx / Win32 是运行在一个控制台程序，而非 windows 服务方式的。服务器方式目前还是开发尝试中。 使用nginx 的使用比较简单，就是几条命令。 常用到的命令如下： nginx -s stop ：快速关闭Nginx，可能不保存相关信息，并迅速终止web服务。 nginx -s quit ：平稳关闭Nginx，保存相关信息，有安排的结束web服务。 nginx -s reload ：因改变了Nginx相关配置，需要重新加载配置而重载。 nginx -s reopen ：重新打开日志文件。 nginx -c filename ：为 Nginx 指定一个配置文件，来代替缺省的。 nginx -t ：不运行，而仅仅测试配置文件。nginx 将检查配置文件的语法的正确性，并尝试打开配置文件中所引用到的文件。 nginx -v：显示 nginx 的版本。 nginx -V：显示 nginx 的版本，编译器版本和配置参数。 如果不想每次都敲命令，可以在nginx安装目录下新添一个启动批处理文件startup.bat，双击即可运行。内容如下： 123456789101112@echo offrem 如果启动前已经启动nginx并记录下pid文件，会kill指定进程nginx.exe -s stoprem 测试配置文件语法正确性nginx.exe -t -c conf/nginx.confrem 显示版本信息nginx.exe -vrem 按照指定配置去启动nginxnginx.exe -c conf/nginx.conf 如果是运行在 Linux 下，写一个 shell 脚本，大同小异。 nginx 配置实战我始终认为，各种开发工具的配置还是结合实战来讲述，会让人更易理解。 http反向代理配置我们先实现一个小目标：不考虑复杂的配置，仅仅是完成一个 http 反向代理。 nginx.conf 配置文件如下： 注：conf / nginx.conf 是 nginx 的默认配置文件。你也可以使用 nginx -c 指定你的配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107#运行用户#user somebody;#启动进程,通常设置成和cpu的数量相等worker_processes 1;#全局错误日志error_log D:/Tools/nginx-1.10.1/logs/error.log;error_log D:/Tools/nginx-1.10.1/logs/notice.log notice;error_log D:/Tools/nginx-1.10.1/logs/info.log info;#PID文件，记录当前启动的nginx的进程IDpid D:/Tools/nginx-1.10.1/logs/nginx.pid;#工作模式及连接数上限events &#123; worker_connections 1024; #单个后台worker process进程的最大并发链接数&#125;#设定http服务器，利用它的反向代理功能提供负载均衡支持http &#123; #设定mime类型(邮件支持类型),类型由mime.types文件定义 include D:/Tools/nginx-1.10.1/conf/mime.types; default_type application/octet-stream; #设定日志 log_format main '[$remote_addr] - [$remote_user] [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log D:/Tools/nginx-1.10.1/logs/access.log main; rewrite_log on; #sendfile 指令指定 nginx 是否调用 sendfile 函数（zero copy 方式）来输出文件，对于普通应用， #必须设为 on,如果用来进行下载等应用磁盘IO重负载应用，可设置为 off，以平衡磁盘与网络I/O处理速度，降低系统的uptime. sendfile on; #tcp_nopush on; #连接超时时间 keepalive_timeout 120; tcp_nodelay on; #gzip压缩开关 #gzip on; #设定实际的服务器列表 upstream zp_server1&#123; server 127.0.0.1:8089; &#125; #HTTP服务器 server &#123; #监听80端口，80端口是知名端口号，用于HTTP协议 listen 80; #定义使用www.xx.com访问 server_name www.helloworld.com; #首页 index index.html #指向webapp的目录 root D:_WorkspaceProjectgithubzpSpringNotesspring-securityspring-shirosrcmainwebapp; #编码格式 charset utf-8; #代理配置参数 proxy_connect_timeout 180; proxy_send_timeout 180; proxy_read_timeout 180; proxy_set_header Host $host; proxy_set_header X-Forwarder-For $remote_addr; #反向代理的路径（和upstream绑定），location 后面设置映射的路径 location / &#123; proxy_pass http://zp_server1; &#125; #静态文件，nginx自己处理 location ~ ^/(images|javascript|js|css|flash|media|static)/ &#123; root D:_WorkspaceProjectgithubzpSpringNotesspring-securityspring-shirosrcmainwebappiews; #过期30天，静态文件不怎么更新，过期可以设大一点，如果频繁更新，则可以设置得小一点。 expires 30d; &#125; #设定查看Nginx状态的地址 location /NginxStatus &#123; stub_status on; access_log on; auth_basic "NginxStatus"; auth_basic_user_file conf/htpasswd; &#125; #禁止访问 .htxxx 文件 location ~ /.ht &#123; deny all; &#125; #错误处理页面（可选择性配置） #error_page 404 /404.html; #error_page 500 502 503 504 /50x.html; #location = /50x.html &#123; # root html; #&#125; &#125;&#125; 好了，让我们来试试吧： 启动 webapp，注意启动绑定的端口要和nginx中的 upstream 设置的端口保持一致。 更改 host：在 C:WindowsSystem32driversetc 目录下的host文件中添加一条DNS 记录127.0.0.1 www.helloworld.com 启动前文中 startup.bat 的命令 在浏览器中访问 www.helloworld.com ，不出意外，已经可以访问了。 负载均衡配置上一个例子中，代理仅仅指向一个服务器。 但是，网站在实际运营过程中，多半都是有多台服务器运行着同样的app，这时需要使用负载均衡来分流。 nginx也可以实现简单的负载均衡功能。 假设这样一个应用场景：将应用部署在 192.168.1.11:80、192.168.1.12:80、192.168.1.13:80 三台linux环境的服务器上。网站域名叫 www.helloworld.com ，公网IP为 192.168.1.11。在公网IP所在的服务器上部署 nginx，对所有请求做负载均衡处理。 nginx.conf 配置如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748http &#123; #设定mime类型,类型由mime.type文件定义 include /etc/nginx/mime.types; default_type application/octet-stream; #设定日志格式 access_log /var/log/nginx/access.log; #设定负载均衡的服务器列表 upstream load_balance_server &#123; #weigth参数表示权值，权值越高被分配到的几率越大 server 192.168.1.11:80 weight=5; server 192.168.1.12:80 weight=1; server 192.168.1.13:80 weight=6; &#125; #HTTP服务器 server &#123; #侦听80端口 listen 80; #定义使用www.xx.com访问 server_name www.helloworld.com; #对所有请求进行负载均衡请求 location / &#123; root /root; #定义服务器的默认网站根目录位置 index index.html index.htm; #定义首页索引文件的名称 proxy_pass http://load_balance_server ;#请求转向load_balance_server 定义的服务器列表 #以下是一些反向代理的配置(可选择性配置) #proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header X-Forwarded-For $remote_addr; proxy_connect_timeout 90; #nginx跟后端服务器连接超时时间(代理连接超时) proxy_send_timeout 90; #后端服务器数据回传时间(代理发送超时) proxy_read_timeout 90; #连接成功后，后端服务器响应时间(代理接收超时) proxy_buffer_size 4k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 proxy_buffers 4 32k; #proxy_buffers缓冲区，网页平均在32k以下的话，这样设置 proxy_busy_buffers_size 64k; #高负荷下缓冲大小（proxy_buffers*2） proxy_temp_file_write_size 64k; #设定缓存文件夹大小，大于这个值，将从upstream服务器传 client_max_body_size 10m; #允许客户端请求的最大单文件字节数 client_body_buffer_size 128k; #缓冲区代理缓冲用户端请求的最大字节数 &#125; &#125;&#125; 网站有多个webapp的配置当一个网站功能越来越丰富时，往往需要将一些功能相对独立的模块剥离出来，独立维护。这样的话，通常，会有多个 webapp。 举个例子：假如 www.helloworld.com 站点有好几个webapp，finance（金融）、product（产品）、admin（用户中心）。访问这些应用的方式通过上下文(context)来进行区分: www.helloworld.com/finance/ www.helloworld.com/product/ www.helloworld.com/admin/ 我们知道，http的默认端口号是80，如果在一台服务器上同时启动这3个 webapp 应用，都用80端口，肯定是不成的。所以，这三个应用需要分别绑定不同的端口号。 那么，问题来了，用户在实际访问 www.helloworld.com 站点时，访问不同 webapp，总不会还带着对应的端口号去访问吧。所以，你再次需要用到反向代理来做处理。 配置也不难，来看看怎么做吧： 1234567891011121314151617181920212223242526272829303132333435http &#123; #此处省略一些基本配置 upstream product_server&#123; server www.helloworld.com:8081; &#125; upstream admin_server&#123; server www.helloworld.com:8082; &#125; upstream finance_server&#123; server www.helloworld.com:8083; &#125; server &#123; #此处省略一些基本配置 #默认指向product的server location / &#123; proxy_pass http://product_server; &#125; location /product/&#123; proxy_pass http://product_server; &#125; location /admin/ &#123; proxy_pass http://admin_server; &#125; location /finance/ &#123; proxy_pass http://finance_server; &#125; &#125;&#125; https反向代理配置一些对安全性要求比较高的站点，可能会使用 HTTPS（一种使用ssl通信标准的安全HTTP协议）。 这里不科普 HTTP 协议和 SSL 标准。但是，使用 nginx 配置 https 需要知道几点： HTTPS 的固定端口号是 443，不同于 HTTP 的 80 端口 SSL 标准需要引入安全证书，所以在 nginx.conf 中你需要指定证书和它对应的 key 其他和 http 反向代理基本一样，只是在 Server 部分配置有些不同。 12345678910111213141516171819202122232425#HTTP服务器 server &#123; #监听443端口。443为知名端口号，主要用于HTTPS协议 listen 443 ssl; #定义使用www.xx.com访问 server_name www.helloworld.com; #ssl证书文件位置(常见证书文件格式为：crt/pem) ssl_certificate cert.pem; #ssl证书key位置 ssl_certificate_key cert.key; #ssl配置参数（选择性配置） ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; #数字签名，此处使用MD5 ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; location / &#123; root /root; index index.html index.htm; &#125; &#125; 静态站点配置有时候，我们需要配置静态站点(即 html 文件和一堆静态资源)。 举例来说：如果所有的静态资源都放在了 /app/dist 目录下，我们只需要在 nginx.conf 中指定首页以及这个站点的 host 即可。 配置如下： 123456789101112131415161718192021222324252627worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; gzip on; gzip_types text/plain application/x-javascript text/css application/xml text/javascript application/javascript image/jpeg image/gif image/png; gzip_vary on; server &#123; listen 80; server_name static.zp.cn; location / &#123; root /app/dist; index index.html; #转发任何请求到 index.html &#125; &#125;&#125; 然后，添加 HOST：127.0.0.1 static.zp.cn，此时，在本地浏览器访问 static.zp.cn ，就可以访问静态站点了。 跨域解决方案web 领域开发中，经常采用前后端分离模式。这种模式下，前端和后端分别是独立的 web 应用程序，例如：后端是 Java 程序，前端是 React 或 Vue 应用。 各自独立的 web app 在互相访问时，势必存在跨域问题。解决跨域问题一般有两种思路： CORS在后端服务器设置 HTTP 响应头，把你需要运行访问的域名加入加入 Access-Control-Allow-Origin 中。 jsonp把后端根据请求，构造json数据，并返回，前端用 jsonp 跨域。 这两种思路，本文不展开讨论。 需要说明的是，nginx 根据第一种思路，也提供了一种解决跨域的解决方案。 举例：www.helloworld.com 网站是由一个前端 app ，一个后端 app 组成的。前端端口号为 9000， 后端端口号为 8080。 前端和后端如果使用 http 进行交互时，请求会被拒绝，因为存在跨域问题。来看看，nginx 是怎么解决的吧： 首先，在 enable-cors.conf 文件中设置 cors ： 1234567891011121314151617181920212223242526# allow origin listset $ACAO '*';# set single originif ($http_origin ~* (www.helloworld.com)$) &#123; set $ACAO $http_origin;&#125;if ($cors = "trueget") &#123; add_header 'Access-Control-Allow-Origin' "$http_origin"; add_header 'Access-Control-Allow-Credentials' 'true'; add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS'; add_header 'Access-Control-Allow-Headers' 'DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type';&#125;if ($request_method = 'OPTIONS') &#123; set $cors "$&#123;cors&#125;options";&#125;if ($request_method = 'GET') &#123; set $cors "$&#123;cors&#125;get";&#125;if ($request_method = 'POST') &#123; set $cors "$&#123;cors&#125;post";&#125; 接下来，在你的服务器中 include enable-cors.conf 来引入跨域配置： 12345678910111213141516171819202122232425262728# ----------------------------------------------------# 此文件为项目 nginx 配置片段# 可以直接在 nginx config 中 include（推荐）# 或者 copy 到现有 nginx 中，自行配置# www.helloworld.com 域名需配合 dns hosts 进行配置# 其中，api 开启了 cors，需配合本目录下另一份配置文件# ----------------------------------------------------upstream front_server&#123; server www.helloworld.com:9000;&#125;upstream api_server&#123; server www.helloworld.com:8080;&#125;server &#123; listen 80; server_name www.helloworld.com; location ~ ^/api/ &#123; include enable-cors.conf; proxy_pass http://api_server; rewrite "^/api/(.*)$" /$1 break; &#125; location ~ ^/ &#123; proxy_pass http://front_server; &#125;&#125; 到此，就完成了。]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[彻底理解HashMap]]></title>
    <url>%2F2018%2F07%2F08%2F%E5%BD%BB%E5%BA%95%E7%90%86%E8%A7%A3HashMap%2F</url>
    <content type="text"><![CDATA[摘要： HashMap是Map族中最为常用的一种，也是 Java Collection Framework 的重要成员。本文首先给出了 HashMap 的实质并概述了其与 Map、HashSet 的关系，紧接着给出了 HashMap 在 JDK 中的定义，并结合源码分析了其四种构造方式。最后，通过对 HashMap 的数据结构、实现原理、源码实现三个方面的剖析，深入到它底层 Hash 存储机制，解释了其底层数组长度总是 2 的 n 次方的原因，也揭示了其快速存取、扩容及扩容后的重哈希的原理与实现。 友情提示： 本文所有关于HashMap的源码都是基于 JDK 1.6 的，不同 JDK 版本之间也许会有些许差异，但不影响我们对 HashMap 的数据结构、原理等整体的把握和了解。 HashMap 概述 Map 是 Key-Value 对映射的抽象接口，该映射不包括重复的键，即一个键对应一个值。HashMap 是 Java Collection Framework 的重要成员，也是Map族(如下图所示)中我们最为常用的一种。简单地说，HashMap 是基于哈希表的 Map 接口的实现，以 Key-Value 的形式存在，即存储的对象是 Entry (同时包含了 Key 和 Value) 。在HashMap中，其会根据hash算法来计算key-value的存储位置并进行快速存取。特别地，HashMap最多只允许一条Entry的键为Null(多条会覆盖)，但允许多条Entry的值为Null。此外，HashMap 是 Map 的一个非同步的实现。 同样地，HashSet 也是 Java Collection Framework 的重要成员，是 Set 接口的常用实现类，但其与 HashMap 有很多相似之处。对于 HashSet 而言，其采用 Hash 算法决定元素在Set中的存储位置，这样可以保证元素的快速存取；对于 HashMap 而言，其将 key-value 当成一个整体(Entry 对象)来处理，其也采用同样的 Hash 算法去决定 key-value 的存储位置从而保证键值对的快速存取。虽然 HashMap 和 HashSet 实现的接口规范不同，但是它们底层的 Hash 存储机制完全相同。实际上，HashSet 本身就是在 HashMap 的基础上实现的。因此，通过对 HashMap 的数据结构、实现原理、源码实现三个方面了解，我们不但可以进一步掌握其底层的 Hash 存储机制，也有助于对 HashSet 的了解。 必须指出的是，虽然容器号称存储的是 Java 对象，但实际上并不会真正将 Java 对象放入容器中，只是在容器中保留这些对象的引用。也就是说，Java 容器实际上包含的是引用变量，而这些引用变量指向了我们要实际保存的 Java 对象。 HashMap 在 JDK 中的定义 HashMap实现了Map接口，并继承 AbstractMap 抽象类，其中 Map 接口定义了键值映射规则。和 AbstractCollection抽象类在 Collection 族的作用类似， AbstractMap 抽象类提供了 Map 接口的骨干实现，以最大限度地减少实现Map接口所需的工作。HashMap 在JDK中的定义为： 12345public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable&#123;...&#125; HashMap 的构造函数 HashMap 一共提供了四个构造函数，其中 默认无参的构造函数 和 参数为Map的构造函数 为 Java Collection Framework 规范的推荐实现，其余两个构造函数则是 HashMap 专门提供的。 HashMap() 该构造函数意在构造一个具有&gt; 默认初始容量 (16) 和 默认负载因子(0.75) 的空 HashMap，是 Java Collection Framework 规范推荐提供的，其源码如下： 1234567891011121314151617 /** * Constructs an empty HashMap with the default initial capacity * (16) and the default load factor (0.75). */public HashMap() &#123; //负载因子:用于衡量的是一个散列表的空间的使用程度 this.loadFactor = DEFAULT_LOAD_FACTOR; //HashMap进行扩容的阈值，它的值等于 HashMap 的容量乘以负载因子 threshold = (int)(DEFAULT_INITIAL_CAPACITY * DEFAULT_LOAD_FACTOR); // HashMap的底层实现仍是数组，只是数组的每一项都是一条链 table = new Entry[DEFAULT_INITIAL_CAPACITY]; init();&#125; HashMap(int initialCapacity, float loadFactor) 该构造函数意在构造一个 指定初始容量 和 指定负载因子的空 HashMap，其源码如下： 1234567891011121314151617181920212223242526272829303132/** * Constructs an empty HashMap with the specified initial capacity and load factor. */public HashMap(int initialCapacity, float loadFactor) &#123; //初始容量不能小于 0 if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); //初始容量不能超过 2^30 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; //负载因子不能小于 0 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); // HashMap 的容量必须是2的幂次方，超过 initialCapacity 的最小 2^n int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; //负载因子 this.loadFactor = loadFactor; //设置HashMap的容量极限，当HashMap的容量达到该极限时就会进行自动扩容操作 threshold = (int)(capacity * loadFactor); // HashMap的底层实现仍是数组，只是数组的每一项都是一条链 table = new Entry[capacity]; init();&#125; HashMap(int initialCapacity) 该构造函数意在构造一个指定初始容量和默认负载因子 (0.75)的空 HashMap，其源码如下： 1234// Constructs an empty HashMap with the specified initial capacity and the default load factor (0.75)public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); // 直接调用上述构造函数&#125; HashMap(Map&lt;? extends K, ? extends V&gt; m) 该构造函数意在构造一个与指定 Map 具有相同映射的 HashMap，其 初始容量不小于 16 (具体依赖于指定Map的大小)，负载因子是 0.75，是 Java Collection Framework 规范推荐提供的，其源码如下： 12345678910// Constructs a new HashMap with the same mappings as the specified Map. // The HashMap is created with default load factor (0.75) and an initial capacity// sufficient to hold the mappings in the specified Map.public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; // 初始容量不小于 16 this(Math.max((int) (m.size() / DEFAULT_LOAD_FACTOR) + 1, DEFAULT_INITIAL_CAPACITY), DEFAULT_LOAD_FACTOR); putAllForCreate(m);&#125; 在这里，我们提到了两个非常重要的参数：初始容量 和 负载因子，这两个参数是影响HashMap性能的重要参数。其中，容量表示哈希表中桶的数量 (table 数组的大小)，初始容量是创建哈希表时桶的数量；负载因子是哈希表在其容量自动增加之前可以达到多满的一种尺度，它衡量的是一个散列表的空间的使用程度，负载因子越大表示散列表的装填程度越高，反之愈小。 对于使用 拉链法（下文会提到）的哈希表来说，查找一个元素的平均时间是 O(1+a)，a 指的是链的长度，是一个常数。特别地，若负载因子越大，那么对空间的利用更充分，但查找效率的也就越低；若负载因子越小，那么哈希表的数据将越稀疏，对空间造成的浪费也就越严重。系统默认负载因子为 0.75，这是时间和空间成本上一种折衷，一般情况下我们是无需修改的。 HashMap 的数据结构哈希的相关概念 Hash 就是把任意长度的输入(又叫做预映射， pre-image)，通过哈希算法，变换成固定长度的输出(通常是整型)，该输出就是哈希值。这种转换是一种 压缩映射 ，也就是说，散列值的空间通常远小于输入的空间。不同的输入可能会散列成相同的输出，从而不可能从散列值来唯一的确定输入值。简单的说，就是一种将任意长度的消息压缩到某一固定长度的息摘要函数。 哈希的应用：数据结构 我们知道，数组的特点是：寻址容易，插入和删除困难；而链表的特点是：寻址困难，插入和删除容易。那么我们能不能综合两者的特性，做出一种寻址容易，插入和删除也容易的数据结构呢？答案是肯定的，这就是我们要提起的哈希表。事实上，哈希表有多种不同的实现方法，我们接下来解释的是最经典的一种方法 —— 拉链法，我们可以将其理解为链表的数组，如下图所示： 我们可以从上图看到，左边很明显是个数组，数组的每个成员是一个链表。该数据结构所容纳的所有元素均包含一个指针，用于元素间的链接。我们根据元素的自身特征把元素分配到不同的链表中去，反过来我们也正是通过这些特征找到正确的链表，再从链表中找出正确的元素。其中，根据元素特征计算元素数组下标的方法就是 哈希算法。 总的来说，哈希表适合用作快速查找、删除的基本数据结构，通常需要总数据量可以放入内存。在使用哈希表时，有以下几个关键点： hash 函数（哈希算法）的选择：针对不同的对象(字符串、整数等)具体的哈希方法； 碰撞处理：常用的有两种方式，一种是open hashing，即 &gt;拉链法；另一种就是 closed hashing，即开地址法(opened addressing)。 HashMap 的数据结构 我们知道，在Java中最常用的两种结构是 数组 和 链表，几乎所有的数据结构都可以利用这两种来组合实现，HashMap 就是这种应用的一个典型。实际上，HashMap 就是一个 链表数组，如下是它数据结构： 从上图中，我们可以形象地看出HashMap底层实现还是数组，只是数组的每一项都是一条链。其中参数initialCapacity 就代表了该数组的长度，也就是桶的个数。在第三节我们已经了解了HashMap 的默认构造函数的源码： 1234567891011121314151617/** * Constructs an empty HashMap with the default initial capacity * (16) and the default load factor (0.75). */ public HashMap() &#123; //负载因子:用于衡量的是一个散列表的空间的使用程度 this.loadFactor = DEFAULT_LOAD_FACTOR; //HashMap进行扩容的阈值，它的值等于 HashMap 的容量乘以负载因子 threshold = (int)(DEFAULT_INITIAL_CAPACITY * DEFAULT_LOAD_FACTOR); // HashMap的底层实现仍是数组，只是数组的每一项都是一条链 table = new Entry[DEFAULT_INITIAL_CAPACITY]; init(); &#125; 从上述源码中我们可以看出，每次新建一个HashMap时，都会初始化一个Entry类型的table数组，其中 Entry类型的定义如下： 1234567891011121314151617181920static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; // 键值对的键 V value; // 键值对的值 Entry&lt;K,V&gt; next; // 下一个节点 final int hash; // hash(key.hashCode())方法的返回值 /** * Creates new entry. */ Entry(int h, K k, V v, Entry&lt;K,V&gt; n) &#123; // Entry 的构造函数 value = v; next = n; key = k; hash = h; &#125; ......&#125; 其中，Entry为HashMap的内部类，实现了 Map.Entry 接口，其包含了键key、值value、下一个节点next，以及hash值四个属性。事实上，Entry 是构成哈希表的基石，是哈希表所存储的元素的具体形式。 HashMap 的快速存取 在HashMap中，我们最常用的两个操作就是：put(Key,Value) 和 get(Key)。我们都知道，HashMap中的Key是唯一的，那它是如何保证唯一性的呢？我们首先想到的是用equals比较，没错，这样可以实现，但随着元素的增多，put 和 get 的效率将越来越低，这里的时间复杂度是O(n)。也就是说，假如 HashMap 有1000个元素，那么 put时就需要比较 1000 次，这是相当耗时的，远达不到HashMap快速存取的目的。实际上，HashMap 很少会用到equals方法，因为其内通过一个哈希表管理所有元素，利用哈希算法可以快速的存取元素。当我们调用put方法存值时，HashMap首先会调用Key的hashCode方法，然后基于此获取Key哈希码，通过哈希码快速找到某个桶，这个位置可以被称之为 bucketIndex。通过《Java 中的 ==, equals 与 hashCode 的区别与联系》 所述hashCode的协定可以知道，如果两个对象的hashCode不同，那么equals一定为 false；否则，如果其hashCode相同，equals也不一定为 true。所以，理论上，hashCode 可能存在碰撞的情况，当碰撞发生时，这时会取出bucketIndex桶内已存储的元素，并通过hashCode() 和 equals() 来逐个比较以判断Key是否已存在。如果已存在，则使用新Value值替换旧Value值，并返回旧Value值；如果不存在，则存放新的键值对&lt;Key, Value&gt;到桶中。因此，在 HashMap中，equals() 方法只有在哈希码碰撞时才会被用到。 下面我们结合JDK源码看HashMap 的存取实现。 HashMap 的存储实现 在 HashMap 中，键值对的存储是通过 put(key,vlaue) 方法来实现的，其源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940/** * Associates the specified value with the specified key in this map. * If the map previously contained a mapping for the key, the old * value is replaced. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with key, or null if there was no mapping for key. * Note that a null return can also indicate that the map previously associated null with key. */public V put(K key, V value) &#123; //当key为null时，调用putForNullKey方法，并将该键值对保存到table的第一个位置 if (key == null) return putForNullKey(value); //根据key的hashCode计算hash值 int hash = hash(key.hashCode()); // ------- (1) //计算该键值对在数组中的存储位置（哪个桶） int i = indexFor(hash, table.length); // ------- (2) //在table的第i个桶上进行迭代，寻找 key 保存的位置 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; // ------- (3) Object k; //判断该条链上是否存在hash值相同且key值相等的映射，若存在，则直接覆盖 value，并返回旧value if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; // 返回旧值 &#125; &#125; modCount++; //修改次数增加1，快速失败机制 //原HashMap中无该映射，将该添加至该链的链头 addEntry(hash, key, value, i); return null;&#125; 通过上述源码我们可以清楚了解到HashMap保存数据的过程。首先，判断key是否为null，若为null，则直接调用putForNullKey方法；若不为空，则先计算key的hash值，然后根据hash值搜索在table数组中的索引位置，如果table数组在该位置处有元素，则查找是否存在相同的key，若存在则覆盖原来key的value，否则将该元素保存在链头（最先保存的元素放在链尾）。此外，若table在该处没有元素，则直接保存。这个过程看似比较简单，但其实有很多需要回味的地方，下面我们一一来看。 先看源码中的 (3) 处，此处迭代原因就是为了防止存在相同的key值。如果发现两个hash值（key）相同时，HashMap的处理方式是用新value替换旧value，这里并没有处理key，这正好解释了 HashMap 中没有两个相同的 key。 对NULL键的特别处理：putForNullKey()我们直接看其源码： 1234567891011121314151617/** * Offloaded version of put for null keys */private V putForNullKey(V value) &#123; // 若key==null，则将其放入table的第一个桶，即 table[0] for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) &#123; // 若已经存在key为null的键，则替换其值，并返回旧值 V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; // 快速失败 addEntry(0, null, value, 0); // 否则，将其添加到 table[0] 的桶中 return null;&#125; 通过上述源码我们可以清楚知到，HashMap 中可以保存键为NULL的键值对，且该键值对是唯一的。若再次向其中添加键为NULL的键值对，将覆盖其原值。此外，如果HashMap中存在键为NULL的键值对，那么一定在第一个桶中。 HashMap 中的哈希策略（算法） 在上述的 put(key,vlaue) 方法的源码中，我们标出了 HashMap 中的哈希策略（即(1)、(2)两处），hash() 方法用于对Key的hashCode进行重新计算，而 indexFor() 方法用于生成这个Entry对象的插入位置。当计算出来的hash值与hashMap的(length-1)做了&amp;运算后，会得到位于区间[0，length-1]的一个值。特别地，这个值分布的越均匀， HashMap 的空间利用率也就越高，存取效率也就越好。 我们首先看(1)处的 hash() 方法，该方法为一个纯粹的数学计算，用于进一步计算key的hash值，源码如下： 12345678910111213141516/** * Applies a supplemental hash function to a given hashCode, which * defends against poor quality hash functions. This is critical * because HashMap uses power-of-two length hash tables, that * otherwise encounter collisions for hashCodes that do not differ * in lower bits. * * Note: Null keys always map to hash 0, thus index 0. */static int hash(int h) &#123; // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125;12345678910111213141516 正如JDK官方对该方法的描述那样，使用hash()方法对一个对象的hashCode进行重新计算是为了防止质量低下的hashCode()函数实现。由于hashMap的支撑数组长度总是 2 的幂次，通过右移可以使低位的数据尽量的不同，从而使hash值的分布尽量均匀。 通过上述hash()方法计算得到 Key 的 hash值 后，怎么才能保证元素均匀分布到table的每个桶中呢？我们会想到取模，但是由于取模的效率较低，HashMap 是通过调用(2)处的indexFor()方法处理的，其不但简单而且效率很高，对应源码如下所示： 123456/** * Returns index for hash code h. */static int indexFor(int h, int length) &#123; return h &amp; (length-1); // 作用等价于取模运算，但这种方式效率更高&#125; 我们知道，HashMap的底层数组长度总是2的n次方。当length为2的n次方时，h&amp;(length - 1)就相当于对length取模，而且速度比直接取模要快得多，这是HashMap在速度上的一个优化。至于HashMap的底层数组长度为什么是2的n次方，下一节将给出解释。 总而言之，上述的hash()方法和indexFor()方法的作用只有一个：保证元素均匀分布到table的每个桶中以便充分利用空间。 HashMap 中键值对的添加：addEntry()我们直接看其源码： 123456789101112131415161718192021 /** * Adds a new entry with the specified key, value and hash code to * the specified bucket. It is the responsibility of this * method to resize the table if appropriate. * * Subclass overrides this to alter the behavior of put method. * * 永远都是在链表的表头添加新元素 */void addEntry(int hash, K key, V value, int bucketIndex) &#123; //获取bucketIndex处的链表 Entry&lt;K,V&gt; e = table[bucketIndex]; //将新创建的 Entry 链入 bucketIndex处的链表的表头 table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); //若HashMap中元素的个数超过极限值 threshold，则容量扩大两倍 if (size++ &gt;= threshold) resize(2 * table.length);&#125; 通过上述源码我们可以清楚地了解到 链的产生时机。HashMap 总是将新的Entry对象添加到bucketIndex处，若bucketIndex处已经有了Entry对象，那么新添加的Entry对象将指向原有的Entry对象，并形成一条新的以它为链头的Entry链；但是，若bucketIndex处原先没有Entry对象，那么新添加的Entry对象将指向 null，也就生成了一条长度为 1 的全新的Entry链了。HashMap 永远都是在链表的表头添加新元素。此外，若HashMap中元素的个数超过极限值 threshold，其将进行扩容操作，一般情况下，容量将扩大至原来的两倍。 HashMap 的扩容：resize() 随着HashMap中元素的数量越来越多，发生碰撞的概率将越来越大，所产生的子链长度就会越来越长，这样势必会影响HashMap的存取速度。为了保证HashMap的效率，系统必须要在某个临界点进行扩容处理，该临界点就是HashMap中元素的数量在数值上等于threshold（table数组长度*加载因子）。但是，不得不说，扩容是一个非常耗时的过程，因为它需要重新计算这些元素在新table数组中的位置并进行复制处理。所以，如果我们能够提前预知HashMap 中元素的个数，那么在构造HashMap时预设元素的个数能够有效的提高HashMap的性能。我们直接看其源码： 123456789101112131415161718192021222324252627282930313233 /** * Rehashes the contents of this map into a new array with a * larger capacity. This method is called automatically when the * number of keys in this map reaches its threshold. * * If current capacity is MAXIMUM_CAPACITY, this method does not * resize the map, but sets threshold to Integer.MAX_VALUE. * This has the effect of preventing future calls. * * @param newCapacity the new capacity, MUST be a power of two; * must be greater than current capacity unless current * capacity is MAXIMUM_CAPACITY (in which case value * is irrelevant). */void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; // 若 oldCapacity 已达到最大值，直接将 threshold 设为 Integer.MAX_VALUE if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; // 直接返回 &#125; // 否则，创建一个更大的数组 Entry[] newTable = new Entry[newCapacity]; //将每条Entry重新哈希到新的数组中 transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor); // 重新设定 threshold&#125; 该方法的作用及触发动机如下： Rehashes the contents of this map into a new array with a larger capacity. This method is called automatically when the number of keys in this map reaches its threshold. HashMap 的重哈希：transfer() 重哈希的主要是一个重新计算原HashMap中的元素在新table数组中的位置并进行复制处理的过程，我们直接看其源码： 1234567891011121314151617181920212223242526272829/** * Transfers all entries from current table to newTable. */void transfer(Entry[] newTable) &#123; // 将原数组 table 赋给数组 src Entry[] src = table; int newCapacity = newTable.length; // 将数组 src 中的每条链重新添加到 newTable 中 for (int j = 0; j &lt; src.length; j++) &#123; Entry&lt;K,V&gt; e = src[j]; if (e != null) &#123; src[j] = null; // src 回收 // 将每条链的每个元素依次添加到 newTable 中相应的桶中 do &#123; Entry&lt;K,V&gt; next = e.next; // e.hash指的是 hash(key.hashCode())的返回值; // 计算在newTable中的位置，注意原来在同一条子链上的元素可能被分配到不同的子链 int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; while (e != null); &#125; &#125;&#125; 特别需要注意的是，在重哈希的过程中，原属于一个桶中的Entry对象可能被分到不同的桶，因为HashMap 的容量发生了变化，那么 h&amp;(length - 1) 的值也会发生相应的变化。极端地说，如果重哈希后，原属于一个桶中的Entry对象仍属于同一桶，那么重哈希也就失去了意义。 HashMap 的读取实现 相对于HashMap的存储而言，读取就显得比较简单了。因为，HashMap只需通过key的hash值定位到table数组的某个特定的桶，然后查找并返回该key对应的value即可，源码如下： 123456789101112131415161718192021222324252627282930313233343536/** * Returns the value to which the specified key is mapped, * or &#123;@code null&#125; if this map contains no mapping for the key. * * &lt;p&gt;More formally, if this map contains a mapping from a key * &#123;@code k&#125; to a value &#123;@code v&#125; such that &#123;@code (key==null ? k==null : * key.equals(k))&#125;, then this method returns &#123;@code v&#125;; otherwise * it returns &#123;@code null&#125;. (There can be at most one such mapping.) * * &lt;p&gt;A return value of &#123;@code null&#125; does not &lt;i&gt;necessarily&lt;/i&gt; * indicate that the map contains no mapping for the key; it's also * possible that the map explicitly maps the key to &#123;@code null&#125;. * The &#123;@link #containsKey containsKey&#125; operation may be used to * distinguish these two cases. * * @see #put(Object, Object) */ public V get(Object key) &#123; // 若为null，调用getForNullKey方法返回相对应的value if (key == null) // 从table的第一个桶中寻找 key 为 null 的映射；若不存在，直接返回null return getForNullKey(); // 根据该 key 的 hashCode 值计算它的 hash 码 int hash = hash(key.hashCode()); // 找出 table 数组中对应的桶 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; //若搜索的key与查找的key相同，则返回相对应的value if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) return e.value; &#125; return null; &#125; 在这里能够根据key快速的取到value，除了和HashMap的数据结构密不可分外，还和Entry有莫大的关系。在前面就已经提到过，HashMap在存储过程中并没有将key，value分开来存储，而是当做一个整体key-value来处理的，这个整体就是Entry对象。特别地，在Entry对象中，value的地位要比key低一些，相当于是 key 的附属。 其中，针对键为NULL的键值对，HashMap 提供了专门的处理：getForNullKey()，其源码如下： 12345678910111213141516/** * Offloaded version of get() to look up null keys. Null keys map * to index 0. This null case is split out into separate methods * for the sake of performance in the two most commonly used * operations (get and put), but incorporated with conditionals in * others. */ private V getForNullKey() &#123; // 键为NULL的键值对若存在，则必定在第一个桶中 for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) return e.value; &#125; // 键为NULL的键值对若不存在，则直接返回 null return null; &#125; 因此，调用HashMap的get(Object key)方法后，若返回值是 NULL，则存在如下两种可能： 该 key 对应的值就是 null; HashMap 中不存在该 key。 HashMap 存取小结 在存储的过程中，系统根据key的hash值来定位Entry在table数组中的哪个桶，并且将其放到对应的链表的链头；在取的过程中，同样根据key的hash值来定位Entry在table数组中的哪个桶，然后在该桶中查找并返回。 HashMap 的底层数组长度为何总是2的n次方？ 我们知道，HashMap的底层数组长度总是2的n次方，原因是 HashMap 在其构造函数 HashMap(int initialCapacity, float loadFactor) 中作了特别的处理，如下面的代码所示。当底层数组的length为2的n次方时， h&amp;(length - 1) 就相当于对length取模，其效率要比直接取模高得多，这是HashMap在效率上的一个优化。 1234// HashMap 的容量必须是2的幂次方，超过 initialCapacity 的最小 2^n int capacity = 1;while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; 在上文已经提到过，HashMap 中的数据结构是一个数组链表，我们希望的是元素存放的越均匀越好。最理想的效果是，Entry数组中每个位置都只有一个元素，这样，查询的时候效率最高，不需要遍历单链表，也不需要通过equals去比较Key，而且空间利用率最大。 那如何计算才会分布最均匀呢？正如上一节提到的，HashMap采用了一个分两步走的哈希策略： 使用 hash() 方法用于对Key的hashCode进行重新计算，以防止质量低下的hashCode()函数实现。由于hashMap的支撑数组长度总是 2 的倍数，通过右移可以使低位的数据尽量的不同，从而使Key的hash值的分布尽量均匀； 使用 indexFor() 方法进行取余运算，以使Entry对象的插入位置尽量分布均匀(下文将专门对此阐述)。 对于取余运算，我们首先想到的是：哈希值%length = bucketIndex。但当底层数组的length为2的n次方时， h&amp;(length - 1) 就相当于对length取模，而且速度比直接取模快得多，这是HashMap在速度上的一个优化。除此之外，HashMap 的底层数组长度总是2的n次方的主要原因是什么呢？我们借助于 chenssy 在其博客《java提高篇（二三）—–HashMap》 中的关于这个问题的阐述： 这里，我们假设length分别为16(2^4) 和 15，h 分别为 5、6、7。 我们可以看到，当n=15时，6和7的结果一样，即它们位于table的同一个桶中，也就是产生了碰撞，6、7就会在这个桶中形成链表，这样就会导致查询速度降低。诚然这里只分析三个数字不是很多，那么我们再看 h 分别取 0-15时的情况。 从上面的图表中我们可以看到，当 length 为15时总共发生了8次碰撞，同时发现空间浪费非常大，因为在 1、3、5、7、9、11、13、15 这八处没有存放数据。这是因为hash值在与14（即 1110）进行&amp;运算时，得到的结果最后一位永远都是0，即 0001、0011、0101、0111、1001、1011、1101、1111位置处是不可能存储数据的。这样，空间的减少会导致碰撞几率的进一步增加，从而就会导致查询速度慢。 而当length为16时，length – 1 = 15， 即 1111，那么，在进行低位&amp;运算时，值总是与原来hash值相同，而进行高位运算时，其值等于其低位值。所以，当 length=2^n 时，不同的hash值发生碰撞的概率比较小，这样就会使得数据在table数组中分布较均匀，查询速度也较快。 因此，总的来说，HashMap 的底层数组长度总是2的n次方的原因有两个，即当 length=2^n 时： 不同的hash值发生碰撞的概率比较小，这样就会使得数据在table数组中分布较均匀，空间利用率较高，查询速度也较快； h&amp;(length - 1) 就相当于对length取模，而且在速度、效率上比直接取模要快得多，即二者是等价不等效的，这是HashMap在速度和效率上的一个优化。 参考： https://blog.csdn.net/justloveyou_/article/details/62893086 http://alex09.iteye.com/blog/539545/ http://www.cnblogs.com/chenssy/p/3521565.html]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java GC]]></title>
    <url>%2F2018%2F07%2F05%2FJava-GC%2F</url>
    <content type="text"><![CDATA[范围：要回收哪些区域​ 在JVM五种内存模型中，有三个是不需要进行垃圾回收的：程序计数器、JVM栈、本地方法栈。因为它们的生命周期是和线程同步的，随着线程的销毁，它们占用的内存会自动释放，所以只有方法区和堆需要进行GC。 前提：如何判断对象已死​ 所有的垃圾收集算法都面临同一个问题，那就是找出应用程序不可到达的内存块，将其释放，这里面讲的不可达主要是指应用程序已经没有内存块的引用了， 在Java中，某个对象对应用程序是可到达的是指：这个对象被根（根主要是指类的静态变量，或者活跃在所有线程栈的对象的引用）引用或者对象被另一个可到达的对象引用。 引用计数算法​ 引用计数是最简单直接的一种方式，这种方式在每一个对象中增加一个引用的计数，这个计数代表当前程序有多少个引用引用了此对象，如果此对象的引用计数变为0，那么此对象就可以作为垃圾收集器的目标对象来收集。 优点：简单，直接，不需要暂停整个应用。 缺点：1.需要编译器的配合，编译器要生成特殊的指令来进行引用计数的操作；2.不能处理循环引用的问题。 因此这种方法是垃圾收集的早期策略，现在很少使用。Sun的JVM并没有采用引用计数算法来进行垃圾回收，而是基于根搜索算法的。 可达性分析算法（根搜索算法）​ 通过一系列的名为“GC Root”的对象作为起点，从这些节点向下搜索，搜索所走过的路径称为引用链(Reference Chain)，当一个对象到GC Root没有任何引用链相连时，则该对象不可达，该对象是不可使用的，垃圾收集器将回收其所占的内存。 ​ 在java语言中，可作为GCRoot的对象包括以下几种：a. java虚拟机栈(栈帧中的本地变量表)中的引用的对象。b.方法区中的类静态属性引用的对象。c.方法区中的常量引用的对象。d.本地方法栈中JNI本地方法的引用对象。 四种引用​ GC在收集一个对象的时候会判断是否有引用指向对象，在JAVA中的引用主要有四种： 强引用（Strong Reference）​ 强引用是使用最普遍的引用。如果一个对象具有强引用，那垃圾回收器绝不会回收它。当内存空间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足的问题。 软引用（Soft Reference）​ 如果一个对象只具有软引用，则内存空间足够，垃圾回收器就不会回收它；如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。 下面举个例子，假如有一个应用需要读取大量的本地图片，如果每次读取图片都从硬盘读取，则会严重影响性能，但是如果全部加载到内存当中，又有可能造成内存溢出，此时使用软引用可以解决这个问题。 设计思路是：用一个HashMap来保存图片的路径和相应图片对象关联的软引用之间的映射关系，在内存不足时，JVM会自动回收这些缓存图片对象所占用的空间，从而有效地避免了内存溢出的问题。软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收器回收，Java虚拟机就会把这个软引用加入到与之关联的引用队列中。 弱引用（Weak Reference）​ 弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程，因此不一定会很快发现那些只具有弱引用的对象。 ​ 弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。 虚引用（PhantomReference）​ 虚应用也称为幽灵应用或者幻影应用，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。在JDK 1.2之后，提供了PhantomReference类来实现虚引用。]]></content>
      <categories>
        <category>Java虚拟机</category>
      </categories>
      <tags>
        <tag>垃圾回收</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式之单例模式]]></title>
    <url>%2F2018%2F06%2F13%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[懒汉式(支持多线程)123456789101112131415public class SingletonLazyLoad &#123; private volatile static SingletonLazyLoad singleton; private SingletonLazyLoad() &#123; &#125; public static SingletonLazyLoad getInstance() &#123; if (singleton == null) &#123; synchronized(SingletonLazyLoad.class) &#123; if (singleton == null) &#123; singleton=new SingletonLazyLoad(); &#125; &#125; &#125; return singleton; &#125;&#125; 饿汉式(支持多线程)12345678public class Singleton &#123; private static final Singleton instance=new Singleton(); private Singleton()&#123; &#125; public static Singleton getInstance()&#123; return instance; &#125;&#125; 静态内部类123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton () &#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 枚举12345public enum Singleton &#123; INSTANCE; public void whateverMethod() &#123; &#125; &#125; 总结在所有的单例实现方式中，枚举是一种在代码写法上最简单的方式，之所以代码十分简洁，是因为Java给我们提供了enum关键字，我们便可以很方便的声明一个枚举类型，而不需要关心其初始化过程中的线程安全问题，因为枚举类在被虚拟机加载的时候会保证线程安全的被初始化。 使用非枚举的方式实现单例，都要自己来保证线程安全，所以，这就导致其他方法必然是比较臃肿的。 除此之外，在序列化方面，Java中有明确规定，枚举的序列化和反序列化是有特殊定制的。这就可以避免反序列化过程中由于反射而导致的单例被破坏问题。 普通的Java类的反序列化过程中，会通过反射调用类的默认构造函数来初始化对象。所以，即使单例中构造函数是私有的，也会被反射给破坏掉。由于反序列化后的对象是重新new出来的，所以这就破坏了单例。 但是，枚举的反序列化并不是通过反射实现的。所以，也就不会发生由于反序列化导致的单例破坏问题。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程池：使用Executors和ThreadPoolExecutor]]></title>
    <url>%2F2018%2F06%2F05%2F%E7%BA%BF%E7%A8%8B%E6%B1%A0%EF%BC%9A%E4%BD%BF%E7%94%A8Executors%E5%92%8CThreadPoolExecutor%2F</url>
    <content type="text"><![CDATA[阿里发布的 Java开发手册中强制线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。 问：为什么很多 Java 规范都建议不要显式的创建 Thread，而使用线程池？ 答：因为使用线程池的好处是减少在创建和销毁线程上所消耗的时间和系统资源，解决资源不足的问题，如果不使用线程池，有可能造成系统创建大量同类线程而导致消耗完内存或者过渡切换问题。 问：为什么不建议在代码中直接使用 Executors 创建线程池，而是推荐通过 ThreadPoolExecutor 方式创建？ 答：其实不直接使用工具类的目的只有一个，那就是可以明确的让我们知道线程池的运行规则，避免使用工具类的包装而不够直观内部机制而导致潜在的问题。譬如使用 Executors 的 FixedThreadPool 和 SingleThreadPool 创建线程池的原理都允许请求的队列长度为 Integer 的最大值，这样的话可能会堆积大量的请求导致 OOM；而使用 Executors 的 CachedThreadPool 和 ScheduledThreadPool 创建线程池的原理都允许创建线程数量为 Integer 的最大值，这样的话可能会导致创建大量的线程而导致 OOM，所以推荐直接通过明确的构造参数创建线程池，这样就相当与时刻提醒自己的线程池特性是什么。 线程池负责管理工作线程，包含一个等待执行的任务队列。线程池的任务队列是一个Runnable集合，工作线程负责从任务队列中取出并执行Runnable对象。 java.util.concurrent.executors 提供了 java.util.concurrent.executor 接口的一个Java实现，可以创建线程池。下面是一个简单示例： 首先创建一个Runable 类： WorkerThread.java 12345678910111213141516171819202122232425262728public class WorkerThread implements Runnable &#123; private String command; public WorkerThread(String s)&#123; this.command=s; &#125; @Override public void run() &#123; System.out.println(Thread.currentThread().getName()+" Start. Command = "+command); processCommand(); System.out.println(Thread.currentThread().getName()+" End."); &#125; private void processCommand() &#123; try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; @Override public String toString()&#123; return this.command; &#125;&#125; 下面是一个测试程序，从 Executors 框架中创建固定大小的线程池： SimpleThreadPool.java 123456789101112131415161718import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors; public class SimpleThreadPool &#123; public static void main(String[] args) &#123; ExecutorService executor = Executors.newFixedThreadPool(5); for (int i = 0; i &lt; 10; i++) &#123; Runnable worker = new WorkerThread("" + i); executor.execute(worker); &#125; executor.shutdown(); while (!executor.isTerminated()) &#123; &#125; System.out.println("Finished all threads"); &#125; &#125; 在上面的程序中，我们创建了包含5个工作线程的固定大小线程池。然后，我们向线程池提交10个任务。由于线程池的大小是5，因此首先会启动5个工作线程，其他任务将进行等待。一旦有任务结束，工作线程会从等待队列中挑选下一个任务并开始执行。 以上程序的输出结果如下： 123456789101112131415161718192021pool-1-thread-2 Start. Command = 1pool-1-thread-4 Start. Command = 3pool-1-thread-1 Start. Command = 0pool-1-thread-3 Start. Command = 2pool-1-thread-5 Start. Command = 4pool-1-thread-4 End.pool-1-thread-5 End.pool-1-thread-1 End.pool-1-thread-3 End.pool-1-thread-3 Start. Command = 8pool-1-thread-2 End.pool-1-thread-2 Start. Command = 9pool-1-thread-1 Start. Command = 7pool-1-thread-5 Start. Command = 6pool-1-thread-4 Start. Command = 5pool-1-thread-2 End.pool-1-thread-4 End.pool-1-thread-3 End.pool-1-thread-5 End.pool-1-thread-1 End.Finished all threads 从输出结果看，线程池中有五个名为“pool-1-thread-1”…“pool-1-thread-5”的工作线程负责执行提交的任务。 Executors 类使用 ExecutorService 提供了一个 ThreadPoolExecutor 的简单实现，但 ThreadPoolExecutor 提供的功能远不止这些。我们可以指定创建 ThreadPoolExecutor 实例时活跃的线程数，并且可以限制线程池的大小，还可以创建自己的 RejectedExecutionHandler 实现来处理不适合放在工作队列里的任务。 下面是一个 RejectedExecutionHandler 接口的自定义实现： RejectedExecutionHandlerImpl.java 1234567891011import java.util.concurrent.RejectedExecutionHandler;import java.util.concurrent.ThreadPoolExecutor; public class RejectedExecutionHandlerImpl implements RejectedExecutionHandler &#123; @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) &#123; System.out.println(r.toString() + " is rejected"); &#125; &#125; ThreadPoolExecutor 提供了一些方法，可以查看执行状态、线程池大小、活动线程数和任务数。所以，我通过一个监视线程在固定间隔输出执行信息。 MyMonitorThread.java 123456789101112131415161718192021222324252627282930313233343536373839404142import java.util.concurrent.ThreadPoolExecutor; public class MyMonitorThread implements Runnable&#123; private ThreadPoolExecutor executor; private int seconds; private boolean run=true; public MyMonitorThread(ThreadPoolExecutor executor, int delay) &#123; this.executor = executor; this.seconds=delay; &#125; public void shutdown()&#123; this.run=false; &#125; @Override public void run() &#123; while(run)&#123; System.out.println( String.format("[monitor] [%d/%d] Active: %d, Completed: %d, Task: %d, isShutdown: %s, isTerminated: %s", this.executor.getPoolSize(), this.executor.getCorePoolSize(), this.executor.getActiveCount(), this.executor.getCompletedTaskCount(), this.executor.getTaskCount(), this.executor.isShutdown(), this.executor.isTerminated())); try &#123; Thread.sleep(seconds*1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 下面是使用 ThreadPoolExecutor 的线程池实现示例： WorkerPool.java 123456789101112131415161718192021222324252627282930313233import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.Executors;import java.util.concurrent.ThreadFactory;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit; public class WorkerPool &#123; public static void main(String args[]) throws InterruptedException&#123; //RejectedExecutionHandler implementation RejectedExecutionHandlerImpl rejectionHandler = new RejectedExecutionHandlerImpl(); //Get the ThreadFactory implementation to use ThreadFactory threadFactory = Executors.defaultThreadFactory(); //creating the ThreadPoolExecutor ThreadPoolExecutor executorPool = new ThreadPoolExecutor(2, 4, 10, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(2), threadFactory, rejectionHandler); //start the monitoring thread MyMonitorThread monitor = new MyMonitorThread(executorPool, 3); Thread monitorThread = new Thread(monitor); monitorThread.start(); //submit work to the thread pool for(int i=0; i&lt;10; i++)&#123; executorPool.execute(new WorkerThread("cmd"+i)); &#125; Thread.sleep(30000); //shut down the pool executorPool.shutdown(); //shut down the monitor thread Thread.sleep(5000); monitor.shutdown(); &#125;&#125; 请注意：在初始化 ThreadPoolExecutor 时，初始线程池大小设为2、最大值设为4、工作队列大小设为2。所以，如果当前有4个任务正在运行而此时又有新任务提交，工作队列将只存储2个任务和其他任务将交由RejectedExecutionHandlerImpl 处理。 程序执行的结果如下，确认了上面的结论： 12345678910111213141516171819202122232425262728cmd6 is rejectedcmd7 is rejectedcmd8 is rejectedcmd9 is rejectedpool-1-thread-1Start. Command=cmd0pool-1-thread-2Start. Command=cmd1pool-1-thread-3Start. Command=cmd4pool-1-thread-4Start. Command=cmd5[monitor] [0/2] Active: 0, Completed: 0, Task: 6, isShutdown: false, isTerminated: false[monitor] [4/2] Active: 4, Completed: 0, Task: 6, isShutdown: false, isTerminated: falsepool-1-thread-2End. pool-1-thread-1End. pool-1-thread-2Start. Command=cmd2pool-1-thread-1Start. Command=cmd3pool-1-thread-3End. pool-1-thread-4End. [monitor] [4/2] Active: 2, Completed: 4, Task: 6, isShutdown: false, isTerminated: false[monitor] [4/2] Active: 2, Completed: 4, Task: 6, isShutdown: false, isTerminated: falsepool-1-thread-2End. pool-1-thread-1End. [monitor] [4/2] Active: 0, Completed: 6, Task: 6, isShutdown: false, isTerminated: false[monitor] [2/2] Active: 0, Completed: 6, Task: 6, isShutdown: false, isTerminated: false[monitor] [2/2] Active: 0, Completed: 6, Task: 6, isShutdown: false, isTerminated: false[monitor] [2/2] Active: 0, Completed: 6, Task: 6, isShutdown: false, isTerminated: false[monitor] [2/2] Active: 0, Completed: 6, Task: 6, isShutdown: false, isTerminated: false[monitor] [2/2] Active: 0, Completed: 6, Task: 6, isShutdown: false, isTerminated: false[monitor] [0/2] Active: 0, Completed: 6, Task: 6, isShutdown: true, isTerminated: true[monitor] [0/2] Active: 0, Completed: 6, Task: 6, isShutdown: true, isTerminated: true 请注意活跃线程、已完成线程和任务完成总数的变化。我们可以调用 shutdown() 结束所有已提交任务并终止线程池。 ThreadPoolExecutor 先看看如何使用ThreadPoolExecutor创建线程池： 1234567publicThreadPoolExecutor(intcorePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) 线程池的构造函数参数多达7个，现在我们一一来分析它们对线程池的影响。 ​ corePoolSize：线程池中核心线程数的最大值 ​ maximumPoolSize：线程池中能拥有最多线程数 ​ workQueue：用于缓存任务的阻塞队列 ​ 我们现在通过向线程池添加新的任务来说明着三者之间的关系。 ​ （1）如果没有空闲的线程执行该任务且当前运行的线程数少于corePoolSize，则添加新的线程执行该任务。 ​ （2）如果没有空闲的线程执行该任务且当前的线程数等于corePoolSize同时阻塞队列未满，则将任务入队列，而不添加新的线程。 ​ （3）如果没有空闲的线程执行该任务且阻塞队列已满同时池中的线程数小于maximumPoolSize，则创建新的线程执行任务。 ​ （4）如果没有空闲的线程执行该任务且阻塞队列已满同时池中的线程数等于maximumPoolSize，则根据构造函数中的handler指定的策略来拒绝新的任务。 ​ 注意，线程池并没有标记哪个线程是核心线程，哪个是非核心线程，线程池只关心核心线程的数量。 ​ 通俗解释，如果把线程池比作一个单位的话，corePoolSize就表示正式工，线程就可以表示一个员工。当我们向单位委派一项工作时，如果单位发现正式工还没招满，单位就会招个正式工来完成这项工作。随着我们向这个单位委派的工作增多，即使正式工全部满了，工作还是干不完，那么单位只能按照我们新委派的工作按先后顺序将它们找个地方搁置起来，这个地方就是workQueue，等正式工完成了手上的工作，就到这里来取新的任务。如果不巧，年末了，各个部门都向这个单位委派任务，导致workQueue已经没有空位置放新的任务，于是单位决定招点临时工吧（临时工：又是我！）。临时工也不是想招多少就找多少，上级部门通过这个单位的maximumPoolSize确定了你这个单位的人数的最大值，换句话说最多招maximumPoolSize–corePoolSize个临时工。当然，在线程池中，谁是正式工，谁是临时工是没有区别，完全同工同酬。 ​ keepAliveTime：表示空闲线程的存活时间。 ​ TimeUnitunit：表示keepAliveTime的单位。 ​ 为了解释keepAliveTime的作用，我们在上述情况下做一种假设。假设线程池这个单位已经招了些临时工，但新任务没有继续增加，所以随着每个员工忙完手头的工作，都来workQueue领取新的任务（看看这个单位的员工多自觉啊）。随着各个员工齐心协力，任务越来越少，员工数没变，那么就必定有闲着没事干的员工。这样的话领导不乐意啦，但是又不能轻易fire没事干的员工，因为随时可能有新任务来，于是领导想了个办法，设定了keepAliveTime，当空闲的员工在keepAliveTime这段时间还没有找到事情干，就被辞退啦，毕竟地主家也没有余粮啊！当然辞退到corePoolSize个员工时就不再辞退了，领导也不想当光杆司令啊！ ​ handler：表示当workQueue已满，且池中的线程数达到maximumPoolSize时，线程池拒绝添加新任务时采取的策略。 为了解释handler的作用，我们在上述情况下做另一种假设。假设线程池这个单位招满临时工，但新任务依然继续增加，线程池从上到下，从里到外真心忙的不可开交，阻塞队列也满了，只好拒绝上级委派下来的任务。怎么拒绝是门艺术，handler一般可以采取以下四种取值。 ThreadPoolExecutor.AbortPolicy() 抛出RejectedExecutionException异常 ThreadPoolExecutor.CallerRunsPolicy() 由向线程池提交任务的线程来执行该任务 ThreadPoolExecutor.DiscardOldestPolicy() 抛弃最旧的任务（最先提交而没有得到执行的任务） ThreadPoolExecutor.DiscardPolicy() 抛弃当前的任务 ​ threadFactory：指定创建线程的工厂。 线程池中的线程数量 ​ 默认情况下，当池中有空闲线程，且线程的数量大于corePoolSize时，空闲时间超过keepAliveTime的线程会自行销毁，池中仅仅会保留corePoolSize个线程。如果线程池中调用了allowCoreThreadTimeOut这个方法，则空闲时间超过keepAliveTime的线程全部都会自行销毁，而不必理会corePoolSize这个参数。 ​ 如果池中的线程数量小于corePoolSize时，调用prestartAllCoreThreads方法，则无论是否有待执行的任务，线程池都会创建新的线程，直到池中线程数量达到corePoolSize。 参考： http://www.importnew.com/8542.html https://blog.csdn.net/qq_33300570/article/details/78394188 https://www.cnblogs.com/nullzx/p/5184164.html https://blog.csdn.net/linghu_java/article/details/17123057]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>内存</tag>
        <tag>线程</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解Java的接口和抽象类]]></title>
    <url>%2F2018%2F05%2F30%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E7%9A%84%E6%8E%A5%E5%8F%A3%E5%92%8C%E6%8A%BD%E8%B1%A1%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[原文出处： 海 子 对于面向对象编程来说，抽象是它的一大特征之一。在Java中，可以通过两种形式来体现OOP的抽象：接口和抽象类。这两者有太多相似的地方，又有太多不同的地方。很多人在初学的时候会以为它们可以随意互换使用，但是实际则不然。今天我们就一起来学习一下Java中的接口和抽象类。 抽象类在了解抽象类之前，先来了解一下抽象方法。抽象方法是一种特殊的方法：它只有声明，而没有具体的实现。抽象方法的声明格式为： 1abstract void fun(); 抽象方法必须用abstract关键字进行修饰。如果一个类含有抽象方法，则称这个类为抽象类，抽象类必须在类前用abstract关键字修饰。因为抽象类中含有无具体实现的方法，所以不能用抽象类创建对象。 下面要注意一个问题：在《Java编程思想》一书中，将抽象类定义为“包含抽象方法的类”，但是后面发现如果一个类不包含抽象方法，只是用abstract修饰的话也是抽象类。也就是说抽象类不一定必须含有抽象方法。个人觉得这个属于钻牛角尖的问题吧，因为如果一个抽象类不包含任何抽象方法，为何还要设计为抽象类？所以暂且记住这个概念吧，不必去深究为什么。 123[public] abstract class ClassName &#123; abstract void fun();&#125; 从这里可以看出，抽象类就是为了继承而存在的，如果你定义了一个抽象类，却不去继承它，那么等于白白创建了这个抽象类，因为你不能用它来做任何事情。对于一个父类，如果它的某个方法在父类中实现出来没有任何意义，必须根据子类的实际需求来进行不同的实现，那么就可以将这个方法声明为abstract方法，此时这个类也就成为abstract类了。 包含抽象方法的类称为抽象类，但并不意味着抽象类中只能有抽象方法，它和普通类一样，同样可以拥有成员变量和普通的成员方法。注意，抽象类和普通类的主要有三点区别： 1）抽象方法必须为public或者protected（因为如果为private，则不能被子类继承，子类便无法实现该方法），缺省情况下默认为public。 2）抽象类不能用来创建对象； 3）如果一个类继承于一个抽象类，则子类必须实现父类的抽象方法。如果子类没有实现父类的抽象方法，则必须将子类也定义为为abstract类。 在其他方面，抽象类和普通的类并没有区别。 接口接口，英文称作interface，在软件工程中，接口泛指供别人调用的方法或者函数。从这里，我们可以体会到Java语言设计者的初衷，它是对行为的抽象。在Java中，定一个接口的形式如下： 123[public] interface InterfaceName &#123; &#125; 接口中可以含有 变量和方法。但是要注意，接口中的变量会被隐式地指定为public static final变量（并且只能是public static final变量，用private修饰会报编译错误），而方法会被隐式地指定为public abstract方法且只能是public abstract方法（用其他关键字，比如private、protected、static、 final等修饰会报编译错误），并且接口中所有的方法不能有具体的实现（声明下，java8中有新特性，接口中可以有具体的实现方法，需要以default关键字开头 ），也就是说，接口中的方法必须都是抽象方法。从这里可以隐约看出接口和抽象类的区别，接口是一种极度抽象的类型，它比抽象类更加“抽象”，并且一般情况下不在接口中定义变量。 要让一个类遵循某组特地的接口需要使用implements关键字，具体格式如下： 12class ClassName implements Interface1,Interface2,[....]&#123;&#125; 可以看出，允许一个类遵循多个特定的接口。如果一个非抽象类遵循了某个接口，就必须实现该接口中的所有方法。对于遵循某个接口的抽象类，可以不实现该接口中的抽象方法。 抽象类和接口的区别1.语法层面上的区别 1）抽象类可以提供成员方法的实现细节，而接口中只能存在public abstract 方法； 2）抽象类中的成员变量可以是各种类型的，而接口中的成员变量只能是public static final类型的； 3）接口中不能含有静态代码块以及静态方法，而抽象类可以有静态代码块和静态方法； 4）一个类只能继承一个抽象类，而一个类却可以实现多个接口。 2.设计层面上的区别 1）抽象类是对一种事物的抽象，即对类抽象，而接口是对行为的抽象。抽象类是对整个类整体进行抽象，包括属性、行为，但是接口却是对类局部（行为）进行抽象。举个简单的例子，飞机和鸟是不同类的事物，但是它们都有一个共性，就是都会飞。那么在设计的时候，可以将飞机设计为一个类Airplane，将鸟设计为一个类Bird，但是不能将 飞行 这个特性也设计为类，因此它只是一个行为特性，并不是对一类事物的抽象描述。此时可以将 飞行 设计为一个接口Fly，包含方法fly( )，然后Airplane和Bird分别根据自己的需要实现Fly这个接口。然后至于有不同种类的飞机，比如战斗机、民用飞机等直接继承Airplane即可，对于鸟也是类似的，不同种类的鸟直接继承Bird类即可。从这里可以看出，继承是一个 “是不是”的关系，而 接口 实现则是 “有没有”的关系。如果一个类继承了某个抽象类，则子类必定是抽象类的种类，而接口实现则是有没有、具备不具备的关系，比如鸟是否能飞（或者是否具备飞行这个特点），能飞行则可以实现这个接口，不能飞行就不实现这个接口。 2）设计层面不同，抽象类作为很多子类的父类，它是一种模板式设计。而接口是一种行为规范，它是一种辐射式设计。什么是模板式设计？最简单例子，大家都用过ppt里面的模板，如果用模板A设计了ppt B和ppt C，ppt B和ppt C公共的部分就是模板A了，如果它们的公共部分需要改动，则只需要改动模板A就可以了，不需要重新对ppt B和ppt C进行改动。而辐射式设计，比如某个电梯都装了某种报警器，一旦要更新报警器，就必须全部更新。也就是说对于抽象类，如果需要添加新的方法，可以直接在抽象类中添加具体的实现，子类可以不进行变更；而对于接口则不行，如果接口进行了变更，则所有实现这个接口的类都必须进行相应的改动。 下面看一个网上流传最广泛的例子：门和警报的例子：门都有open( )和close( )两个动作，此时我们可以定义通过抽象类和接口来定义这个抽象概念： 1234abstract class Door &#123; public abstract void open(); public abstract void close();&#125; 或者： 1234interface Door &#123; public abstract void open(); public abstract void close();&#125; 但是现在如果我们需要门具有报警alarm( )的功能，那么该如何实现？下面提供两种思路： 1）将这三个功能都放在抽象类里面，但是这样一来所有继承于这个抽象类的子类都具备了报警功能，但是有的门并不一定具备报警功能； 2）将这三个功能都放在接口里面，需要用到报警功能的类就需要实现这个接口中的open( )和close( )，也许这个类根本就不具备open( )和close( )这两个功能，比如火灾报警器。 从这里可以看出， Door的open() 、close()和alarm()根本就属于两个不同范畴内的行为，open()和close()属于门本身固有的行为特性，而alarm()属于延伸的附加行为。因此最好的解决办法是单独将报警设计为一个接口，包含alarm()行为,Door设计为单独的一个抽象类，包含open和close两种行为。再设计一个报警门继承Door类和实现Alarm接口。 1234567891011121314151617181920interface Alram &#123; void alarm();&#125; abstract class Door &#123; void open(); void close();&#125; class AlarmDoor extends Door implements Alarm &#123; void oepn() &#123; //.... &#125; void close() &#123; //.... &#125; void alarm() &#123; //.... &#125;&#125; 抽象类和接口的对比 参数 抽象类 接口 默认的方法实现 它可以有默认的方法实现 接口完全是抽象的。它根本不存在方法的实现 实现 子类使用extends关键字来继承抽象类。如果子类不是抽象类的话，它需要提供抽象类中所有声明的方法的实现。 子类使用关键字implements来实现接口。它需要提供接口中所有声明的方法的实现 构造器 抽象类可以有构造器 接口不能有构造器 与正常Java类的区别 除了你不能实例化抽象类之外，它和普通Java类没有任何区别 接口是完全不同的类型 访问修饰符 抽象方法可以有public、protected和default这些修饰符 接口方法默认修饰符是public。你不可以使用其它修饰符。 main方法 抽象方法可以有main方法并且我们可以运行它 接口没有main方法，因此我们不能运行它。 多继承 抽象方法可以继承一个类和实现多个接口 接口只可以继承一个或多个其它接口 速度 它比接口速度要快 接口是稍微有点慢的，因为它需要时间去寻找在类中实现的方法。 添加新方法 如果你往抽象类中添加新的方法，你可以给它提供默认的实现。因此你不需要改变你现在的代码。 如果你往接口中添加方法，那么你必须改变实现该接口的类。 什么时候使用抽象类和接口 如果你拥有一些方法并且想让它们中的一些有默认实现，那么使用抽象类吧。 如果你想实现多重继承，那么你必须使用接口。由于Java不支持多继承，子类不能够继承多个类，但可以实现多个接口。因此你就可以使用接口来解决它。 如果基本功能在不断改变，那么就需要使用抽象类。如果不断改变基本功能并且使用接口，那么就需要改变所有实现了该接口的类。 Java8中的默认方法和静态方法Oracle已经开始尝试向接口中引入默认方法和静态方法，以此来减少抽象类和接口之间的差异。现在，我们可以为接口提供默认实现的方法了并且不用强制子类来实现它。]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关系数据库的第一第二第三范式与BCNF范式]]></title>
    <url>%2F2018%2F05%2F28%2F%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AC%AC%E4%BA%8C%E7%AC%AC%E4%B8%89%E8%8C%83%E5%BC%8F%E4%B8%8EBCNF%E8%8C%83%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[首先要明白”范式（NF）”是什么意思。按照教材中的定义，范式是“符合某一种级别的关系模式的集合，表示一个关系内部各属性之间的联系的合理化程度”。很晦涩吧？实际上你可以把它粗略地理解为一张数据表的表结构所符合的某种设计标准的级别。就像家里装修买建材，最环保的是E0级，其次是E1级，还有E2级等等。数据库范式也分为1NF，2NF，3NF，BCNF，4NF，5NF。一般在我们设计关系型数据库的时候，最多考虑到BCNF就够。符合高一级范式的设计，必定符合低一级范式，例如符合2NF的关系模式，必定符合1NF。 接下来就对每一级范式进行一下解释，首先是第一范式（1NF）。 符合1NF的关系（你可以理解为数据表。“关系模式”和“关系”的区别，类似于面向对象程序设计中”类“与”对象“的区别。”关系“是”关系模式“的一个实例，你可以把”关系”理解为一张带数据的表，而“关系模式”是这张数据表的表结构。1NF的定义为：符合1NF的关系中的每个属性都不可再分。表1所示的情况，就不符合1NF的要求。 表1所示的情况，就不符合1NF的要求。 实际上，1NF是所有关系型数据库的最基本要求，你在关系型数据库管理系统（RDBMS），例如SQL Server，Oracle，MySQL中创建数据表的时候，如果数据表的设计不符合这个最基本的要求，那么操作一定是不能成功的。也就是说，只要在RDBMS中已经存在的数据表，一定是符合1NF的。如果我们要在RDBMS中表现表中的数据，就得设计为表2的形式： 但是仅仅符合1NF的设计，仍然会存在数据冗余过大，插入异常，删除异常，修改异常的问题，例如对于表3中的设计： 每一名学生的学号、姓名、系名、系主任这些数据重复多次。每个系与对应的系主任的数据也重复多次——数据冗余过大 假如学校新建了一个系，但是暂时还没有招收任何学生（比如3月份就新建了，但要等到8月份才招生），那么是无法将系名与系主任的数据单独地添加到数据表中去的 （注１）——插入异常注１：根据三种关系完整性约束中实体完整性的要求，关系中的码（注２）所包含的任意一个属性都不能为空，所有属性的组合也不能重复。为了满足此要求，图中的表，只能将学号与课名的组合作为码，否则就无法唯一地区分每一条记录。注２：码：关系中的某个属性或者某几个属性的组合，用于区分每个元组（可以把“元组”理解为一张表中的每条记录，也就是每一行）。 假如将某个系中所有学生相关的记录都删除，那么所有系与系主任的数据也就随之消失了（一个系所有学生都没有了，并不表示这个系就没有了）。——删除异常 假如李小明转系到法律系，那么为了保证数据库中数据的一致性，需要修改三条记录中系与系主任的数据。——修改异常。 正因为仅符合1NF的数据库设计存在着这样那样的问题，我们需要提高设计标准，去掉导致上述四种问题的因素，使其符合更高一级的范式（2NF），这就是所谓的“规范化”。 第二范式（2NF）在关系理论中的严格定义我这里就不多介绍了（因为涉及到的铺垫比较多），只需要了解2NF对1NF进行了哪些改进即可。其改进是，2NF在1NF的基础之上，消除了非主属性对于码的部分函数依赖。接下来对这句话中涉及到的四个概念——“函数依赖”、“码”、“非主属性”、与“部分函数依赖”进行一下解释。 函数依赖我们可以这么理解（但并不是特别严格的定义）：若在一张表中，在属性（或属性组）X的值确定的情况下，必定能确定属性Y的值，那么就可以说Y函数依赖于X，写作 X → Y。也就是说，在数据表中，不存在任意两条记录，它们在X属性（或属性组）上的值相同，而在Y属性上的值不同。这也就是“函数依赖”名字的由来，类似于函数关系 y = f(x)，在x的值确定的情况下，y的值一定是确定的。 例如，对于表3中的数据，找不到任何一条记录，它们的学号相同而对应的姓名不同。所以我们可以说姓名函数依赖于学号，写作 学号 → 姓名。但是反过来，因为可能出现同名的学生，所以有可能不同的两条学生记录，它们在姓名上的值相同，但对应的学号不同，所以我们不能说学号函数依赖于姓名。表中其他的函数依赖关系还有如： 系名 → 系主任 学号 → 系主任 （学号，课名） → 分数 但以下函数依赖关系则不成立： 学号 → 课名 学号 → 分数 课名 → 系主任 （学号，课名） → 姓名 从“函数依赖”这个概念展开，还会有三个概念： 完全函数依赖 在一张表中，若 X → Y，且对于 X 的任何一个真子集（假如属性组 X 包含超过一个属性的话），X ‘ → Y 不成立，那么我们称 Y 对于 X 完全函数依赖，记作 X F→ Y。（那个F应该写在箭头的正上方，没办法打出来……，正确的写法如图1） 例如： 学号 F→ 姓名 （学号，课名） F→ 分数 （注：因为同一个的学号对应的分数不确定，同一个课名对应的分数也不确定） 部分函数依赖 假如 Y 函数依赖于 X，但同时 Y 并不完全函数依赖于 X，那么我们就称 Y 部分函数依赖于 X，记作 X P→ Y，如图2。 例如： （学号，课名） P→ 姓名 传递函数依赖假如 Z 函数依赖于 Y，且 Y 函数依赖于 X ，那么我们就称 Z 传递函数依赖于 X ，记作 X T→ Z，如图3。 码设 K 为某表中的一个属性或属性组，若除 K 之外的所有属性都完全函数依赖于 K（这个“完全”不要漏了），那么我们称 K 为候选码，简称为码。在实际中我们通常可以理解为：假如当 K 确定的情况下，该表除 K 之外的所有属性的值也就随之确定，那么 K 就是码。一张表中可以有超过一个码。（实际应用中为了方便，通常选择其中的一个码作为主码） 例如：对于表3，（学号、课名）这个属性组就是码。该表中有且仅有这一个码。（假设所有课没有重名的情况） 非主属性包含在任何一个码中的属性成为主属性。 例如：对于表3，主属性就有两个，学号 与 课名。 终于可以回过来看2NF了。首先，我们需要判断，表3是否符合2NF的要求？根据2NF的定义，判断的依据实际上就是看数据表中是否存在非主属性对于码的部分函数依赖。若存在，则数据表最高只符合1NF的要求，若不存在，则符合2NF的要求。判断的方法是： 第一步：找出数据表中所有的码。第二步：根据第一步所得到的码，找出所有的主属性。第三步：数据表中，除去所有的主属性，剩下的就都是非主属性了。第四步：查看是否存在非主属性对码的部分函数依赖。 对于表3，根据前面所说的四步，我们可以这么做： 第一步： 查看所有每一单个属性，当它的值确定了，是否剩下的所有属性值都能确定。 查看所有包含有两个属性的属性组，当它的值确定了，是否剩下的所有属性值都能确定。 …… 查看所有包含了六个属性，也就是所有属性的属性组，当它的值确定了，是否剩下的所有属性值都能确定。 看起来很麻烦是吧，但是这里有一个诀窍，就是假如A是码，那么所有包含了A的属性组，如（A，B）、（A，C）、（A，B，C）等等，都不是码了（因为作为码的要求里有一个“完全函数依赖”）。 图4表示了表中所有的函数依赖关系： 这一步完成以后，可以得到，表3的码只有一个，就是（学号、课名）。 第二步：主属性有两个：学号 与 课名 第三步：非主属性有四个：姓名、系名、系主任、分数 第四步：对于（学号，课名） → 姓名，有 学号 → 姓名，存在非主属性 姓名 对码（学号，课名）的部分函数依赖。对于（学号，课名） → 系名，有 学号 → 系名，存在非主属性 系名 对码（学号，课名）的部分函数依赖。对于（学号，课名） → 系主任，有 学号 → 系主任，存在非主属性 对码（学号，课名）的部分函数依赖。 所以表3存在非主属性对于码的部分函数依赖，最高只符合1NF的要求，不符合2NF的要求。 为了让表3符合2NF的要求，我们必须消除这些部分函数依赖，只有一个办法，就是将大数据表拆分成两个或者更多个更小的数据表，在拆分的过程中，要达到更高一级范式的要求，这个过程叫做”模式分解“。模式分解的方法不是唯一的，以下是其中一种方法：选课（学号，课名，分数）学生（学号，姓名，系名，系主任） 我们先来判断以下，选课表与学生表，是否符合了2NF的要求？ 对于选课表，其码是（学号，课名），主属性是学号和课名，非主属性是分数，学号确定，并不能唯一确定分数，课名确定，也不能唯一确定分数，所以不存在非主属性分数对于码 （学号，课名）的部分函数依赖，所以此表符合2NF的要求。 对于学生表，其码是学号，主属性是学号，非主属性是姓名、系名和系主任，因为码只有一个属性，所以不可能存在非主属性对于码 的部分函数依赖，所以此表符合2NF的要求。 图5表示了模式分解以后的新的函数依赖关系 表4表示了模式分解以后新的数据 现在我们来看一下，进行同样的操作，是否还存在着之前的那些问题？ 李小明转系到法律系只需要修改一次李小明对应的系的值即可。——有改进 数据冗余是否减少了？学生的姓名、系名与系主任，不再像之前一样重复那么多次了。——有改进 删除某个系中所有的学生记录该系的信息仍然全部丢失。——无改进 插入一个尚无学生的新系的信息。因为学生表的码是学号，不能为空，所以此操作不被允许。——无改进 所以说，仅仅符合2NF的要求，很多情况下还是不够的，而出现问题的原因，在于仍然存在非主属性系主任对于码学号的传递函数依赖。为了能进一步解决这些问题，我们还需要将符合2NF要求的数据表改进为符合3NF的要求。 第三范式（3NF） 3NF在2NF的基础之上，消除了非主属性对于码的传递函数依赖。也就是说， 如果存在非主属性对于码的传递函数依赖，则不符合3NF的要求。 接下来我们看看表4中的设计，是否符合3NF的要求。 对于选课表，主码为（学号，课名），主属性为学号和课名，非主属性只有一个，为分数，不可能存在传递函数依赖，所以选课表的设计，符合3NF的要求。 对于学生表，主码为学号，主属性为学号，非主属性为姓名、系名和系主任。因为 学号 → 系名，同时 系名 → 系主任，所以存在非主属性系主任对于码学号的传递函数依赖，所以学生表的设计，不符合3NF的要求。。 为了让数据表设计达到3NF，我们必须进一步进行模式分解为以下形式：选课（学号，课名，分数）学生（学号，姓名，系名）系（系名，系主任） 对于选课表，符合3NF的要求，之前已经分析过了。 对于学生表，码为学号，主属性为学号，非主属性为系名，不可能存在非主属性对于码的传递函数依赖，所以符合3NF的要求。 对于系表，码为系名，主属性为系名，非主属性为系主任，不可能存在非主属性对于码的传递函数依赖（至少要有三个属性才可能存在传递函数依赖关系），所以符合3NF的要求。。 新的函数依赖关系如图6 新的数据表如表5 现在我们来看一下，进行同样的操作，是否还存在着之前的那些问题？ 删除某个系中所有的学生记录该系的信息不会丢失。——有改进 插入一个尚无学生的新系的信息。因为系表与学生表目前是独立的两张表，所以不影响。——有改进 数据冗余更加少了。——有改进 结论由此可见，符合3NF要求的数据库设计，基本上解决了数据冗余过大，插入异常，修改异常，删除异常的问题。当然，在实际中，往往为了性能上或者应对扩展的需要，经常只做到2NF或者1NF，但是作为数据库设计人员，至少应该知道，3NF的要求是怎样的。 BCNF范式 要了解 BCNF 范式，那么先看这样一个问题： 若： 某公司有若干个仓库； 每个仓库只能有一名管理员，一名管理员只能在一个仓库中工作； 一个仓库中可以存放多种物品，一种物品也可以存放在不同的仓库中。每种物品在每个仓库中都有对应的数量。 那么关系模式 仓库（仓库名，管理员，物品名，数量） 属于哪一级范式？ 答：已知函数依赖集：仓库名 → 管理员，管理员 → 仓库名，（仓库名，物品名）→ 数量码：（管理员，物品名），（仓库名，物品名）主属性：仓库名、管理员、物品名非主属性：数量∵ 不存在非主属性对码的部分函数依赖和传递函数依赖。∴ 此关系模式属于3NF。 基于此关系模式的关系（具体的数据）可能如图所示： 好，既然此关系模式已经属于了 3NF，那么这个关系模式是否存在问题呢？我们来看以下几种操作： 先新增加一个仓库，但尚未存放任何物品，是否可以为该仓库指派管理员？——不可以，因为物品名也是主属性，根据实体完整性的要求，主属性不能为空。 某仓库被清空后，需要删除所有与这个仓库相关的物品存放记录，会带来什么问题？——仓库本身与管理员的信息也被随之删除了。 如果某仓库更换了管理员，会带来什么问题？——这个仓库有几条物品存放记录，就要修改多少次管理员信息。 从这里我们可以得出结论，在某些特殊情况下，即使关系模式符合 3NF 的要求，仍然存在着插入异常，修改异常与删除异常的问题，仍然不是 ”好“ 的设计。 造成此问题的原因：存在着主属性对于码的部分函数依赖与传递函数依赖。（在此例中就是存在主属性【仓库名】对于码【（管理员，物品名）】的部分函数依赖。 解决办法就是要在 3NF 的基础上消除主属性对于码的部分与传递函数依赖。 仓库（仓库名，管理员）库存（仓库名，物品名，数量） 这样，之前的插入异常，修改异常与删除异常的问题就被解决了。 以上就是关于 BCNF 的解释。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[事务的4种隔离级别]]></title>
    <url>%2F2018%2F05%2F26%2F%E4%BA%8B%E5%8A%A1%E7%9A%844%E7%A7%8D%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%2F</url>
    <content type="text"><![CDATA[数据库事务的隔离级别有4种，由低到高分别为Read uncommitted 、Read committed 、Repeatable read 、Serializable 。而且，在事务的并发操作中可能会出现脏读，不可重复读，幻读。下面通过事例一一阐述它们的概念与联系。 Read uncommitted读未提交，顾名思义，就是一个事务可以读取另一个未提交事务的数据。 事例：老板要给程序员发工资，程序员的工资是3.6万/月。但是发工资时老板不小心按错了数字，按成3.9万/月，该钱已经打到程序员的户口，但是事务还没有提交，就在这时，程序员去查看自己这个月的工资，发现比往常多了3千元，以为涨工资了非常高兴。但是老板及时发现了不对，马上回滚差点就提交了的事务，将数字改成3.6万再提交。 分析：实际程序员这个月的工资还是3.6万，但是程序员看到的是3.9万。他看到的是老板还没提交事务时的数据。这就是脏读。 那怎么解决脏读呢？Read committed！读提交，能解决脏读问题。 Read committed读提交，顾名思义，就是一个事务要等另一个事务提交后才能读取数据。 事例：程序员拿着信用卡去享受生活（卡里当然是只有3.6万），当他埋单时（程序员事务开启），收费系统事先检测到他的卡里有3.6万，就在这个时候！！程序员的妻子要把钱全部转出充当家用，并提交。当收费系统准备扣款时，再检测卡里的金额，发现已经没钱了（第二次检测金额当然要等待妻子转出金额事务提交完）。程序员就会很郁闷，明明卡里是有钱的… 分析：这就是读提交，若有事务对数据进行更新（UPDATE）操作时，读操作事务要等待这个更新操作事务提交后才能读取数据，可以解决脏读问题。但在这个事例中，出现了一个事务范围内两个相同的查询却返回了不同数据，这就是不可重复读。 那怎么解决可能的不可重复读问题？Repeatable read ！ Repeatable read重复读，就是在开始读取数据（事务开启）时，不再允许修改操作 事例：程序员拿着信用卡去享受生活（卡里当然是只有3.6万），当他埋单时（事务开启，不允许其他事务的UPDATE修改操作），收费系统事先检测到他的卡里有3.6万。这个时候他的妻子不能转出金额了。接下来收费系统就可以扣款了。 分析：重复读可以解决不可重复读问题。写到这里，应该明白的一点就是，不可重复读对应的是修改，即UPDATE操作。但是可能还会有幻读问题。因为幻读问题对应的是插入INSERT操作，而不是UPDATE操作。 什么时候会出现幻读？ 事例：程序员某一天去消费，花了2千元，然后他的妻子去查看他今天的消费记录（全表扫描FTS，妻子事务开启），看到确实是花了2千元，就在这个时候，程序员花了1万买了一部电脑，即新增INSERT了一条消费记录，并提交。当妻子打印程序员的消费记录清单时（妻子事务提交），发现花了1.2万元，似乎出现了幻觉，这就是幻读。 那怎么解决幻读问题？Serializable！ Serializable 序列化Serializable 是最高的事务隔离级别，在该级别下，事务串行化顺序执行，可以避免脏读、不可重复读与幻读。但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。 值得一提的是：大多数数据库默认的事务隔离级别是Read committed，比如Sql Server , Oracle。Mysql的默认隔离级别是Repeatable read。 总结事务的隔离性上，从低到高可能产生的读现象分别是：脏读、不可重复读、幻读。 脏读。是指当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交(commit)到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是脏数据，依据脏数据所做的操作可能是不正确的。 不可重复读。是指在数据库访问中，一个事务范围内两个相同的查询却返回了不同数据。这是由于查询时系统中其他事务修改的提交而引起的。比如事务T1读取某一数据，事务T2读取并修改了该数据，T1为了对读取值进行检验而再次读取该数据，便得到了不同的结果。 幻读。指同一个事务内多次查询返回的结果集不一样（比如增加了或者减少了行记录）。比如同一个事务A内第一次查询时候有n条记录，但是第二次同等条件下查询却又n+1条记录，这就好像产生了幻觉。 脏读指读到了未提交的数据。 不可重复读指一次事务内的多次相同查询，读取到了不同的结果。 幻读师不可重复读的特殊场景。一次事务内的多次范围查询得到了不同的结果。 通过在写的时候加锁，可以解决脏读。 通过在读的时候加锁，可以解决不可重复读。 通过串行化，可以解决幻读。 参考https://blog.csdn.net/qq_33290787/article/details/51924963 https://mp.weixin.qq.com/s/vkMG5A_DhMs7_wGUzgE1JA]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>隔离级别</tag>
        <tag>脏读</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库死锁]]></title>
    <url>%2F2018%2F05%2F26%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E6%AD%BB%E9%94%81%2F</url>
    <content type="text"><![CDATA[数据库和操作系统一样，是一个多用户使用的共享资源。当多个用户并发地存取数据 时，在数据库中就会产生多个事务同时存取同一数据的情况。若对并发操作不加控制就可能会读取和存储不正确的数据，破坏数据库的一致性。加锁是实现数据库并 发控制的一个非常重要的技术。在实际应用中经常会遇到的与锁相关的异常情况，当两个事务需要一组有冲突的锁，而不能将事务继续下去的话，就会出现死锁，严 重影响应用的正常执行。在数据库中有两种基本的锁类型：排它锁（Exclusive Locks，即X锁）和共享锁（Share Locks，即S锁）。当数据对象被加上排它锁（写锁）时，其他的事务不能对它读取和修改。加了共享锁（读锁）的数据对象可以被其他事务读取，但不能修改。数据库利用这两 种基本的锁类型来对数据库的事务进行并发控制。 死锁的第一种情况一个用户A 访问表A(锁住了表A),然后又访问表B；另一个用户B 访问表B(锁住了表B)，然后企图访问表A；这时用户A由于用户B已经锁住表B，它必须等待用户B释放表B才能继续，同样用户B要等用户A释放表A才能继续，这就死锁就产生了。 解决方法： 这种死锁比较常见，是由于程序的BUG产生的，除了调整的程序的逻辑没有其它的办法。仔细分析程序的逻辑，对于数据库的多表操作时，尽量按照相同的顺序进 行处理，尽量避免同时锁定两个资源，如操作A和B两张表时，总是按先A后B的顺序处理， 必须同时锁定两个资源时，要保证在任何时刻都应该按照相同的顺序来锁定资源。 死锁的第二种情况用户A查询一条纪录，然后修改该条纪录；这时用户B修改该条纪录，这时用户A的事务里锁的性质由查询的共享锁企图上升到独占锁，而用户B里的独占锁由于A 有共享锁存在所以必须等A释放掉共享锁，而A由于B的独占锁而无法上升的独占锁也就不可能释放共享锁，于是出现了死锁。这种死锁比较隐蔽，但在稍大点的项 目中经常发生。如在某项目中，页面上的按钮点击后，没有使按钮立刻失效，使得用户会多次快速点击同一按钮，这样同一段代码对数据库同一条记录进行多次操 作，很容易就出现这种死锁的情况。 解决方法： 1、对于按钮等控件，点击后使其立刻失效，不让用户重复点击，避免对同时对同一条记录操作。2、使用乐观锁进行控制。乐观锁大多是基于数据版本（Version）记录机制实现。即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是 通过为数据库表增加一个“version”字段来实现。读取出数据时，将此版本号一同读出，之后更新时，对此版本号加一。此时，将提交数据的版本数据与数 据库表对应记录的当前版本信息进行比对，如果提交的数据版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据。乐观锁机制避免了长事务中的数据 库加锁开销（用户A和用户B操作过程中，都没有对数据库数据加锁），大大提升了大并发量下的系统整体性能表现。Hibernate 在其数据访问引擎中内置了乐观锁实现。需要注意的是，由于乐观锁机制是在我们的系统中实现，来自外部系统的用户更新操作不受我们系统的控制，因此可能会造 成脏数据被更新到数据库中。3、使用悲观锁进行控制。悲观锁大多数情况下依靠数据库的锁机制实现，如Oracle的Select … for update语句，以保证操作最大程度的独占性。但随之而来的就是数据库性能的大量开销，特别是对长事务而言，这样的开销往往无法承受。如一个金融系统， 当某个操作员读取用户的数据，并在读出的用户数据的基础上进行修改时（如更改用户账户余额），如果采用悲观锁机制，也就意味着整个操作过程中（从操作员读 出数据、开始修改直至提交修改结果的全过程，甚至还包括操作员中途去煮咖啡的时间），数据库记录始终处于加锁状态，可以想见，如果面对成百上千个并发，这 样的情况将导致灾难性的后果。所以，采用悲观锁进行控制时一定要考虑清楚。 死锁的第三种情况如果在事务中执行了一条不满足条件的update语句，则执行全表扫描，把行级锁上升为表级锁，多个这样的事务执行后，就很容易产生死锁和阻塞。类似的情 况还有当表中的数据量非常庞大而索引建的过少或不合适的时候，使得经常发生全表扫描，最终应用系统会越来越慢，最终发生阻塞或死锁。 解决方法： SQL语句中不要使用太复杂的关联多表的查询；使用“执行计划”对SQL语句进行分析，对于有全表扫描的SQL语句，建立相应的索引进行优化。 总结总体上来说，产生内存溢出与锁表都是由于代码写的不好造成的，因此提高代码的质量是最根本的解决办法。有的人认为先把功能实现，有BUG时再在测试阶段进 行修正，这种想法是错误的。正如一件产品的质量是在生产制造的过程中决定的，而不是质量检测时决定的，软件的质量在设计与编码阶段就已经决定了，测试只是 对软件质量的一个验证，因为测试不可能找出软件中所有的BUG。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>死锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于synchronized和ReentrantLock之多线程同步详解]]></title>
    <url>%2F2018%2F05%2F15%2F%E5%85%B3%E4%BA%8Esynchronized%E5%92%8CReentrantLock%E4%B9%8B%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[本篇文章总结关于多线程编程的一些知识点，这其中两个重要的部分就是对于synchronized和ReentrantLock的使用和介绍。 线程同步问题的产生及解决方案问题的产生： Java允许多线程并发控制，当多个线程同时操作一个可共享的资源变量时（如数据的增删改查），将会导致数据不准确，相互之间产生冲突。 如下例：假设有一个卖票系统，一共有100张票，有4个窗口同时卖。 1234567891011121314151617public class Ticket implements Runnable &#123; // 当前拥有的票数 private int num = 100; public void run() &#123; while (true) &#123; if (num &gt; 0) &#123; try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; &#125; // 输出卖票信息 System.out.println(Thread.currentThread().getName() + ".....sale...." + num--); &#125; &#125; &#125;&#125; 12345678910111213public static void main(String[] args) &#123; Ticket t = new Ticket();//创建一个线程任务对象。 //创建4个线程同时卖票 Thread t1 = new Thread(t); Thread t2 = new Thread(t); Thread t3 = new Thread(t); Thread t4 = new Thread(t); //启动线程 t1.start(); t2.start(); t3.start(); t4.start(); &#125; 输出部分结果： Thread-1…..sale….2 Thread-0…..sale….3 Thread-2…..sale….1 Thread-0…..sale….0 Thread-1…..sale….0 Thread-3…..sale….1 显然上述结果是不合理的，对于同一张票进行了多次售出。这就是多线程情况下，出现了数据“脏读”情况。即多个线程访问余票num时，当一个线程获得余票的数量，要在此基础上进行-1的操作之前，其他线程可能已经卖出多张票，导致获得的num不是最新的，然后-1后更新的数据就会有误。这就需要线程同步的实现了。 问题的解决： 因此加入同步锁以避免在该线程没有完成操作之前，被其他线程的调用，从而保证了该变量的唯一性和准确性。 一共有两种锁，来实现线程同步问题，分别是：synchronized和ReentrantLock。下面我们就带着上述问题，看看这两种锁是如何解决的。 synchronized关键字synchronized简介synchronized实现同步的基础：Java中每个对象都可以作为锁。当线程试图访问同步代码时，必须先获得对象锁，退出或抛出异常时必须释放锁。Synchronzied实现同步的表现形式分为：代码块同步和方法同步。 synchronized原理JVM基于进入和退出Monitor对象来实现代码块同步和方法同步，两者实现细节不同。 代码块同步：在编译后通过将monitorenter指令插入到同步代码块的开始处，将monitorexit指令插入到方法结束处和异常处，通过反编译字节码可以观察到。任何一个对象都有一个monitor与之关联，线程执行monitorenter指令时，会尝试获取对象对应的monitor的所有权，即尝试获得对象的锁。 方法同步：synchronized方法在method_info结构有ACC_synchronized标记，线程执行时会识别该标记，获取对应的锁，实现方法同步。 两者虽然实现细节不同，但本质上都是对一个对象的监视器（monitor）的获取。任意一个对象都拥有自己的监视器，当同步代码块或同步方法时，执行方法的线程必须先获得该对象的监视器才能进入同步块或同步方法，没有获取到监视器的线程将会被阻塞，并进入同步队列，状态变为BLOCKED。当成功获取监视器的线程释放了锁后，会唤醒阻塞在同步队列的线程，使其重新尝试对监视器的获取。 对象、监视器、同步队列和执行线程间的关系如下图： synchronized的使用场景①方法同步 1public synchronized void method1 锁住的是该对象,类的其中一个实例，当该对象(仅仅是这一个对象)在不同线程中执行这个同步方法时，线程之间会形成互斥。达到同步效果，但如果不同线程同时对该类的不同对象执行这个同步方法时，则线程之间不会形成互斥，因为他们拥有的是不同的锁。 ②代码块同步 1synchronized(this)&#123; //TODO &#125; 描述同① ③方法同步 1public synchronized static void method3 锁住的是该类，当所有该类的对象(多个对象)在不同线程中调用这个static同步方法时，线程之间会形成互斥，达到同步效果。 ④代码块同步 1synchronized(Test.class)&#123; //TODO&#125; 同③ ⑤代码块同步 1synchronized(o) &#123;&#125; 这里面的o可以是一个任何Object对象或数组，并不一定是它本身对象或者类，谁拥有o这个锁，谁就能够操作该块程序代码。 解决线程同步的实例针对上述方法，具体的解决方式如下： 1234567891011121314151617181920public class Ticket implements Runnable &#123; // 当前拥有的票数 private int num = 100; public void run() &#123; while (true) &#123; try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; &#125; synchronized (this) &#123; // 输出卖票信息 if(num&gt;0)&#123; System.out.println(Thread.currentThread().getName() + ".....sale...." + num--); &#125; &#125; &#125; &#125;&#125; 输出部分结果： Thread-2…..sale….10 Thread-1…..sale….9 Thread-3…..sale….8 Thread-0…..sale….7 Thread-2…..sale….6 Thread-1…..sale….5 Thread-2…..sale….4 Thread-1…..sale….3 Thread-3…..sale….2 Thread-0…..sale….1 可以看出实现了线程同步。同时改了一下逻辑，在进入到同步代码块时，先判断现在是否有没有票，然后再买票，防止出现没票还要售出的情况。通过同步代码块实现了线程同步，其他方法也一样可以实现该效果。 ReentrantLock锁Lock接口Lock，锁对象。在Java中锁是用来控制多个线程访问共享资源的方式，一般来说，一个锁能够防止多个线程同时访问共享资源（但有的锁可以允许多个线程并发访问共享资源，比如读写锁，后面我们会分析）。在Lock接口出现之前，Java程序是靠synchronized关键字（后面分析）实现锁功能的，而JAVA SE5.0之后并发包中新增了Lock接口用来实现锁的功能，它提供了与synchronized关键字类似的同步功能，只是在使用时需要显式地获取和释放锁，缺点就是缺少像synchronized那样隐式获取释放锁的便捷性，但是却拥有了锁获取与释放的可操作性，可中断的获取锁以及超时获取锁等多种synchronized关键字所不具备的同步特性。 Lock接口的主要方法（还有两个方法比较复杂，暂不介绍）： void lock(): 执行此方法时, 如果锁处于空闲状态, 当前线程将获取到锁. 相反, 如果锁已经被其他线程持有, 将禁用当前线程, 直到当前线程获取到锁. boolean tryLock()：如果锁可用, 则获取锁, 并立即返回true, 否则返回false. 该方法和lock()的区别在于, tryLock()只是”试图”获取锁, 如果锁不可用, 不会导致当前线程被禁用, 当前线程仍然继续往下执行代码. 而lock()方法则是一定要获取到锁, 如果锁不可用, 就一直等待, 在未获得锁之前,当前线程并不继续向下执行. 通常采用如下的代码形式调用tryLock()方法: void unlock()：执行此方法时, 当前线程将释放持有的锁. 锁只能由持有者释放, 如果线程并不持有锁, 却执行该方法, 可能导致异常的发生. Condition newCondition()：条件对象，获取等待通知组件。该组件和当前的锁绑定，当前线程只有获取了锁，才能调用该组件的await()方法，而调用后，当前线程将缩放锁。 ReentrantLock，一个可重入的互斥锁，它具有与使用synchronized方法和语句所访问的隐式监视器锁相同的一些基本行为和语义，但功能更强大。（重入锁后面介绍） ReentrantLock的使用关于ReentrantLock的使用很简单，只需要显示调用，获得同步锁，释放同步锁即可。 12345678ReentrantLock lock = new ReentrantLock(); //参数默认false，不公平锁 ..................... lock.lock(); //如果被其它资源锁定，会在此等待锁释放，达到暂停的效果 try &#123; //操作 &#125; finally &#123; lock.unlock(); //释放锁 &#125; 解决线程同步的实例针对上述方法，具体的解决方式如下： 12345678910111213141516171819202122public class Ticket implements Runnable &#123; // 当前拥有的票数 private int num = 100; ReentrantLock lock = new ReentrantLock(); public void run() &#123; while (true) &#123; try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; &#125; lock.lock(); // 输出卖票信息 if (num &gt; 0) &#123; System.out.println(Thread.currentThread().getName() + ".....sale...." + num--); &#125; lock.unlock(); &#125; &#125;&#125; 重入锁当一个线程得到一个对象后，再次请求该对象锁时是可以再次得到该对象的锁的。 具体概念就是：自己可以再次获取自己的内部锁。 Java里面内置锁(synchronized)和Lock(ReentrantLock)都是可重入的。 12345678910111213141516public class SynchronizedTest &#123; public void method1() &#123; synchronized (SynchronizedTest.class) &#123; System.out.println("方法1获得ReentrantTest的锁运行了"); method2(); &#125; &#125; public void method2() &#123; synchronized (SynchronizedTest.class) &#123; System.out.println("方法1里面调用的方法2重入锁,也正常运行了"); &#125; &#125; public static void main(String[] args) &#123; new SynchronizedTest().method1(); &#125;&#125; 上面便是synchronized的重入锁特性，即调用method1()方法时，已经获得了锁，此时内部调用method2()方法时，由于本身已经具有该锁，所以可以再次获取。 1234567891011121314151617181920212223public class ReentrantLockTest &#123; private Lock lock = new ReentrantLock(); public void method1() &#123; lock.lock(); try &#123; System.out.println("方法1获得ReentrantLock锁运行了"); method2(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void method2() &#123; lock.lock(); try &#123; System.out.println("方法1里面调用的方法2重入ReentrantLock锁,也正常运行了"); &#125; finally &#123; lock.unlock(); &#125; &#125; public static void main(String[] args) &#123; new ReentrantLockTest().method1(); &#125;&#125; 上面便是ReentrantLock的重入锁特性，即调用method1()方法时，已经获得了锁，此时内部调用method2()方法时，由于本身已经具有该锁，所以可以再次获取。 公平锁CPU在调度线程的时候是在等待队列里随机挑选一个线程，由于这种随机性所以是无法保证线程先到先得的（synchronized控制的锁就是这种非公平锁）。但这样就会产生饥饿现象，即有些线程（优先级较低的线程）可能永远也无法获取CPU的执行权，优先级高的线程会不断的强制它的资源。那么如何解决饥饿问题呢，这就需要公平锁了。公平锁可以保证线程按照时间的先后顺序执行，避免饥饿现象的产生。但公平锁的效率比较低，因为要实现顺序执行，需要维护一个有序队列。 ReentrantLock便是一种公平锁，通过在构造方法中传入true就是公平锁，传入false，就是非公平锁。 123public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync(); &#125; 以下是使用公平锁实现的效果： 123456789101112131415161718192021public class LockFairTest implements Runnable&#123; //创建公平锁 private static ReentrantLock lock=new ReentrantLock(true); public void run() &#123; while(true)&#123; lock.lock(); try&#123; System.out.println(Thread.currentThread().getName()+"获得锁"); &#125;finally&#123; lock.unlock(); &#125; &#125; &#125; public static void main(String[] args) &#123; LockFairTest lft=new LockFairTest(); Thread th1=new Thread(lft); Thread th2=new Thread(lft); th1.start(); th2.start(); &#125;&#125; 输出结果： Thread-1获得锁 Thread-0获得锁 Thread-1获得锁 Thread-0获得锁 Thread-1获得锁 Thread-0获得锁 Thread-1获得锁 Thread-0获得锁 Thread-1获得锁 Thread-0获得锁 Thread-1获得锁 Thread-0获得锁 Thread-1获得锁 Thread-0获得锁 Thread-1获得锁 Thread-0获得锁 这是截取的部分执行结果，分析结果可看出两个线程是交替执行的，几乎不会出现同一个线程连续执行多次。 synchronized和ReentrantLock的比较区别：1）Lock是一个接口，而synchronized是Java中的关键字，synchronized是内置的语言实现； 2）synchronized在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；而Lock在发生异常时，如果没有主动通过unLock()去释放锁，则很可能造成死锁现象，因此使用Lock时需要在finally块中释放锁； 3）Lock可以让等待锁的线程响应中断，而synchronized却不行，使用synchronized时，等待的线程会一直等待下去，不能够响应中断； 4）通过Lock可以知道有没有成功获取锁，而synchronized却无法办到。 5）Lock可以提高多个线程进行读操作的效率。 总结：ReentrantLock相比synchronized，增加了一些高级的功能。但也有一定缺陷。在ReentrantLock类中定义了很多方法，比如： 1234567isFair() //判断锁是否是公平锁isLocked() //判断锁是否被任何线程获取了isHeldByCurrentThread() //判断锁是否被当前线程获取了hasQueuedThreads() //判断是否有线程在等待该锁 两者在锁的相关概念上区别：1)可中断锁 顾名思义，就是可以相应中断的锁。 在Java中，synchronized就不是可中断锁，而Lock是可中断锁。如果某一线程A正在执行锁中的代码，另一线程B正在等待获取该锁，可能由于等待时间过长，线程B不想等待了，想先处理其他事情，我们可以让它中断自己或者在别的线程中中断它，这种就是可中断锁。 lockInterruptibly()的用法体现了Lock的可中断性。 2)公平锁 公平锁即尽量以请求锁的顺序来获取锁。比如同是有多个线程在等待一个锁，当这个锁被释放时，等待时间最久的线程（最先请求的线程）会获得该锁（并不是绝对的，大体上是这种顺序），这种就是公平锁。 非公平锁即无法保证锁的获取是按照请求锁的顺序进行的。这样就可能导致某个或者一些线程永远获取不到锁。 在Java中，synchronized就是非公平锁，它无法保证等待的线程获取锁的顺序。ReentrantLock可以设置成公平锁。 3)读写锁 读写锁将对一个资源（比如文件）的访问分成了2个锁，一个读锁和一个写锁。 正因为有了读写锁，才使得多个线程之间的读操作可以并发进行，不需要同步，而写操作需要同步进行，提高了效率。 ReadWriteLock就是读写锁，它是一个接口，ReentrantReadWriteLock实现了这个接口。 可以通过readLock()获取读锁，通过writeLock()获取写锁。 4)绑定多个条件 一个ReentrantLock对象可以同时绑定多个Condition对象，而在synchronized中，锁对象的wait()和notify()或notifyAll()方法可以实现一个隐含的条件，如果要和多余一个条件关联的时候，就不得不额外地添加一个锁，而ReentrantLock则无须这么做，只需要多次调用new Condition()方法即可。 性能比较在性能上来说，如果竞争资源不激烈，两者的性能是差不多的，而当竞争资源非常激烈时（即有大量线程同时竞争），此时ReentrantLock的性能要远远优于synchronized。所以说，在具体使用时要根据适当情况选择。 在JDK1.5中，synchronized是性能低效的。因为这是一个重量级操作，它对性能最大的影响是阻塞的是实现，挂起线程和恢复线程的操作都需要转入内核态中完成，这些操作给系统的并发性带来了很大的压力。相比之下使用Java提供的ReentrankLock对象，性能更高一些。到了JDK1.6，发生了变化，对synchronize加入了很多优化措施，有自适应自旋，锁消除，锁粗化，轻量级锁，偏向锁等等。导致在JDK1.6上synchronize的性能并不比Lock差。官方也表示，他们也更支持synchronize，在未来的版本中还有优化余地，所以还是提倡在synchronized能实现需求的情况下，优先考虑使用synchronized来进行同步。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>脏读</tag>
        <tag>锁</tag>
        <tag>synchronized</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你真的了解volatile关键字吗？]]></title>
    <url>%2F2018%2F05%2F13%2F%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3volatile%E5%85%B3%E9%94%AE%E5%AD%97%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[原文出处： Ruheng Java内存模型想要理解volatile为什么能确保可见性，就要先理解Java中的内存模型是什么样的。 Java内存模型规定了所有的变量都存储在主内存中。每条线程中还有自己的工作内存，线程的工作内存中保存了被该线程所使用到的变量（这些变量是从主内存中拷贝而来）。线程对变量的所有操作（读取，赋值）都必须在工作内存中进行。不同线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递均需要通过主内存来完成。 基于此种内存模型，便产生了多线程编程中的数据“脏读”等问题。 举个简单的例子：在java中，执行下面这个语句： 1i = 10; 执行线程必须先在自己的工作线程中对变量i所在的缓存行进行赋值操作，然后再写入主存当中。而不是直接将数值10写入主存当中。 比如同时有2个线程执行这段代码，假如初始时i的值为10，那么我们希望两个线程执行完之后i的值变为12。但是事实会是这样吗？ 可能存在下面一种情况：初始时，两个线程分别读取i的值存入各自所在的工作内存当中，然后线程1进行加1操作，然后把i的最新值11写入到内存。此时线程2的工作内存当中i的值还是10，进行加1操作之后，i的值为11，然后线程2把i的值写入内存。 最终结果i的值是11，而不是12。这就是著名的缓存一致性问题。通常称这种被多个线程访问的变量为共享变量。 那么如何确保共享变量在多线程访问时能够正确输出结果呢？ 在解决这个问题之前，我们要先了解并发编程的三大概念：原子性，有序性，可见性。 原子性定义原子性：即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。 实例一个很经典的例子就是银行账户转账问题： 比如从账户A向账户B转1000元，那么必然包括2个操作：从账户A减去1000元，往账户B加上1000元。 试想一下，如果这2个操作不具备原子性，会造成什么样的后果。假如从账户A减去1000元之后，操作突然中止。这样就会导致账户A虽然减去了1000元，但是账户B没有收到这个转过来的1000元。 所以这2个操作必须要具备原子性才能保证不出现一些意外的问题。 同样地反映到并发编程中会出现什么结果呢？ 举个最简单的例子，大家想一下假如为一个32位的变量赋值过程不具备原子性的话，会发生什么后果？ 1i = 9; 假若一个线程执行到这个语句时，我暂且假设为一个32位的变量赋值包括两个过程：为低16位赋值，为高16位赋值。 那么就可能发生一种情况：当将低16位数值写入之后，突然被中断，而此时又有一个线程去读取i的值，那么读取到的就是错误的数据。 Java中的原子性在Java中，对基本数据类型的变量的读取和赋值操作是原子性操作，即这些操作是不可被中断的，要么执行，要么不执行。 上面一句话虽然看起来简单，但是理解起来并不是那么容易。看下面一个例子i： 请分析以下哪些操作是原子性操作： 1234x = 10; //语句1y = x; //语句2x++; //语句3x = x + 1; //语句4 咋一看，可能会说上面的4个语句中的操作都是原子性操作。其实只有语句1是原子性操作，其他三个语句都不是原子性操作。 语句1是直接将数值10赋值给x，也就是说线程执行这个语句的会直接将数值10写入到工作内存中。 语句2实际上包含2个操作，它先要去读取x的值，再将x的值写入工作内存，虽然读取x的值以及 将x的值写入工作内存 这2个操作都是原子性操作，但是合起来就不是原子性操作了。 同样的，x++和 x = x+1包括3个操作：读取x的值，进行加1操作，写入新的值。 所以上面4个语句只有语句1的操作具备原子性。 也就是说，只有简单的读取、赋值（而且必须是将数字赋值给某个变量，变量之间的相互赋值不是原子操作）才是原子操作。 从上面可以看出，Java内存模型只保证了基本读取和赋值是原子性操作，如果要实现更大范围操作的原子性，可以通过synchronized和Lock来实现。由于synchronized和Lock能够保证任一时刻只有一个线程执行该代码块，那么自然就不存在原子性问题了，从而保证了原子性。 可见性定义可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。 实例举个简单的例子，看下面这段代码： 123456//线程1执行的代码int i = 0;i = 10; //线程2执行的代码j = i; 由上面的分析可知，当线程1执行 i =10这句时，会先把i的初始值加载到工作内存中，然后赋值为10，那么在线程1的工作内存当中i的值变为10了，却没有立即写入到主存当中。 此时线程2执行 j = i，它会先去主存读取i的值并加载到线程2的工作内存当中，注意此时内存当中i的值还是0，那么就会使得j的值为0，而不是10. 这就是可见性问题，线程1对变量i修改了之后，线程2没有立即看到线程1修改的值。 Java中的可见性对于可见性，Java提供了volatile关键字来保证可见性。 当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。 而普通的共享变量不能保证可见性，因为普通共享变量被修改之后，什么时候被写入主存是不确定的，当其他线程去读取时，此时内存中可能还是原来的旧值，因此无法保证可见性。 另外，通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。 有序性定义有序性：即程序执行的顺序按照代码的先后顺序执行。 实例举个简单的例子，看下面这段代码： 123456int i = 0; boolean flag = false;i = 1; //语句1 flag = true; //语句2 上面代码定义了一个int型变量，定义了一个boolean类型变量，然后分别对两个变量进行赋值操作。从代码顺序上看，语句1是在语句2前面的，那么JVM在真正执行这段代码的时候会保证语句1一定会在语句2前面执行吗？不一定，为什么呢？这里可能会发生指令重排序（Instruction Reorder）。 下面解释一下什么是指令重排序，一般来说，处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。 比如上面的代码中，语句1和语句2谁先执行对最终的程序结果并没有影响，那么就有可能在执行过程中，语句2先执行而语句1后执行。 但是要注意，虽然处理器会对指令进行重排序，但是它会保证程序最终结果会和代码顺序执行结果相同，那么它靠什么保证的呢？再看下面一个例子： 1234int a = 10; //语句1int r = 2; //语句2a = a + 3; //语句3r = a*a; //语句4 这段代码有4个语句，那么可能的一个执行顺序是： 那么可不可能是这个执行顺序呢： 语句2 语句1 语句4 语句3 不可能，因为处理器在进行重排序时是会考虑指令之间的数据依赖性，如果一个指令Instruction 2必须用到Instruction 1的结果，那么处理器会保证Instruction 1会在Instruction 2之前执行。 虽然重排序不会影响单个线程内程序执行的结果，但是多线程呢？下面看一个例子： 12345678910//线程1:context = loadContext(); //语句1inited = true; //语句2 //线程2:while(!inited )&#123; sleep()&#125;doSomethingwithconfig(context); 上面代码中，由于语句1和语句2没有数据依赖性，因此可能会被重排序。假如发生了重排序，在线程1执行过程中先执行语句2，而此是线程2会以为初始化工作已经完成，那么就会跳出while循环，去执行doSomethingwithconfig(context)方法，而此时context并没有被初始化，就会导致程序出错。 从上面可以看出，指令重排序不会影响单个线程的执行，但是会影响到线程并发执行的正确性。 也就是说，要想并发程序正确地执行，必须要保证原子性、可见性以及有序性。只要有一个没有被保证，就有可能会导致程序运行不正确。 Java中的有序性在Java内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。 在Java里面，可以通过volatile关键字来保证一定的“有序性”。另外可以通过synchronized和Lock来保证有序性，很显然，synchronized和Lock保证每个时刻是有一个线程执行同步代码，相当于是让线程顺序执行同步代码，自然就保证了有序性。 另外，Java内存模型具备一些先天的“有序性”，即不需要通过任何手段就能够得到保证的有序性，这个通常也称为 happens-before 原则。如果两个操作的执行次序无法从happens-before原则推导出来，那么它们就不能保证它们的有序性，虚拟机可以随意地对它们进行重排序。 下面就来具体介绍下happens-before原则（先行发生原则）：①程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作 ②锁定规则：一个unLock操作先行发生于后面对同一个锁额lock操作 ③volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作 ④传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C ⑤线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作 ⑥线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生 ⑦线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行 ⑧对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始 这8条规则中，前4条规则是比较重要的，后4条规则都是显而易见的。 下面我们来解释一下前4条规则： 对于程序次序规则来说，就是一段程序代码的执行在单个线程中看起来是有序的。注意，虽然这条规则中提到“书写在前面的操作先行发生于书写在后面的操作”，这个应该是程序看起来执行的顺序是按照代码顺序执行的，但是虚拟机可能会对程序代码进行指令重排序。虽然进行重排序，但是最终执行的结果是与程序顺序执行的结果一致的，它只会对不存在数据依赖性的指令进行重排序。因此，在单个线程中，程序执行看起来是有序执行的，这一点要注意理解。事实上，这个规则是用来保证程序在单线程中执行结果的正确性，但无法保证程序在多线程中执行的正确性。 第二条规则也比较容易理解，也就是说无论在单线程中还是多线程中，同一个锁如果处于被锁定的状态，那么必须先对锁进行了释放操作，后面才能继续进行lock操作。 第三条规则是一条比较重要的规则。直观地解释就是，如果一个线程先去写一个变量，然后一个线程去进行读取，那么写入操作肯定会先行发生于读操作。 第四条规则实际上就是体现happens-before原则具备传递性。 深入理解volatile关键字volatile保证可见性一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义： 1）保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。 2）禁止进行指令重排序。 先看一段代码，假如线程1先执行，线程2后执行： 12345678//线程1boolean stop = false;while(!stop)&#123; doSomething();&#125; //线程2stop = true; 这段代码是很典型的一段代码，很多人在中断线程时可能都会采用这种标记办法。但是事实上，这段代码会完全运行正确么？即一定会将线程中断么？不一定，也许在大多数时候，这个代码能够把线程中断，但是也有可能会导致无法中断线程（虽然这个可能性很小，但是只要一旦发生这种情况就会造成死循环了）。 下面解释一下这段代码为何有可能导致无法中断线程。在前面已经解释过，每个线程在运行过程中都有自己的工作内存，那么线程1在运行的时候，会将stop变量的值拷贝一份放在自己的工作内存当中。 那么当线程2更改了stop变量的值之后，但是还没来得及写入主存当中，线程2转去做其他事情了，那么线程1由于不知道线程2对stop变量的更改，因此还会一直循环下去。 但是用volatile修饰之后就变得不一样了： 第一：使用volatile关键字会强制将修改的值立即写入主存； 第二：使用volatile关键字的话，当线程2进行修改时，会导致线程1的工作内存中缓存变量stop的缓存行无效（反映到硬件层的话，就是CPU的L1或者L2缓存中对应的缓存行无效）； 第三：由于线程1的工作内存中缓存变量stop的缓存行无效，所以线程1再次读取变量stop的值时会去主存读取。 那么在线程2修改stop值时（当然这里包括2个操作，修改线程2工作内存中的值，然后将修改后的值写入内存），会使得线程1的工作内存中缓存变量stop的缓存行无效，然后线程1读取时，发现自己的缓存行无效，它会等待缓存行对应的主存地址被更新之后，然后去对应的主存读取最新的值。 那么线程1读取到的就是最新的正确的值。 volatile不能确保原子性下面看一个例子： 1234567891011121314151617181920212223public class Test &#123; public volatile int inc = 0; public void increase() &#123; inc++; &#125; public static void main(String[] args) &#123; final Test test = new Test(); for(int i=0;i&lt;10;i++)&#123; new Thread()&#123; public void run() &#123; for(int j=0;j&lt;1000;j++) test.increase(); &#125;; &#125;.start(); &#125; while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); &#125;&#125; 大家想一下这段程序的输出结果是多少？也许有些朋友认为是10000。但是事实上运行它会发现每次运行结果都不一致，都是一个小于10000的数字。 可能有的朋友就会有疑问，不对啊，上面是对变量inc进行自增操作，由于volatile保证了可见性，那么在每个线程中对inc自增完之后，在其他线程中都能看到修改后的值啊，所以有10个线程分别进行了1000次操作，那么最终inc的值应该是1000*10=10000。 这里面就有一个误区了，volatile关键字能保证可见性没有错，但是上面的程序错在没能保证原子性。可见性只能保证每次读取的是最新的值，但是volatile没办法保证对变量的操作的原子性。 在前面已经提到过，自增操作是不具备原子性的，它包括读取变量的原始值、进行加1操作、写入工作内存。那么就是说自增操作的三个子操作可能会分割开执行，就有可能导致下面这种情况出现： 假如某个时刻变量inc的值为10， 线程1对变量进行自增操作，线程1先读取了变量inc的原始值，然后线程1被阻塞了； 然后线程2对变量进行自增操作，线程2也去读取变量inc的原始值，由于线程1只是对变量inc进行读取操作，而没有对变量进行修改操作，所以不会导致线程2的工作内存中缓存变量inc的缓存行无效，也不会导致主存中的值刷新，所以线程2会直接去主存读取inc的值，发现inc的值时10，然后进行加1操作，并把11写入工作内存，最后写入主存。 然后线程1接着进行加1操作，由于已经读取了inc的值，注意此时在线程1的工作内存中inc的值仍然为10，所以线程1对inc进行加1操作后inc的值为11，然后将11写入工作内存，最后写入主存。 那么两个线程分别进行了一次自增操作后，inc只增加了1。 根源就在这里，自增操作不是原子性操作，而且volatile也无法保证对变量的任何操作都是原子性的。 解决方案：可以通过synchronized或lock，进行加锁，来保证操作的原子性。也可以通过AtomicInteger。 在java 1.5的java.util.concurrent.atomic包下提供了一些原子操作类，即对基本数据类型的 自增（加1操作），自减（减1操作）、以及加法操作（加一个数），减法操作（减一个数）进行了封装，保证这些操作是原子性操作。atomic是利用CAS来实现原子性操作的（Compare And Swap），CAS实际上是利用处理器提供的CMPXCHG指令实现的，而处理器执行CMPXCHG指令是一个原子性操作。 volatile保证有序性在前面提到volatile关键字能禁止指令重排序，所以volatile能在一定程度上保证有序性。 volatile关键字禁止指令重排序有两层意思： 1）当程序执行到volatile变量的读操作或者写操作时，在其前面的操作的更改肯定全部已经进行，且结果已经对后面的操作可见；在其后面的操作肯定还没有进行； 2）在进行指令优化时，不能将在对volatile变量的读操作或者写操作的语句放在其后面执行，也不能把volatile变量后面的语句放到其前面执行。 可能上面说的比较绕，举个简单的例子： 12345678//x、y为非volatile变量//flag为volatile变量 x = 2; //语句1y = 0; //语句2flag = true; //语句3x = 4; //语句4y = -1; //语句5 由于flag变量为volatile变量，那么在进行指令重排序的过程的时候，不会将语句3放到语句1、语句2前面，也不会讲语句3放到语句4、语句5后面。但是要注意语句1和语句2的顺序、语句4和语句5的顺序是不作任何保证的。 并且volatile关键字能保证，执行到语句3时，语句1和语句2必定是执行完毕了的，且语句1和语句2的执行结果对语句3、语句4、语句5是可见的。 那么我们回到前面举的一个例子： 123456789//线程1:context = loadContext(); //语句1inited = true; //语句2 //线程2:while(!inited )&#123; sleep()&#125;doSomethingwithconfig(context); 前面举这个例子的时候，提到有可能语句2会在语句1之前执行，那么久可能导致context还没被初始化，而线程2中就使用未初始化的context去进行操作，导致程序出错。 这里如果用volatile关键字对inited变量进行修饰，就不会出现这种问题了，因为当执行到语句2时，必定能保证context已经初始化完毕。 volatile的实现原理可见性处理器为了提高处理速度，不直接和内存进行通讯，而是将系统内存的数据独到内部缓存后再进行操作，但操作完后不知什么时候会写到内存。 如果对声明了volatile变量进行写操作时，JVM会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写会到系统内存。 这一步确保了如果有其他线程对声明了volatile变量进行修改，则立即更新主内存中数据。 但这时候其他处理器的缓存还是旧的，所以在多处理器环境下，为了保证各个处理器缓存一致，每个处理会通过嗅探在总线上传播的数据来检查 自己的缓存是否过期，当处理器发现自己缓存行对应的内存地址被修改了，就会将当前处理器的缓存行设置成无效状态，当处理器要对这个数据进行修改操作时，会强制重新从系统内存把数据读到处理器缓存里。这一步确保了其他线程获得的声明了volatile变量都是从主内存中获取最新的。 有序性Lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成。 volatile的应用场景synchronized关键字是防止多个线程同时执行一段代码，那么就会很影响程序执行效率，而volatile关键字在某些情况下性能要优于synchronized，但是要注意volatile关键字是无法替代synchronized关键字的，因为volatile关键字无法保证操作的原子性。通常来说，使用volatile必须具备以下2个条件： 1）对变量的写操作不依赖于当前值 2）该变量没有包含在具有其他变量的不变式中 下面列举几个Java中使用volatile的几个场景。 ①.状态标记量 123456789volatile boolean flag = false; //线程1while(!flag)&#123; doSomething();&#125; //线程2public void setFlag() &#123; flag = true;&#125; 根据状态标记，终止线程。 ②.单例模式中的double check 1234567891011121314151617class Singleton&#123; private volatile static Singleton instance = null; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if(instance==null) &#123; synchronized (Singleton.class) &#123; if(instance==null) instance = new Singleton(); &#125; &#125; return instance; &#125;&#125; 为什么要使用volatile 修饰instance？主要在于instance = new Singleton()这句，这并非是一个原子操作，事实上在 JVM 中这句话大概做了下面 3 件事情: 1.给 instance 分配内存 2.调用 Singleton 的构造函数来初始化成员变量 3.将instance对象指向分配的内存空间（执行完这步 instance 就为非 null 了）。 但是在 JVM 的即时编译器中存在指令重排序的优化。也就是说上面的第二步和第三步的顺序是不能保证的，最终的执行顺序可能是 1-2-3 也可能是 1-3-2。如果是后者，则在 3 执行完毕、2 未执行之前，被线程二抢占了，这时 instance 已经是非 null 了（但却没有初始化），所以线程二会直接返回 instance，然后使用，然后顺理成章地报错。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>内存</tag>
        <tag>脏读</tag>
        <tag>锁</tag>
        <tag>synchronized</tag>
        <tag>happens-before</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程和线程的区别与联系]]></title>
    <url>%2F2018%2F05%2F11%2F%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[定义进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动，进程是系统进行资源分配和调度的一个独立单位。 线程是进程的一个实体，是CPU调度的基本单位,它是比进程更小的能独立运行的基本单位.线程自己基本上不拥有系统资源，只拥有一点在运行中必不可少的资源(如程序计数器，一组寄存器和栈)，但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源。 性质进程的特征： 1.动态性：进程的实质是程序的一次执行过程，进程是动态产生，动态消亡的。 2.并发性：任何进程都可以同其他进程一起并发执行。 3.独立性：进程是一个能独立运行的基本单位，同时也是系统分配资源和调度的独立单位。 4.异步性：由于进程间的相互制约，使进程具有执行的间断性，即进程按各自独立的、不可预知的速度向前推进。 线程的性质：1.线程是进程内的一个相对独立的可执行的单元。若把进程称为任务的话，那么线程则是应用中的一个子任务的执行。 2.由于线程是被调度的基本单元，而进程不是调度单元。所以，每个进程在创建时，至少需要同时为该进程创建一个线程。即进程中至少要有一个或一个以上的线程，否则该进程无法被调度执行。 3.进程是被分给并拥有资源的基本单元。同一进程内的多个线程共享该进程的资源，但线程并不拥有资源，只是使用他们。 4.线程是操作系统中基本调度单元，因此线程中应包含有调度所需要的必要信息，且在生命周期中有状态的变化。 5.由于共享资源【包括数据和文件】，所以线程间需要通信和同步机制，且需要时线程可以创建其他线程，但线程间不存在父子关系。 关系进程是系统进行资源资源分配和调度的基本单位，线程是程序运行的基本单位。 一个进程可以有多个线程，所以进程与线程是包含与被包含的关系。 一个线程可以创建和撤销另一个线程;同一个进程中的多个线程之间可以并发执行。 相对进程而言，线程是一个更加接近于执行体的概念，它可以与同进程中的其他线程共享数据，但拥有自己的栈空间，拥有独立的执行序列。 区别程和线程的主要差别在于它们是不同的操作系统资源管理方式。进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程。 1) 简而言之,一个程序至少有一个进程,一个进程至少有一个线程。 2) 线程的划分尺度小于进程，使得多线程程序的并发性高。 3) 另外，进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率。 4) 线程在执行过程中与进程还是有区别的。每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。 5) 从逻辑角度来看，多线程的意义在于一个应用程序中，有多个执行部分可以同时执行。但操作系统并没有将多个线程看做多个独立的应用，来实现进程的调度和管理以及资源分配。这就是进程和线程的重要区别。 背景以下内容转自知乎： 首先来一句概括的总论：进程和线程都是一个时间段的描述，是CPU工作时间段的描述。 下面细说背景：CPU+RAM+各种资源（比如显卡，光驱，键盘，GPS, 等等外设）构成我们的电脑，但是电脑的运行，实际就是CPU和相关寄存器以及RAM之间的事情。 一个最最基础的事实：CPU太快，太快，太快了，寄存器仅仅能够追的上他的脚步，RAM和别的挂在各总线上的设备完全是望其项背。那当多个任务要执行的时候怎么办呢？轮流着来?或者谁优先级高谁来？不管怎么样的策略，一句话就是在CPU看来就是轮流着来。 一个必须知道的事实：执行一段程序代码，实现一个功能的过程介绍 ，当得到CPU的时候，相关的资源必须也已经就位，就是显卡啊，GPS啊什么的必须就位，然后CPU开始执行。这里除了CPU以外所有的就构成了这个程序的执行环境，也就是我们所定义的程序上下文。当这个程序执行完了，或者分配给他的CPU执行时间用完了，那它就要被切换出去，等待下一次CPU的临幸。在被切换出去的最后一步工作就是保存程序上下文，因为这个是下次他被CPU临幸的运行环境，必须保存。 串联起来的事实 先加载程序A的上下文，然后开始执行A，保存程序A的上下文，调入下一个要执行的程序B的程序上下文，然后开始执行B,保存程序B的上下文。。。 进程和线程就是在这样的背景出来的，两个名词不过是对应的CPU时间段的描述，名词就是这样的功能。 进程就是包换上下文切换的程序执行时间总和 = CPU加载上下文+CPU执行+CPU保存上下文 线程是什么呢？进程的颗粒度太大，每次都要有上下的调入，保存，调出。如果我们把进程比喻为一个运行在电脑上的软件，那么一个软件的执行不可能是一条逻辑执行的，必定有多个分支和多个程序段，就好比要实现程序A，实际分成 a，b，c等多个块组合而成。那么这里具体的执行就可能变成： 程序A得到CPU =》CPU加载上下文，开始执行程序A的a小段，然后执行A的b小段，然后再执行A的c小段，最后CPU保存A的上下文。 里的a，b，c就是线程，也就是说线程是共享了进程的上下文环境，的更为细小的CPU时间段。到此全文结束，再一个总结： 进程和线程都是一个时间段的描述，是CPU工作时间段的描述，不过是颗粒大小不同。 开个QQ，开了一个进程；开了迅雷，开了一个进程。在QQ的这个进程里，传输文字开一个线程、传输语音开了一个线程、弹出对话框又开了一个线程。 所以运行某个软件，相当于开了一个进程。在这个软件运行的过程里（在这个进程里），多个工作支撑的完成QQ的运行，那么这“多个工作”分别有一个线程。 所以一个进程管着多个线程。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>内存</tag>
        <tag>线程</tag>
        <tag>进程</tag>
        <tag>CPU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MD5算法]]></title>
    <url>%2F2018%2F05%2F09%2FMD5%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Message Digest Algorithm MD5（中文名为消息摘要算法）为计算机安全领域广泛使用的一种散列函数，用以提供消息的完整性保护，用于确保信息传输完整一致 。 MD5算法的特点1、压缩性：任意长度的数据，算出的MD5值长度都是固定的。 2、容易计算：从原数据计算出MD5值很容易。 3、抗修改性：对原数据进行任何改动，哪怕只修改1个字节，所得到的MD5值都有很大区别。 4、强抗碰撞：已知原数据和其MD5值，想找到一个具有相同MD5值的数据（即伪造数据）是非常困难的。 MD5应用一致性验证MD5的典型应用是对一段信息（Message）产生信息摘要（Message-Digest），以防止被篡改。 MD5将整个文件当作一个大文本信息，通过其不可逆的字符串变换算法，产生了这个唯一的MD5信息摘要。 我们常常在某些软件下载站点的某软件信息中看到其MD5值，它的作用就在于我们可以在下载该软件后，对下载回来的文件用专门的软件（如Windows MD5 Check等）做一次MD5校验，以确保我们获得的文件与该站点提供的文件为同一文件。 具体来说文件的MD5值就像是这个文件的数字指纹。每个文件的MD5值是不同的，如果任何人对文件做了任何改动，其MD5值也就是对应的“数字指纹”就会发生变化。比如下载服务器针对一个文件预先提供一个MD5值，用户下载完该文件后，用我这个算法重新计算下载文件的MD5值，通过比较这两个值是否相同，就能判断下载的文件是否出错，或者说下载的文件是否被篡改了。 利用MD5算法来进行文件校验的方案被大量应用到软件下载站、论坛数据库、系统文件安全等方面。 数字签名MD5的典型应用是对一段Message(字节串)产生fingerprint指纹，以防止被“篡改”。举个例子，你将一段话写在一个叫 readme.txt文件中，并对这个readme.txt产生一个MD5的值并记录在案，然后你可以传播这个文件给别人，别人如果修改了文件中的任何内容，你对这个文件重新计算MD5时就会发现（两个MD5值不相同）。如果再有一个第三方的认证机构，用MD5还可以防止文件作者的“抵赖”，这就是所谓的数字签名应用。 安全访问认证MD5还广泛用于操作系统的登陆认证上，如系统登录密码、数字签名等诸多方面。如在Unix系统中用户的密码是以MD5（或其它类似的算法）经Hash运算后存储在文件系统中。当用户登录的时候，系统把用户输入的密码进行MD5 Hash运算，然后再去和保存在文件系统中的MD5值进行比较，进而确定输入的密码是否正确。通过这样的步骤，系统在并不知道用户密码的明码的情况下就可以确定用户登录系统的合法性。这可以避免用户的密码被具有系统管理员权限的用户知道。MD5将任意长度的“字节串”映射为一个128bit的大整数，并且是通过该128bit反推原始字符串是困难的，换句话说就是，即使你看到源程序和算法描述，也无法将一个MD5的值变换回原始的字符串，从数学原理上说，是因为原始的字符串有无穷多个，这有点象不存在反函数的数学函数。 算法原理对MD5算法简要的叙述可以为：MD5以512位分组来处理输入的信息，且每一分组又被划分为16个32位子分组，经过了一系列的处理后，算法的输出由四个32位分组组成，将这四个32位分组级联后将生成一个128位散列值。总体流程如下图所示， 表示第i个分组，每次的运算都由前一轮的128位结果值和第i块512bit值进行运算。 弱点2004年8月17日的美国加州圣巴巴拉的国际密码学会议（Crypto’2004）上，来自中国山东大学的王小云教授做了破译MD5、HAVAL-128、 MD4和RIPEMD算法的报告，公布了MD系列算法的破解结果。宣告了固若金汤的世界通行密码标准MD5的堡垒轰然倒塌，引发了密码学界的轩然大波。(注意:并非是真正的破解，只是加速了杂凑冲撞）所谓杂凑冲撞指两个完全不同的讯息经杂凑函数计算得出完全相同的杂凑值。根据鸽巢原理，以有长度限制的杂凑函数计算没有长度限制的讯息是必然会有冲撞情况出现的。 MD5不可逆的原因是其是一种散列函数，使用的是hash算法，在计算过程中原文的部分信息是丢失了的，一个MD5理论上的确是可能对应无数多个原文的，因为MD5是有限多个的而原文可以是无数多个。比如主流使用的MD5将任意长度的“字节串映射为一个128bit的大整数。也就是一共有2^128种可能，大概是3.4*10^38，这个数字是有限多个的，而但是世界上可以被用来加密的原文则会有无数的可能性。不过需要注意的一点是，尽量这是一个理论上的有限对无限，不过问题是这个无限在现实生活中并不完全成立，因为一方面现实中原文的长度往往是有限的（以常用的密码为例，一般人都在20位以内），另一方面目前想要发现两段原文对应同一个MD5（专业的说这叫杂凑冲撞）值非常困难，因此某种意义上来说，在一定范围内想构建MD5值与原文的一一对应关系是完全有可能的。所以对于MD5目前最有效的攻击方式就是彩虹表，具体详情你可以通过链接了解。 1、md5就是32个十六进制数，其信息量是2的128次方。任意一个长度超过16字节的字符串，它的变数就已经超过这个值，所以肯定是不可逆的； 2、MD5相当于超损压缩； 3、王小云教授做的不是根据密文能得到原文的这种破解，他只是能伪造一个跟原文不一样值使得这个值的MD5值跟原文的MD5值一样，并不能准确的找到这个值，即：只能找到a和b使md5(a)=md5(b) 并不能明确原值是a还是b；]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>安全</tag>
        <tag>算法</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2018%2F05%2F07%2Ftest%2F</url>
    <content type="text"><![CDATA[标题一测试搭建的文章 标题二搭建好博客后，我们的各种细节配置基本都是在配置文件中完成的，Hexo中的配置文件一共分2中，在文件夹跟目录下的_config.yml叫做站点配置文件,同样的文件名我们可以在theme文件夹下的主题文件夹里面也找的。而主题文件夹下的_config.yml叫做主题配置文件。 1234567891011121314151617public class StringBufferT3 &#123; /** * @param args */ public static void main(String[] args) &#123; // TODO Auto-generated method stub StringBuffer buf=new StringBuffer(); buf.append("World!!"); buf.insert(0, "Hello "); System.out.println(buf); buf.insert(buf.length(), "MM~"); System.out.println(buf); &#125;&#125; 以上是常用的字符串拼接java代码 标题三结束 1$ ssh -T git@github.com]]></content>
  </entry>
  <entry>
    <title><![CDATA[hello,I love zhizhi,Hexo]]></title>
    <url>%2F2018%2F04%2F23%2Fhello-I-love-zhizhi-Hexo%2F</url>
    <content type="text"></content>
  </entry>
</search>
